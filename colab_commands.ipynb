{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"colab_commands.ipynb","provenance":[],"collapsed_sections":["dMgLlWrVZ68H","rBroD07xanI3","nbyWeA3SWVI7"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dMgLlWrVZ68H"},"source":["#pip install, import , check versions"]},{"cell_type":"code","metadata":{"id":"8gm-ECuacUjL","cellView":"both"},"source":["!pip install torch>=1.2.0\n","!pip install torchaudio\n","!pip install samplerate   #resample numpy audio array data\n","!pip install audioread #read info before loading audio\n","!pip install pydub    #AudioSegmentaion\n","!pip install pyaudio\n","!pip install pytube3 --upgrade  #YouTube\n","!pip install PyAutoGUI    #keyboard, mouse, display,  -  control\n","!apt install libasound2-dev portaudio19-dev libportaudio2 libportaudiocpp0 ffmpeg\n","!pip install ffmpeg-python\n","!pip install webp     # read and write .webp images\n","!apt-get install tree\n","!pip install --upgrade keras\n","!pip install keras-self-attention\n","!pip install bpemb\n","!pip install mega.py    #Mega\n","!pip install kaggle\n","\n","\n","%matplotlib inline\n","\n","from IPython.display import Javascript\n","from google.colab.output import eval_js\n","from base64 import b64decode\n","import ffmpeg\n","import audioread\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torchaudio\n","import librosa\n","from pydub import AudioSegment\n","import keras\n","import tensorflow as tf\n","import sys\n","import sklearn as sk\n","import pandas as pd\n","\n","print(\"Tensor Flow Version: {}\".format(tf.__version__))\n","print(\"Keras Version: {}\".format(keras.__version__))\n","print(\"Python {}\".format(sys.version))\n","print('Pandas {}'.format(pd.__version__))\n","print('Scikit-Learn {}'.format(sk.__version__))\n","print(torchaudio.__version__)\n","!pip show tensorflow"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rBroD07xanI3"},"source":["#built-in methods and opearations"]},{"cell_type":"code","metadata":{"id":"BeSNlwcqk6wn"},"source":["#@title string operations\n","string = ' string '\n",".capitalize()#\tConverts the first character to upper case of a string>> hello => Hello\n","\n","len_ =  10   +len(string)+   10\n","fillwith = ' '\n",".center(len_, fillwith)#\tReturns a centered string\n","\n","value, start, end = 'a', 3, 10\n",".count(value, start, end)#\tReturns the number of times a specified value occurs in a string  # case sensitive  a != A\n","\n","enc = ['UTF-8','ascii','iso-8859-1']\n","err = ['backslashreplace',      #uses a backslash instead of the character that could not be encoded\n","       'ignore',                #ignores the characters that cannot be encoded\n","       'namereplace',           #replaces the character with a text explaining the character\n","       'strict',                #Default, raises an error on failure\n","       'replace',               #replaces the non-printable character with a questionmark\n","       'xmlcharrefreplace'      #replaces the character with an xml character\n","       ]\n",".encode('utf-8', 'replace').decode('utf-8','surrogateescape')#on error surrogate not allowed or character is not printable\n",".encode(encoding=enc[0], errors=err[0])#\tReturns an encoded version of the string with replaced or  removed unknown chars like: å\n","str(string,enc[0],err[0])\n","\n","value, start, end = '\\n', 3, 10\n",".endswith(value, start, end)#\tReturns true if the string ends with the specified value\n","\n",".expandtabs(10)#\tSets the tab size of the string\n","\n","a:<\t\t#Left aligns the result (within the available space)\n","a:>\t\t#Right aligns the result (within the available space)\n","a:^\t\t#Center aligns the result (within the available space)\n","a:=\t\t#Places the sign to the left most position\n","a:+\t\t#Use a plus sign to indicate if the result is positive or negative\n","a:-\t\t#Use a minus sign for negative values only\n","a: \t\t#Use a space to insert an extra space before positive numbers (and a minus sign befor negative numbers)\n","a:,\t\t#Use a comma as a thousand separator\n","a:_\t\t#Use a underscore as a thousand separator\n","a:b\t\t#Binary format\n","a:c\t\t#Converts the value into the corresponding unicode character\n","a:d\t\t#Decimal format\n","a:e\t\t#Scientific format, with a lower case e\n","a:E\t\t#Scientific format, with an upper case E\n","a:f\t\t#Fix point number format\n","a:F\t\t#Fix point number format, in uppercase format (show inf and nan as INF and NAN)\n","a:g\t\t#General format\n","a:G\t\t#General format (using a upper case E for scientific notations)\n","a:o\t\t#Octal format\n","a:x\t\t#Hex format, lower case\n","a:X\t\t#Hex format, upper case\n","a:n\t\t#Number format\n","a:%\t\t#Percentage format\n","\n","# all equavalent  to f'' method\n","a,b = 10,20\n","'{} {}'.format(a,b)\n","'{0} {1}'.format(a,b)\n","'{a} {b}'.format(a,b)\n","'{a:.2f} {b:.2f}'.format(a,b)\n","\"{:.2f}\".format(a)\n","\n","#c type print\n","\"%4.3f %4.3f\" % (a,b)\n","\"%4.3f\" % a\n","\n","point = {'a':4,'b':-5}\n","'{a} {b}'.format(**point)\n","'{a} {b}'.format_map(point)\n","\n","class point(dict):\n","    def __missing__(self, key):return key\n","'{a}, {b}'.format_map(  point(a='6')  )             #6,b\n","'{a}, {b}'.format_map(  point(b='5')  )             #a,5\n","'{a}, {b}'.format_map(  point(a='6', b='5')  )      #6,5\n","\n","value, start, end = 'a', 3, 10\n",".index(value, start, end)# return start index of search string and if not found then raise exception\n","\n",".isalnum()#\tReturns True if all characters in the string are alphanumeric\n",".isalpha()#\tReturns True if all characters in the string are in the alphabet\n",".isdecimal()#\tReturns True if all characters in the string are decimals\n",".isdigit()#\tReturns True if all characters in the string are digits\n",".isidentifier()\t#Returns True if the string is an identifier >> no valid: '2adfd' or 'adf ad' or 'as#%^&ad'\n",".islower()\t#Returns True if all characters in the string are lower case\n",".isnumeric()\t#Returns True if all characters in the string are numeric\n",".isprintable()\t#Returns True if all characters in the string are printable\n",".isspace()\t#Returns True if all characters in the string are whitespaces\n",".istitle()\t#Returns True if the string follows the rules of a title>> valid: 'Go 1234324 _^%*&^&* Back' or 'Go Back'\n",".isupper()\t#Returns True if all characters in the string are upper case\n","\n","joinwith = '_'\n","strings = ['an','bat','cook'] # list,tuple, dict,set any iterable\n","joinwith.join()\t#Joins the elements of an iterable to the end of the string\n","\n",".casefold()#string into lower case  and better than lower()  #slower than .lowecase()\n",".lower()\t #string into lower case# faster than casefold()\n","\n","by= '/'  # search by first occurence\n","'fasf/afa/fa/fad'.partition(by)\t#Returns a tuple where the string is parted into three parts>> ('fasf','/','afa/fa/fad')\n","by= 'dd'  # search by last occurance\n","'fasf/afa/fa/fad'.rpartition(by)\t#Returns a tuple where the string is parted into three parts>> ('fasf/afa/fa,'/','fad')\n","\n","oldvalue, newvalue, count = 'hello', 'bye', 3     #replace 'hello' with 'bye' in count 3\n",".replace(oldvalue, newvalue, count)\t#Returns a string where a specified value is replaced with a specified value\n","#return -1\n","value, start, end = 'a', 3, 10\n",".find(value, start, end)# return start index of search string and -1 if not found whre index() raise exception\n","#return -1\n","value, start, end = 'hello', 3, 10   # find 'hello' from 3~10 positions only by rights most occurence\n",".rfind(value, start, end)\t#Searches the string for a specified value and returns the last position of where it was found\n","#raise exception\n","value, start, end = 'hello', 3, 10   # find 'hello' from 3~10 positions only by rights most occurence\n",".rindex(value,start,end)\t#Searches the string for a specified value and returns the last position of where it was found\n","\n","string = 'hello'\n","length = len(string)+5\n","fillwith = '/'\n",".ljust(length,fillwith)\t#Returns a left justified version of the string  >> /////hello\n",".rjust(length,fillwith)\t#Returns a right justified version of the string>> 'abc/////'\n","\n","remove = [' /.as#7']    #chars to remove by right side of string\n",".rstrip(remove)\t#Returns a right trim version of the string\n","\n","remove = [' /.as#7']    #chars to remove by left side of string\n",".lstrip(remove)\t#Returns a left trim version of the string\n","\n","remove = [' /.as#7']    #chars to remove by both sides of string\n",".strip(remove)\t#Returns a trim version of the string\n","\n","list(filter(None, string))#remove any empty '' string\n","\n","separator, maxsplit = '_', 2\n",".rsplit(separator, maxsplit-1)\t#Splits the string by right at the specified separator, and returns a list>> ['ab','bc']\n","\n","separator, maxsplit = '_', 2\n",".split(separator, maxsplit-1)\t#Splits the string at the specified separator, and returns a list>> ['ab','bc']\n","\n","keeplinebreaks = True #bool value to keep or not keeep linebreaks\n",".splitlines(keeplinebreaks)\t#Splits the string at line breaks and returns a list\n","\n","value, start, end = 'a', 3, 10 # if string from 3~10 starts with 'a'\n",".startswith(value, start, end)\t#Returns true if the string starts with the specified value>> bool(True|False)\n","\n","\n",".swapcase()\t#Swaps cases, lower case becomes upper case and vice versa\n","title()\t#Converts the first character of each word to upper case\n",".upper()\t#Converts a string into upper case\n","\n","len_ = 5 + len(string)\n",".zfill(len_)\t#Fills the string with a specified number of 0 values at the beginning>> '00000ab'\n","\n","#encoding \n","replace, with_ = 'o a', 'O_A'     #chars to replace\n","remove='wr'                     #chars to remove\n","Dic = 'how are you?'.maketrans(replace,with_,remove)        # replace chars with values and none values\n","'how are you?'.translate(Dic)     # use replaced or encoded chars\n","#or\n","char= 'aiour</'     #characters to remove from string\n","'how are you?'.translate(''.maketrans('','',char))     # translate uses the replaced or encoded chars\n","\n","#reverse string\n","string[::-1]# Hello >> olleH\n","\n","# string modul\n","import string\n",".whitespace# = ' \\t\\n\\r\\v\\f'\n",".ascii_lowercase# = 'abcdefghijklmnopqrstuvwxyz'\n",".ascii_uppercase# = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",".ascii_letters# = ascii_lowercase + ascii_uppercase\n",".digits# = '0123456789'\n",".hexdigits# = digits + 'abcdef' + 'ABCDEF'\n",".octdigits# = '01234567'\n",".punctuation# = r\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",".printable# = digits + ascii_letters + punctuation + whitespace"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jk8_kdr_tZw-","cellView":"form"},"source":["#@title built-in methods function operations\n","source= 'a = 5\\nb=6\\nsum=a+b\\nprint(\"sum =\",sum)'# a normal string, a byte string, or an AST object\n","filename= 'abc'# - file from which the code was read. If it wasn't read from a file, you can give a name yourself\n","mode=[ #- Either exec or eval or single.\n","  'eval', #- accepts only a single expression.\n","  'exec', #- It can take a code block that has Python statements, class and functions and so on.\n","  'single']# - if it consists of a single interactive statement\n","flags# (optional) and dont_inherit (optional) - controls which future statements affect the compilation of the source. Default Value: 0\n","optimize# (optional) - optimization level of the compiler. Default value -1.\n","compile(source, filename, mode[1], flags=0, dont_inherit=False, optimize=-1)\n","\n","code = 'a = 5\\nb=6\\nsum=a+b\\nprint(\"sum =\",sum)'# it have access to global variables\n","globals = {'func':func,'add':add} #or {} none\n","locals = {'a': a, 'sqrt': sqrt}\n","eval(expression, globals, locals)\n","\n","code = 'print(dir())'\n","globalsParameter = {'__builtins__' : None}\n","localsParameter = {'print': print, 'dir': dir}\n","exec(code, globalsParameter, localsParameter)\n","\n","'<' - Left aligns the result (within the available space)\n","'>' - Right aligns the result (within the available space)\n","'^' - Center aligns the result (within the available space)\n","'=' - Places the sign to the left most position\n","'+' - Use a plus sign to indicate if the result is positive or negative\n","'-' - Use a minus sign for negative values only\n","' ' - Use a leading space for positive numbers\n","',' - Use a comma as a thousand separator\n","'_' - Use a underscore as a thousand separator\n","'b' - Binary format\n","'c' - Converts the value into the corresponding unicode character\n","'d' - Decimal format\n","'e' - Scientific format, with a lower case e\n","'E' - Scientific format, with an upper case E\n","'f' - Fix point number format\n","'F' - Fix point number format, upper case\n","'g' - General format\n","'G' - General format (using a upper case E for scientific notations)\n","'o' - Octal format\n","'x' - Hex format, lower case\n","'X' - Hex format, upper case\n","'n' - Number format\n","'%' - Percentage format\n","foramt(0.5,'%') # 50%\n","format(1234, \"*>+7,d\")\n","format(123.4567, \"^-09.3f\")\n","\n","isinstance(\"Hello\", (float, int, str, list, dict, tuple))\n","\n","iterable = [1,2,3,4,5]\n","sentinal = 3    #iter till 3\n","iter(iterable,sentinal)\n","\n","list_ = [1,2,3]\n","end = 'done!'       #return default value at end of the list when no elements left\n","next(list_, end)#Exception: StopIteration\n","\n","map(myfunc, ('apple', 'banana', 'cherry'), ('orange', 'lemon', 'pineapple'),...)#myfunc(apple,orange)\n","\n","x = memoryview(b\"Hello\")\n","print(x)#memroy address\n","print(x[0])#return the Unicode of the first character>>72\n","\n","a=2\n","oct(a)# == 0o2 == 0O2 == 2\n","hex(a)#0x2 == 0X2 == 2\n","bin(a)#0b10 == 0B10 == 2\n","chr(65);ord('A')\n","base = 0#2,8,10,16>> bin,oct,int|integer,hex\n","int(a, base)\n","\n","value = [\"nan\",\"NaN\",\"inf\",\"InF\",\"InFiNiTy\",\"infinity\"]\n","float(value[0])#infinity and Nan(Not a number)\n","\n","import fractions\n","fractions.Fraction(1.5)#3/2\n","\n","#File\n","\"r\"# - Read - Default value. Opens a file for reading, error if the file does not exist\n","\"a\"# - Append - Opens a file for appending, creates the file if it does not exist\n","\"w\"# - Write - Opens a file for writing, creates the file if it does not exist\n","\"x\"# - Create - Creates the specified file, returns an error if the file exists\n","\"t\"# - Text - Default value. Text mode\n","\"b\"# - Binary - Binary mode (e.g. images)\n","mode = 'rwaxtb+'#can use multiple modes at once: 'r+b'   read/write in bin\n","f = open(filepath, mode[0], buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)\n","f.write(string)\n",".writelines(list_)#list of lines or strings; it wont add \\n automatically\n","#first 4 chars   #next 4 chars    #remaining chars\n",".read(chars)       ;f.read(chars);       f.read()\n",".tell()#current position\n",".seek(position)#go to start char if position=0 >>return new position\n",".readline(chars);f.readline();f.readline();f.readline()#read 4 lines\n",".readlines(lines)#list the lines\n",".truncate(chars)#what should be the total len of file # resizes the file\n",".writable()#bool if opened with 'a' or 'w'\n",".readable()#bool\n",".seekable()#bool\n",".name# filename of file\n",".closed# bool if closed\n",".mode # the mode by which the files was opened\n",".flush()#send to sys buffer to be written to file in special case only\n","os.fsync()# write buffere to disk file to help .flush()\n","\n","f.close()#very important\n","\n","output = [sys.stdout, fname]\n","with open(fname,'w') as f:    #write output to file f\n","  print('hello',40,10.2, sep=' ', end='\\n', file=f, flush=False)               #'ajfjdfj',54354353  will be writen to file not to sys output to display\n","\n","reversed(list_)# [3,2,1]\n","pow(4,3,5)#Return the value of 4 to the power of 3, modulus 5 (same as (4 * 4 * 4) % 5)\n","round(2.665, 2)#2.67\n","[0,1,2,3,4,5,6,7,8,9].slice(0,10,2)#[0,2,4,6,8]\n","\n","a = (1, 2, 3, 4, 5)\n","sum(a, 7)# 7+sum(a)\n","\n","super().__init__()#call to parent class method without knowing or typing the parent class name\n","zip(list1,list2,...)\n","new = List.copy()#list,dict,set,tuple"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIDqa8FKSeaQ","cellView":"form"},"source":["#@title binary and byte operation\n","# https://www.devdungeon.com/content/working-binary-data-python\n","# https://www.djangospin.com/working-binary-files-python/\n","# https://www.python.org/dev/peps/pep-0263/\n","# https://www.datacamp.com/community/tutorials/reading-writing-files-python\n","# https://dotnettutorials.net/lesson/working-with-binary-files-in-python/\n","# https://stackoverflow.com/questions/1035340/reading-binary-file-and-looping-over-each-byte\n","# https://docs.python.org/3/library/binary.html\n","# https://www.datacamp.com/community/data-science-cheatsheets\n","\n","# file signatures\n","# http://self.gutenberg.org/articles/eng/List_of_file_signatures\n","# https://digital-forensics.sans.org/media/hex_file_and_regex_cheat_sheet.pdf\n","# https://en.wikipedia.org/wiki/List_of_file_signatures\n","# https://www.garykessler.net/library/file_sigs.html\n","# https://www.filesignatures.net/index.php?page=all\n","\n","bin_file:\n","    header:(file type or extension)\n","            hex code indicating the start of the file\n","    footer:\n","            hex code indicating the end of the file\n","    blocks\n","flat_bin_file: does not contain headers\n","base64: converts bin file into a plain text representation. Encoding the data has the disadvantage of increasing the file size during the transfer (for example, using Base64 will increase the file's size by approximately 30%), as well as requiring translation back into binary after receipt. \n","Endianness:\n","            the order or sequence of bytes of a word of digital data in computer memory.\n","            Computers store information in various sized groups of binary bits. Each group is assigned a number, called its address,\n","              that the computer uses to access that data.\n","              On most modern computers, the smallest data group with an address is eight bits long and is called a byte.\n","              Larger groups comprise two or more bytes, for example, a 32-bit word contains four bytes.\n","              There are two possible ways a computer could number the individual bytes in a larger group, starting at either end.\n","              Both types of endianness are in widespread use in digital electronic engineering. \n","            big-endian (BE):  stores the most significant byte of a word at the smallest memory address and the least significant byte at the largest.\n","            little-endian (LE): stores the least-significant byte at the smallest address. \n","TIFF: MM - big-endian and II - little-endian\n","\n","f = open(\"myfile\", \"rb\")\n","try:\n","    byte = f.read(1)\n","    while byte != \"\":\n","        mymehtod(byte)\n","        byte = f.read(1)\n","finally:\n","    f.close()\n","# or\n","with open(\"myfile\", \"rb\") as f:\n","  while (byte := f.read(1)): method(byte)\n","  # or\n","    byte = f.read(1)\n","    while byte != \"\":\n","        mymehtod(byte)\n","        byte = f.read(1)\n","\n","# read in chunks\n","def bytes_from_file(filename, chunksize=8192):\n","    with open(filename, \"rb\") as f:\n","        while True:\n","            chunk = f.read(chunksize)\n","            if chunk:\n","                for b in chunk:\n","                    yield b\n","            else:\n","                break\n","\n","# \n","callable = lambda: file.read(nbytes)\n","sentinel = b''#empty byte string, end of file\n","iter(callable, sentinel): \n","# or\n"," iter(lambda: f.read(nbytes), b'')\n","\n","\n","#  fastset file reading, check the speed of this\n","file = \"binary_file.bin\"\n","data = np.fromfile(file, 'u1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGc4gn8HqCYt","cellView":"form"},"source":["#@title Dictionary methods operations\n","defalutValue = 0.0# if key is missing# optional\n",".get('salary', defalutValue)#>>None or defalutvalue\n",".setdefault('salary', defalutValue)#>>None or defalutvalue\n","d['salary']#error\n","\n","dict.fromkeys(Set)\n",".items()#>>list of tuples of k,v pairs\n",".popitem()\n",".pop(key,defaultValue)#>>value  #optional if not in dict\n",".keys()#>> list of keys\n",".values()#>>list of values\n",".update(dic)# add another dic\n",".update(key=value,...)# add another dict\n","\n","# list of keys\n","list(dic)\n","# delete key:value\n","del dic[keyname]\n","# add key:value\n","dic[newKeyName] = value\n","\n","# custom\n","#reverse the key with values\n","Dic = {1:'a',2:'b'}\n","rDic = dict(map(reversed, Dic.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FoU6eFMF1JEM","cellView":"form"},"source":["#@title set methods operations\n",".add(2)\n",".update([list_],{set_})\n",".remove(itm)# error if not present\n",".discard(itm)#no error if not present\n",".pop()#remove random ele as set is unordered always\n","\n","a|b# union\n","a.union(b) == b.union(a)\n","\n","a&b&c...# intersection\n","a.intersection(b,...) == b.intersection(a,...)#returns set\n","a.intersection_update(b,...)#returns None and update 'a'\n","\n","a-b#difference # remove b's elements from a\n","b-a#difference # remove a's elements from b\n","a.difference(b) != b.difference(a)# returns the difference without affecting actual sets\n","a.difference_update(b)#returns None and update \"a\"\n","\n","a^b#symmetric_difference # remove common elemnts\n","a.symmetric_difference(b) == b.symmetric_difference(a)\n","a.symmetric_difference_update(b)#returns None and updates 'a'\n","\n","a.isdisjoint(b)#>>bool:True if sets have no commom elements\n","a.issubset(b)#>>bool:True if all 'a' elements are in 'b'\n","b.issuperset(a)#bool:True if 'b' has all elements of 'a'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciHn-39LbEvO","cellView":"form"},"source":["#@title list and tuple operations\n","a = [1,1]#(1,1) for tuple operations\n","#repeat list or tuple 3 times\n","a * 3 # [1,1,1,1,1,1]\n","#empty the list\n","a.clear()\n","#or\n","a = []\n","#increasing list  or tuple\n","a + a # [1,1,1,1]\n","#or\n","c=(1,2);    b = 3\n","tuple([b]+list(c))  # (3,1,2)\n","[b]+list(c)  # [3,1,2]\n","\n","#get index of element in list or array\n","a.index('10')\n","a.index(33)\n","#multiply or product of elements\n","np.prod(a)\n","np.multiply(a)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z9yHA0SIyijw","cellView":"form"},"source":["#@title zip() method\n","a = [1,2,3,4,5,6,7,8,9,0]\n","b = [1,2,3,4,5,6,7,8,9,0]\n","for i,j in zip(a,b):\n","  print(i,j)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8Wp_qFBfXHC"},"source":["# exit process\n","print_this= 1#or 'hello' \n","sys.exit(print_this)    #exit running process with error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4ZPh8LcpTst","cellView":"form"},"source":["#@title regresion regex\n","import re\n","# https://www.w3schools.com/jsref/jsref_obj_regexp.asp\n","pattern = \n","'^a'#startwith 'a'\n","'a$'#endswith 'a'\n","'abc*'#0 or more chars matched at left side set\n","'abc+'#1 or more chars matched at left side set\n","'abc?'#0 or 1 only chars matched at left side set\n","'a{2,5}'#min 2 max 5 times repetition of 'a'\n","'a-f'# a to f\n","'[a-f]'#any of a- f\n","'[^a-f]'# none of a-f\n","'[a-f]{2,5}'#any of a-f in min 2 max 5 times occurence\n","'a|b'# any of a or b\n","'(a|b|c)ok'#any of a,b,c followed by 'ok'\n","'\\$'#startswith '$'\n","'\\Aabc'#startswith 'abc'\n","'\\babc'#any word in string startswith 'abc'\n","'abc\\b'#any word in string endswith 'abc'\n","'\\Babc'#any word in string not startswith 'abc'\n","'abc\\B'#any word in string not endswith 'abc'\n","'\\d'# any digit in string   >>adef12adf\n","'\\D'# any non-digit in string   >>12a23\n","'\\s' or '[ \\t\\n\\r\\f\\v]' #any whitespace char in string >> r'asdf\\vasdf\\t\\t adf \\n '\n","'\\S' or '[^ \\t\\n\\r\\f\\v]' #any non-whitespace char in string >> r' \\t\\n\\n\\tabc\\t'\n","'\\w' or '[a-zA-Z0-9_]' #any alphanumeric chars in string >> a3_\n","'\\W' or '[^a-zA-Z0-9_]' #any non-alphanumeric chars in string >> a3_%$\n","'\\Zabc' #endswith 'abc'\n","\n","pattern = 'r'+pattern # r'\\d'\n","re.findall(pattern, string)#list:including the pattern chars\n","\n","occr = 2# split by first 2 occurence only\n","re.split(pattern, string,occr) #list:not including the pattern chars\n","\n","occr = 2# replace by first 2 occurence only\n","re.sub(pattern, replace, string, occr)#str: replaced with matched pattern chars\n","\n","re.subn(pattern, replace, string)#(str,count): replaced with matched pattern chars, counted replaced\n","\n","match = re.search(pattern, string)#obj: if pattern found at first occurence of string\n","index1 = 1# first mached group\n","match.group(index1,index2,...)\n","match.groups()#tuple: matched words\n","match.start();match.end();match.span()# index of start,end, start-end\n","match.re#pattern\n","match.string#string\n","\n","re.match(pattern,  string)#obj\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1I9qoUZVNF6y","cellView":"form"},"source":["#@title datatime data time timestamp\n","from datetime import date,time, datetime,timedelta\n","#date\n","date(year = 2018, month = 7, day = 12)#2018-07-12\n","date.today()#2018-12-19\n","date.today().year # .month . day\n","date.fromtimestamp(1326244364)#2012-01-11\n","#time default values are 0:0:0:0\n","                                                                                '''Unix was originally developed in the 60s and 70s so the \"start\" of Unix Time was\n","                                                                                 set to January 1st 1970 at midnight GMT (Greenwich Mean Time) - this date/time was \n","                                                                                 assigned the Unix Time value of 0. This is what is know as the Unix Epoch.\n","                                                                                '''\n","time.time()#seconds from January 1, 1970, 00:00:00 unix epoch\n","time.ctime(unixepoch)#current time\n","time(11, 34, 56, 234566)#11:34:56.234566\n","time(11, 34, 56, 234566).hour# .minute .second .microsecond\n","#datetime\n","datetime(2017, 11, 28, 23, 55, 59, 342380)#2017-11-28 23:55:59.342380\n","datetime(2017, 11, 28, 23, 55, 59, 342380).year #.month .day .hour .minute .second .microsecond .timestamp . now()\n","datetime.now().time()#\n","\n","'%a'\tAbbreviated weekday name.\tSun, Mon, ...\n","'%A'\tFull weekday name.\tSunday, Monday, ...\n","'%w'\tWeekday as a decimal number.\t0, 1, ..., 6\n","'%d'\tDay of the month as a zero-padded decimal.\t01, 02, ..., 31\n","'%-d'\tDay of the month as a decimal number.\t1, 2, ..., 30\n","'%b'\tAbbreviated month name.\tJan, Feb, ..., Dec\n","'%B'\tFull month name.\tJanuary, February, ...\n","'%m'\tMonth as a zero-padded decimal number.\t01, 02, ..., 12\n","'%-m'\tMonth as a decimal number.\t1, 2, ..., 12\n","'%y'\tYear without century as a zero-padded decimal number.\t00, 01, ..., 99\n","'%-y'\tYear without century as a decimal number.\t0, 1, ..., 99\n","'%Y'\tYear with century as a decimal number.\t2013, 2019 etc.\n","'%H'\tHour (24-hour clock) as a zero-padded decimal number.\t00, 01, ..., 23\n","'%-H'\tHour (24-hour clock) as a decimal number.\t0, 1, ..., 23\n","'%I'\tHour (12-hour clock) as a zero-padded decimal number.\t01, 02, ..., 12\n","'%-I'\tHour (12-hour clock) as a decimal number.\t1, 2, ... 12\n","'%p'\tLocale’s AM or PM.\tAM, PM\n","'%M'\tMinute as a zero-padded decimal number.\t00, 01, ..., 59\n","'%-M'\tMinute as a decimal number.\t0, 1, ..., 59\n","'%S'\tSecond as a zero-padded decimal number.\t00, 01, ..., 59\n","'%-S'\tSecond as a decimal number.\t0, 1, ..., 59\n","'%f'\tMicrosecond as a decimal number, zero-padded on the left.\t000000 - 999999\n","'%z'\tUTC offset in the form +HHMM or -HHMM.\t \n","'%Z'\tTime zone name.\t \n","'%j'\tDay of the year as a zero-padded decimal number.\t001, 002, ..., 366\n","'%-j'\tDay of the year as a decimal number.\t1, 2, ..., 366\n","'%U'\tWeek number of the year (Sunday as the first day of the week). All days in a new year preceding the first Sunday are considered to be in week 0.\t00, 01, ..., 53\n","'%W'\tWeek number of the year (Monday as the first day of the week). All days in a new year preceding the first Monday are considered to be in week 0.\t00, 01, ..., 53\n","'%c'\tLocale’s appropriate date and time representation.\tMon Sep 30 07:06:05 2013\n","'%x'\tLocale’s appropriate date representation.\t09/30/13\n","'%X'\tLocale’s appropriate time representation.\t07:06:05\n","'%%'\tA literal '%' character.\t%\n","datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")# d-m-y to m-d-y\n","datestring = \"21 / June, 2018\"\n","formate =    \"%d / %B  , %Y\" # it should represent the datestring format to mask the datestring\n","datetime.strptime(datestring, formate)# 2018-06-21 00:00:00\n","#timedelta objects supports + - / * \n","a = date(year = 2018, month = 7, day = 12);         b = date(year = 2017, month = 12, day = 23)\n","a - b#201 days, 0:00:00\n","a = datetime(year = 2018, month = 7, day = 12, hour = 7, minute = 9, second = 33);      b = datetime(year = 2019, month = 6, day = 10, hour = 5, minute = 55, second = 13)\n","a - b#-333 days, 1:14:20\n","abs(a - b)# no negative values anymoew\n","a = timedelta(weeks = 2, days = 5, hours = 1, seconds = 33);        b = timedelta(days = 4, hours = 11, minutes = 4, seconds = 54)\n","a - b#14 days, 13:55:39\n","a.total_seconds()#3434.43432\n","\n","# current date and time\n","now = datetime.now()\n","timestamp = datetime.timestamp(now)\n","print(\"timestamp =\", timestamp)\n","#timestamp = 1545730073\n","dt_object = datetime.fromtimestamp(timestamp)\n","print(\"dt_object =\", dt_object)\n","print(\"type(dt_object) =\", type(dt_object))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JvsYy5Nkxjzp","cellView":"form"},"source":["#@title os modul: folder file dir mkdir rmdir create make isfile isdir exist getcwd\n","os.path.exists(path)  - Returns True if the path is a file, directory, or a valid symlink.\n","os.path.isfile(path)  - Returns True if the path is a regular file or a symlink to a file.\n","os.path.isdir(path)   - Returns True if the path is a directory or a symlink to a directory.\n","os.path.isabs(path)   - Returns True if the path is a absolute path or not.\n","os.path.abspath(path) - Returns absolute path of the given file or folder.\n","os.path.getsize(file) - Returns file size in number of bytes\n","os.getcwd()                                                                    # current directory\n","os.chdir('/newdir')\n","os.listdir(path='sample_data/')                                                # everything in that directory\n","os.listdir(os.getcwd())         #Display all of the files found in your current working directory\n","os.removedirs('/content/a')                                                    # delete\n","os.makedirs('/content/a', exist_ok=True)                                       # add, set True if already exist\n","%mkdir directoryname -p                   # make direcory\n","# both are same(delete file from the path)\n","  os.unlink(file_name)\n","  os.remove(file_name)\n","# check diractory (socket, FIFO, device file)\n","path=\"abc.txt\"  \n","if not os.path.isdir(path) and not os.path.isfile(path):\n","  print(\"It is a special file (socket, FIFO, device file)\" )\n","#or\n","Path.is_dir()\n","Path.is_file()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IZkEpGVHiI0"},"source":["#@title numpy.ndarray np\n","# create np.array of size 10\n","a = np.arange(10)\n","#list to np.array\n","a = [1,2,4,5,6]\n","a = np.array(a, dtype=np.int32)\n","# reshape array only if array|list size is same before and after reshape\n","a.reshape(5,1)\n","# create int array of zeros with shape(44000,1)\n","a = np.zeros((44000, 1), dtype= int))\n","# create empty np array with 0 rows and each row having 2 items\n","np.empty((0,2))\n","# delete remove\n","arr = np.delete(arr,row,0)\n","# if shape and elements are equal\n","np.array_equal(a,b) # True|False\n","# if all inner arrays of big array follows the small array's shape and elements patteren\n","a= np.array([1,2])# (1,2)\n","b = np.array([  [ [1,2],[1,2] ],  [ [1,2],[1,2] ] ]) # (1,2(1,2(1,2)))\n","np.array_equiv(a,b) # True|False  hence: (2,) == (2, 2, 2)\n","# random values\n"," a = np.random.rand(30,2,2,2)\n","# check number of dimensions\n","a.ndim\n","#get max value\n","np.amax(a)# np.amin(a)\n","#get max value index\n","np.argmax(a)# np.argmin(a)\n","#get elements which are greatere than 0, compare\n","newArr = arr[arr > 0]\n","# create array with specified vales\n","np.full((10), -1)\n","# create array with specific values\n","start = end = []\n","for _ in range(50000):\n","  start.extend(np.full((1), -1.0))\n","  end.extend(np.full((1), 1.0))\n","# slice divide separate in len(a)/50 parts\n","a = a[::50]\n","#multiply elements\n","np.prod([2,2,2])# 8\n","np.multiply(2.0, 4.0)# 8.0\n","# one type array to another type\n","arr.astype(np.int)#[1.,2.3,3.0] => [1,2,3]\n","#save one or more np.array in compressed file.npz\n","np.savez_compressed('./fname_without_ext',array_name1=array1,array_name2=array2)\n","#load compressed npz file\n","data = np.load('./fname_without_ext',allow_pickle=True)# returns the dict{name:aray}\n","data['array_name1']# gives array1\n","data['array_name2']# gives array2\n","# append in np numpy array list\n","Y = np.array([1,2,3,4,5,6,7,8,9,0])\n","samples = np.empty((0,2))#placeholder or empty nparray\n","for i in range(int(Y.size/2)):\n","  samples = np.append(samples, [Y[:2]], axis=0)\n","  Y = Y[2:]\n","# byte to ndarray\n","  #byte string to unsigned 16bit int\n","  np.frombuffer(byte_data_string, dtype=np.uint16)\n","#sort\n","sorted = np.sort(a, axis=-1, kind='quicksort')#{‘quicksort’, ‘mergesort’, ‘heapsort’, ‘stable’}\n","# or\n","dtype = [('name', 'S10'), ('height', float), ('age', int)]\n","values = [('Arthur', 1.8, 41), ('Lancelot', 1.9, 38),('Galahad', 1.7, 38)]\n","a = np.array(values, dtype=dtype)       # create a structured array\n","b = np.sort(a, order='height') \n","# shape (1,2,2,2) to shape (1,2*3,1,1)  # repeates that specific or any or all array\n","aa = np.tile(a,(1,3,1,1)))  # array of 1 image to array of 3 images\n","# nparray slicing\n","a[:2, 1:3] == a[:2][1:3]\n","a[0, 1] == a[0][1]\n","a[0, 0] = 77    #set element at index a[0][0]\n","# [[ 1  2  3  4]\n","#  [ 5  6  7  8]\n","#  [ 9 10 11 12]]\n","a[1, :]    #     [5 6 7 8]   of shape (4,)\n","a[1:2, :]  #    [[5 6 7 8]]  of shape (1, 4)\n","a[[1], :]  #    [[5 6 7 8]]  of shape (1, 4)\n","\n","a[:, 1]   #   [ 2  6 10]    (3,)\n","a[:, 1:2] #   [[ 2]\n","            #  [ 6]\n","            #  [10]] (3, 1)\n","# rows        indics\n","a[[0, 1, 2], [0, 1, 0]] == [a[0, 0], a[1, 1], a[2, 0]]\n","# returns mask type of bools of same shape as array \"a\"\n","mask = (a > 2) #[[False False]\n","              #  [ True  True]\n","              #  [ True  True]]\n","a[mask] == a[a > 2]  # returns elements which are greater than 2\n","# info about int or float data type\n","# int\n","np.iinfo(a.dtype)\n","# float\n","np.finfo(a.dtype)#finfo(resolution=1e-15, min=-1.7976931348623157e+308, max=1.7976931348623157e+308, dtype=float64)\n","# remove axis whose value is 1, eg. shape(2,1,3,1)\n","# remove all axes of value 1\n","np.squeeze(a)#shape(2,3)\n","# remove only specified axis\n","np.squeeze(a,1)#shape(2,3,1)\n","# add axis\n","a = a[np.newaxis,:,:]#shape(1,2,1,3,1)\n","# numpy dtypes\n","\n","Numpy type                         C type               Description\n","np.int8                            int8_t               Byte (-128 to 127)\n","np.int16                           int16_t              Integer (-32768 to 32767)\n","np.int32                           int32_t              Integer (-2147483648 to 2147483647)\n","np.int64                           int64_t              Integer (-9223372036854775808 to 9223372036854775807)\n","np.uint8                           uint8_t              Unsigned integer (0 to 255)\n","np.uint16                          uint16_t             Unsigned integer (0 to 65535)\n","np.uint32                          uint32_t             Unsigned integer (0 to 4294967295)\n","np.uint64                          uint64_t             Unsigned integer (0 to 18446744073709551615)\n","np.intp                            intptr_t             Integer used for indexing, typically the same as ssize_t\n","np.uintp                           uintptr_t            Integer large enough to hold a pointer\n","np.float32                         float\n","np.float64,np.float_               double               Note that this matches the precision of the builtin python float.\n","np.complex64                       float complex        Complex number, represented by two 32-bit floats (real and imaginary components)\n","np.complex128,np.complex_          double complex       Note that this matches the precision of the builtin python complex.\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3IS8hwIU7sT","cellView":"form"},"source":["#@title random modul\n","a=[1,2,3,4]\n","#choose random item or element\n","random.choice(a)\n","#shuffle\n","random.shuffle(a)# does change original list\n","#or\n","res = random.sample(a, len(a))#does not change original list\n","# random integer array form 0 to 4 of shape(2,4)\n","np.random.randint(5, size=(2, 4))# array([[4, 0, 2, 1],\n","                                 #        [3, 2, 2, 0]])\n","# random integer array with individual max value, shape(3)\n","np.random.randint(1, [3, 5, 10])# array([2, 2, 9])\n","# random integer array with individual min value, shape(3)\n","np.random.randint([1, 5, 7], 10)# array([9, 8, 7])\n","# random integer array with individual max values of individual rows, shape(2,4) using broadcasting with dtype of uint8\n","np.random.randint([1, 3, 5, 7], [[10], [20]], dtype=np.uint8)# array([[ 8,  6,  9,  7],\n","                                                             #        [ 1, 16,  9, 12]], dtype=uint8)\n","# random float audio sample\n","np.random.uniform(low=-1.0, high=1.0,size=(44100, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5_BxdALjrTQ","cellView":"form"},"source":["#@title sorting lists or tuples\n","a = [(1, 2), (3, 3), (1, 1)]\n","#decending\n","a.sort(reverse=True)\n","#by second elements\n","def by2nd(val): \n","    return val[1] \n","a.sort(key = by2nd)  #[(1, 1), (1, 2), (3, 3)]\n","# string by length\n","a.sort(key = len) \n","# acending\n","sorted(a)\n","#string  by int values\n","a.sort(key = int)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nm9r_rbcbOtf"},"source":["#media methods and opeartions"]},{"cell_type":"code","metadata":{"id":"aSRJXSHxAErV","cellView":"form"},"source":["#@title Video\n","# visit: https://file-examples.com/index.php/sample-video-files/\n","# visit: https://filesamples.com/categories/video     # all formats\n","# https://www.learningcontainer.com/mp4-sample-video-files-download/\n","# https://standaloneinstaller.com/blog/big-list-of-sample-videos-for-testers-124.html\n","# https://www.clipcanvas.com/a/video-clip-formats-and-codec-samples\n","# http://techslides.com/sample-files-for-development\n","# https://ffmpeg.org/documentation.html\n","# https://forum.videohelp.com/threads/373264-FFMpeg-List-of-working-sample-formats-per-format-and-encoder\n","!pip install moviepy# media editor API\n","  #visit site and see reference manule to explor everything.\n","  import moviepy.editor\n","  # NOTE: it does not immediately applies the effect to all the frames of the clip, but only to the first\n","    # frame: all the other frames will be resized only when required (that is, when you will write the whole\n","    # clip to a file of when you will preview it). Said otherwise, creating a new clip is neither time nor \n","    # memory hungry, all the computations happen during the final rendering.\n","    # see https://zulko.github.io/moviepy/getting_started/videoclips.html\n","  #load\n","    # form file\n","    Vclip = moviepy.editor.VideoFileClip(file_name)\n","    Vclip = ImageSequenceClip([list of files] or foldername)# from image_list or folder_name\n","    #from data\n","    # a video clip that always displays the same image. \n","      Vclip = ImageClip(--image_file or np.array--) # or .jpeg, .tiff, ...\n","      Vclip = vclip.to_ImageClip(t)# t: get frames at time t\n","    Vclip = TextClip(\"Hello\")#text to use\n","    Vclip = ColorClip(size=(460,380),color=[R,G,B])    \n","    def make_frame(t): return np.zeros((640,480,3),dtype=np.int)# get called fps-times to make frames = fps\n","    Vclip = VideoClip(make_frame, duration=3)\n","    # AUDIO CLIPS\n","      Aclip = moviepy.editor.AudioFileClip(--audio or video file or np.array or fun()--)\n","    #or  always use this\n","    with AudioFileClip(--file_name) as Aclip:pass\n","  #load or create arguments\n","    # one composant per pixel, between 0 and 1 (1:fully visible pixel ,0: transparent pixel). mask is always in greyscale.\n","      ismask=True    # if the clip is a mask clip :when creating or laoding\n","        #VideoClip(),\n","    has_mask=False#\n","      # VideoFileClip()\n","    fps = 25 # set fps for vclip or fps=44100 for aclip\n","    font=\"Amiri-Bold\",#text font to use\n","    color=\"black\",#text color\n","    fontsize=70,#text size\n","    stroke_width=5).resize(height=15)#stroke wont work on small text, use big txt then resize to small txt\n","    load_images=True#optional to load all at once\n","    duration=3# lenght of clip in seconds\n","      # VideoClip()\n","    transparent=True# to keep mask transparent or opaque\n","      # ImageClip()\n","    apply_to = 'mask'#'audio', ['mask','audio']. Specifies if the filter fl should also be applied to the audio or the mask of the clip, if any.\n","    keep_duration = True# if the transformation does not change the duration of the clip.\n","    audio=False# to mute or unmute clip\n","    codec= 'libx264'# to save vclip\n","    bitrate = #\n","    program='ffmpeg'# to use backed for write gifs:optional\n","    withmask=False# to save or not the clip with mask as alpha layer\n","    dtype='uint8'# to save clips\n","    change_end=True# change the end time by end = start+duraton\n","    make_frame = method()# a method that will be called to make frame for video or sample for audio\n","      #VideoClip(),\n","    has_constant_size=True#\n","    constant_size= False# for clips with moving image size.\n","  # fx and modifications, set methods  and returns new clip\n","    new_clip = clip_tobe_modified.method(arguemts)# all methods follows same pattren of syntax\n","    # aclip\n","      .volumex(1.2)\n","    # vlcip\n","      .volumex(0.8)\n","      .resize( (460,720) ) # New resolution: (460,720)\n","      .resize(0.6) # width and heigth multiplied by 0.6, shrink size, or expand, downsize 60%\n","      .resize(width=800) # height computed automatically.\n","      .resize(fun)  # a fun() that returns width and|or height or float-value\n","      .fx( vfx.resize, width=480,method='bilinear') # resize (keep aspect ratio)\n","      .fx( vfx.colorx, 0.5)) # darken the picture\n","      .margin(10) # add 10px contour , a frames around the entire Vclip\n","      .fx( vfx.mirror_x)#  left->righ mirror\n","      .fx( vfx.mirror_y)# upside-down mirror\n","      .add_mask()#adds mask made of 1s fully visible\n","      .subclip(stat_t,end_t)# extract the subclip from 00:00:50 - 00:00:60 duration\n","        # seconds =230.54 or (minutes, seconds)=(3,50.54) or (hour, min, sec)=(0,3,50.54) or time ='00:03:50.54' or time = (-30,-10): from end\n","      itr_obj = clip.iter_frames(#HxWxN: N=1 for mask, N=3 for RGB to return\n","                          fps=None,#optional\n","                           with_times=False,# returns: (time_t, frame)\n","                           logger=None,\n","                           dtype=None)#default: 'uint8',  float64 doesnot normalize images here-> 123.0 like data\n","    # for all clips\n","      .set_audio(Aclip)#set audio to video\n","      .set_mask(maskclip)# set mask\n","      .fx( vfx.speedx, 2) # double the speed\n","      .fl(make_frame,apply_to,keep_duration)\n","      .set_duration(t)# applied for all corresponding clips: vclip or aclip or gifclip or maskclip which are attched with it\n","      .set_fps(20)\n","      .set_start(t, change_end=True)\n","      .set_ismask(True)      \n","  # Meta-data, get methods\n","    returned_values = clip.attribute_name(values_if_any)#Syntax\n","    fps = clip.fps# returns clip's fps\n","    fonts = TextClip.list('font')#get a list of the possible fonts\n","    fonts = TextClip.search('Amiri', 'font') # Returns all font names containing Amiri\n","    frameORsample = clip.get_frame(t)#return rgb np.array or mono|stereo sample\n","    clip.w, clip.h#The width and height of the clip, in pixels.\n","      .size#The size of the clip, (width,heigth), in pixels.\n","      .ismask#Boolean set to True if the clip is a mask.\n","      .make_frame#A function t-> frame at time t where frame is a w*h*3 RGB array.\n","      .mask#VideoClip mask attached to this clip. If mask is None,The video clip is fully opaque.#returns mask layer or frame\n","      .audio#An AudioClip instance containing the audio of the video clip.\n","      .pos#A function t->(x,y) where x,y is the position of the clip when it is composed with other clips. See VideoClip.set_pos for more details\n","      .relative_pos\n","      # clip <-> mastk\n","        mclip = clip.to_mask()\n","        clip = mask_clip.to_RGB()\n","  #save or write audio or vidoe or gif\n","    Aclip.write_audiofile(\"G:\\Python37\\sample.mp3\")\n","    Vclip.write_videofile(\"myHolidays_edited.webm\",fps=12)# args are optional\n","    vclip.write_gif(file.gif)\n","    clip.save_frame(\"frame.png\") # by default the first frame is extracted or use t= fream at time t\n","  # merger or concatenate clips of any sizes to a single long clip\n","      Vclip = concatenate_videoclips([Vclip1,Vclip2,Vclip3])\n","      Aclip = concatenate_audioclips([aclip1, aclip2, aclip3])\n","    # array of mutated or modified clips\n","      clip1 = Vclip.margin(10) # add 10px contour , a frames around the entire Vclip\n","      clip2 = clip1.fx( vfx.mirror_x)#  left mirror\n","      clip3 = clip1.fx( vfx.mirror_y)# down mirror\n","      #resize or shrink or expand\n","      clip4 = clip1.resize(0.60) # downsize 60%, shrink size, or expand\n","      # array is, how to present each clip in single frame of 4 videos \n","      final_clip = clips_array([[clip1, clip2],\n","                              [clip3, clip4]]) \n","  # show\n","    clip.show or clip.preview# fastest way here\n","    my_clip.preview(fps=15, audio=False) # default;15 and don't generate/play the audio.\n","    my_audio_clip.preview(fps=22000)\n","    #ipython_display() supports html5 arguments\n","      clip.ipython_display(width=480, t=15)#frame at time t\n","      ipython_display(my_video_clip,fps=25,autoplay=1, loop=1)) # embeds a video# argumets are optional\n","      ipython_display(my_imageclip) # embeds an image\n","      ipython_display(my_audio_clip) # embeds a sound\n","      ipython_display(\"my_picture.jpeg\") # embeds an image\n","      ipython_display(\"my_video.mp4\") # embeds a video\n","      ipython_display(\"my_sound.mp3\") # embeds a sound\n","\n","import cv2  #read file, write video, capture from device, screenshots\n","  import numpy as np\n","  !pip install PyAutoGUI\n","  import pyautogui#screen caputre\n","  import os\n","\n","  to_filename = './video.avi'# write captured frames to this file\n","  FPS = 30.0;   res = '720'\n","  RESOLUTIONS =  {\"480\": (640, 480),\"720\": (1280, 720),\"1080\": (1920, 1080),\"4k\": (3840, 2160)}\n","  #(XVID is more preferable. MJPG results in high size video. X264 gives very small size video)\n","  VIDEO_CODECS = {'avi': cv2.VideoWriter_fourcc(*'XVID'),'mp4': cv2.VideoWriter_fourcc(*'XVID')}##https://www.fourcc.org/codecs.php list of codecs\n","  \n","  # set resolution dimensions\n","  def set_res(cap, res='480'):\n","      # get\n","      if res in RESOLUTIONS:   width,height = RESOLUTIONS[res]\n","      # set\n","      cap.set(3, width); cap.set(4, height)\n","      return width, height\n","  #  get codecs\n","  def fourcc(filename):\n","      filename, ext = os.path.splitext(filename)\n","      if ext in VIDEO_CODECS: return  VIDEO_CODECS[ext]\n","      return VIDEO_CODECS['avi']#default\n","  #initialize objects\n","    cap = cv2.VideoCapture(0)#init capturing source a video or device\n","    width,height = res,res\n","    cap.set(3, width); cap.set(4, height)#set_res(cap,res)\n","    out = cv2.VideoWriter(to_filename, fourcc(to_filename), FPS, (width,height))#init video obj file configs\n","  #START Capturing and saving\n","  while True:\n","    # make a screenshot\n","      img = pyautogui.screenshot()#region= (top, left, width ,height))#set vaules bro\n","      frame = np.array(img)\n","    #or capture from device or video\n","      ret, frame = cap.read()#capture frame:frame and return ret:bool\n","    out.write(frame)#write frame to video object file\n","    cv2.imshow('frame',frame)#optional to show frames\n","    if cv2.waitKey(1) & 0xFF == ord('q'):break\n","  #close all opend objects\n","  cap.release()\n","  out.release()\n","  cv2.destroyAllWindows()\n","  #get and set video propert \n","    cap.get(propId)\n","    cap.set(propId, value)\n","    # list# Note: When querying a property that is not supported by the backend used by the VideoCapture class, value 0 is returned.\n","      # 0:CV_CAP_PROP_POS_MSEC - Current position of the video file in milliseconds or video capture timestamp.\n","      # 1:CV_CAP_PROP_POS_FRAMES - 0-based index of the frame to be decoded/captured next.\n","      # 2:CV_CAP_PROP_POS_AVI_RATIO - Relative position of the video file: 0 - start of the film, 1 - end of the film.\n","      # 3:CV_CAP_PROP_FRAME_WIDTH - Width of the frames in the video stream.\n","      # 4:CV_CAP_PROP_FRAME_HEIGHT - Height of the frames in the video stream.\n","      # 5:CV_CAP_PROP_FPS - Frame rate.\n","      # 6:CV_CAP_PROP_FOURCC - 4-character code of codec.\n","      # 7:CV_CAP_PROP_FRAME_COUNT - Number of frames in the video file.\n","      # 8:CV_CAP_PROP_FORMAT - Format of the Mat objects returned by retrieve() .\n","      # 9:CV_CAP_PROP_MODE - Backend-specific value indicating the current capture mode.\n","      # 10:CV_CAP_PROP_BRIGHTNESS - Brightness of the image (only for cameras).\n","      # 11:CV_CAP_PROP_CONTRAST - Contrast of the image (only for cameras).\n","      # 12:CV_CAP_PROP_SATURATION - Saturation of the image (only for cameras).\n","      # 13:CV_CAP_PROP_HUE - Hue of the image (only for cameras).\n","      # 14:CV_CAP_PROP_GAIN - Gain of the image (only for cameras).\n","      # 15:CV_CAP_PROP_EXPOSURE - Exposure (only for cameras).\n","      # 16:CV_CAP_PROP_CONVERT_RGB - Boolean flags indicating whether images should be converted to RGB.\n","      # 17:CV_CAP_PROP_WHITE_BALANCE_U - The U value of the whitebalance setting (note: only supported by DC1394 v 2.x backend currently)\n","      # 18:CV_CAP_PROP_WHITE_BALANCE_V - The V value of the whitebalance setting (note: only supported by DC1394 v 2.x backend currently)\n","      # 19:CV_CAP_PROP_RECTIFICATION - Rectification flag for stereo cameras (note: only supported by DC1394 v 2.x backend currently)\n","      # 20:CV_CAP_PROP_ISO_SPEED - The ISO speed of the camera (note: only supported by DC1394 v 2.x backend currently)\n","      # 21:CV_CAP_PROP_BUFFERSIZE - Amount of frames stored in internal buffer memory (note: only supported by DC1394 v 2.x backend currently)\n","\n","# offline record audio and video\n","  #source: https://stackoverflow.com/questions/14140495/how-to-capture-a-video-and-audio-in-python-from-a-camera-or-webcam?rq=1\n","  import cv2\n","  import pyaudio\n","  import wave\n","  import threading\n","  import time\n","  import subprocess\n","  import os\n","\n","  ########################\n","  ## JRF\n","  ## VideoRecorder and AudioRecorder are two classes based on openCV and pyaudio, respectively. \n","  ## By using multithreading these two classes allow to record simultaneously video and audio.\n","  ## ffmpeg is used for muxing the two signals\n","  ## A timer loop is used to control the frame rate of the video recording. This timer as well as\n","  ## the final encoding rate can be adjusted according to camera capabilities\n","  ##\n","\n","  ########################\n","  ## Usage:\n","  ## \n","  ## numpy, PyAudio and Wave need to be installed\n","  ## install openCV, make sure the file cv2.pyd is located in the same folder as the other libraries\n","  ## install ffmpeg and make sure the ffmpeg .exe is in the working directory\n","  ##\n","  ## \n","  ## start_AVrecording(filename) # function to start the recording\n","  ## stop_AVrecording(filename)  # \"\" ... to stop it\n","  ##\n","  ##\n","  ########################\n","\n","\n","\n","  class VideoRecorder():\n","    \n","    \n","    \n","    # Video class based on openCV \n","    def __init__(self):\n","      \n","      self.open = True\n","      self.device_index = 0\n","      self.fps = 6               # fps should be the minimum constant rate at which the camera can\n","      self.fourcc = \"MJPG\"       # capture images (with no decrease in speed over time; testing is required)\n","      self.frameSize = (640,480) # video formats and sizes also depend and vary according to the camera used\n","      self.video_filename = \"temp_video.avi\"\n","      self.video_cap = cv2.VideoCapture(self.device_index)\n","      self.video_writer = cv2.VideoWriter_fourcc(*self.fourcc)\n","      self.video_out = cv2.VideoWriter(self.video_filename, self.video_writer, self.fps, self.frameSize)\n","      self.frame_counts = 1\n","      self.start_time = time.time()\n","\n","    \n","    # Video starts being recorded \n","    def record(self):\n","      \n","  #\t\tcounter = 1\n","      timer_start = time.time()\n","      timer_current = 0\n","      \n","      \n","      while(self.open==True):\n","        ret, video_frame = self.video_cap.read()\n","        if (ret==True):\n","          \n","            self.video_out.write(video_frame)\n","  #\t\t\t\t\tprint str(counter) + \" \" + str(self.frame_counts) + \" frames written \" + str(timer_current)\n","            self.frame_counts += 1\n","  #\t\t\t\t\tcounter += 1\n","  #\t\t\t\t\ttimer_current = time.time() - timer_start\n","            time.sleep(0.16)\n","            \n","            # Uncomment the following three lines to make the video to be\n","            # displayed to screen while recording\n","            \n","  #\t\t\t\t\tgray = cv2.cvtColor(video_frame, cv2.COLOR_BGR2GRAY)\n","  #\t\t\t\t\tcv2.imshow('video_frame', gray)\n","  #\t\t\t\t\tcv2.waitKey(1)\n","        else:\n","          break\n","                \n","          # 0.16 delay -> 6 fps\n","          # \n","          \n","\n","    # Finishes the video recording therefore the thread too\n","    def stop(self):\n","      \n","      if self.open==True:\n","        \n","        self.open=False\n","        self.video_out.release()\n","        self.video_cap.release()\n","        cv2.destroyAllWindows()\n","        \n","      else: \n","        pass\n","\n","\n","    # Launches the video recording function using a thread\t\t\t\n","    def start(self):\n","      video_thread = threading.Thread(target=self.record)\n","      video_thread.start()\n","\n","\n","\n","\n","\n","  class AudioRecorder():\n","    \n","\n","      # Audio class based on pyAudio and Wave\n","      def __init__(self):\n","          \n","          self.open = True\n","          self.rate = 44100\n","          self.frames_per_buffer = 1024\n","          self.channels = 2\n","          self.format = pyaudio.paInt16\n","          self.audio_filename = \"temp_audio.wav\"\n","          self.audio = pyaudio.PyAudio()\n","          self.stream = self.audio.open(format=self.format,\n","                                        channels=self.channels,\n","                                        rate=self.rate,\n","                                        input=True,\n","                                        frames_per_buffer = self.frames_per_buffer)\n","          self.audio_frames = []\n","\n","\n","      # Audio starts being recorded\n","      def record(self):\n","          \n","          self.stream.start_stream()\n","          while(self.open == True):\n","              data = self.stream.read(self.frames_per_buffer) \n","              self.audio_frames.append(data)\n","              if self.open==False:\n","                  break\n","          \n","              \n","      # Finishes the audio recording therefore the thread too    \n","      def stop(self):\n","        \n","          if self.open==True:\n","              self.open = False\n","              self.stream.stop_stream()\n","              self.stream.close()\n","              self.audio.terminate()\n","                \n","              waveFile = wave.open(self.audio_filename, 'wb')\n","              waveFile.setnchannels(self.channels)\n","              waveFile.setsampwidth(self.audio.get_sample_size(self.format))\n","              waveFile.setframerate(self.rate)\n","              waveFile.writeframes(b''.join(self.audio_frames))\n","              waveFile.close()\n","          \n","          pass\n","      \n","      # Launches the audio recording function using a thread\n","      def start(self):\n","          audio_thread = threading.Thread(target=self.record)\n","          audio_thread.start()\n","\n","\n","    \n","\n","\n","  def start_AVrecording(filename):\n","          \n","    global video_thread\n","    global audio_thread\n","    \n","    video_thread = VideoRecorder()\n","    audio_thread = AudioRecorder()\n","\n","    audio_thread.start()\n","    video_thread.start()\n","\n","    return filename\n","\n","\n","\n","\n","  def start_video_recording(filename):\n","          \n","    global video_thread\n","    \n","    video_thread = VideoRecorder()\n","    video_thread.start()\n","\n","    return filename\n","    \n","\n","  def start_audio_recording(filename):\n","          \n","    global audio_thread\n","    \n","    audio_thread = AudioRecorder()\n","    audio_thread.start()\n","\n","    return filename\n","\n","\n","\n","\n","  def stop_AVrecording(filename):\n","    \n","    audio_thread.stop() \n","    frame_counts = video_thread.frame_counts\n","    elapsed_time = time.time() - video_thread.start_time\n","    recorded_fps = frame_counts / elapsed_time\n","    print \"total frames \" + str(frame_counts)\n","    print \"elapsed time \" + str(elapsed_time)\n","    print \"recorded fps \" + str(recorded_fps)\n","    video_thread.stop() \n","\n","    # Makes sure the threads have finished\n","    while threading.active_count() > 1:\n","      time.sleep(1)\n","\n","    \n","  #\t Merging audio and video signal\n","    \n","    if abs(recorded_fps - 6) >= 0.01:    # If the fps rate was higher/lower than expected, re-encode it to the expected\n","                      \n","      print \"Re-encoding\"\n","      cmd = \"ffmpeg -r \" + str(recorded_fps) + \" -i temp_video.avi -pix_fmt yuv420p -r 6 temp_video2.avi\"\n","      subprocess.call(cmd, shell=True)\n","    \n","      print \"Muxing\"\n","      cmd = \"ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video2.avi -pix_fmt yuv420p \" + filename + \".avi\"\n","      subprocess.call(cmd, shell=True)\n","    \n","    else:\n","      \n","      print \"Normal recording\\nMuxing\"\n","      cmd = \"ffmpeg -ac 2 -channel_layout stereo -i temp_audio.wav -i temp_video.avi -pix_fmt yuv420p \" + filename + \".avi\"\n","      subprocess.call(cmd, shell=True)\n","\n","      print \"..\"\n","\n","\n","\n","\n","  # Required and wanted processing of final files\n","  def file_manager(filename):\n","\n","    local_path = os.getcwd()\n","\n","    if os.path.exists(str(local_path) + \"/temp_audio.wav\"):\n","      os.remove(str(local_path) + \"/temp_audio.wav\")\n","    \n","    if os.path.exists(str(local_path) + \"/temp_video.avi\"):\n","      os.remove(str(local_path) + \"/temp_video.avi\")\n","\n","    if os.path.exists(str(local_path) + \"/temp_video2.avi\"):\n","      os.remove(str(local_path) + \"/temp_video2.avi\")\n","\n","    if os.path.exists(str(local_path) + \"/\" + filename + \".avi\"):\n","      os.remove(str(local_path) + \"/\" + filename + \".avi\")\n","    \n","    \n","\n","    \n","  if __name__== \"__main__\":\n","    \n","    filename = \"Default_user\"\t\n","    file_manager(filename)\n","    \n","    start_AVrecording(filename)  \n","    \n","    time.sleep(10)\n","    \n","    stop_AVrecording(filename)\n","    print \"Done\"\n","# write images to video\n","  import cv2, glob\n","\n","  img_array = []\n","  for filename in glob.glob('C:/New folder/Images/*[.png - .jpg]'):#list of images\n","      img = cv2.imread(filename)\n","      height, width, channel = img.shape\n","      size = (width,height)\n","      img_array.append(img)\n","  #                                                                   fps, (width * hight))\n","  obj = cv2.VideoWriter('project.avi',cv2.VideoWriter_fourcc(*'DIVX'), 15,        size)\n","  for i in range(len(img_array)): obj.write(img_array[i])\n","  obj.release()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1jmXLph8pY7","cellView":"form"},"source":["# @title\n","import numpy as np\n","import glob, os, sys\n","from datetime import datetime\n","# !pip install pytube3 --upgrade  #YouTube\n","# !pip install pydub\n","# !pip install mega.py\n","# !pip install moviepy\n","if '/content/drive/My Drive/RIO_AUDIO/MODULS' not in sys.path: sys.path.append('/content/drive/My Drive/RIO_AUDIO/MODULS')\n","import utils as ut\n","def amax(itr):\n","  while True:\n","    i = next(itr)\n","    v = np.amax(i)\n","    if v > 0.0: print('max value: ',v);break\n","  return i\n","def tc(alpha=3,beta=20, dcode='''type(data)'''):\n","  Tavg=''\n","  dtype = [('idx', int), ('avg', float)]\n","  #definition\n","  def load(code:str,itr=20):\n","    nonlocal avg,j\n","    data = None; rate = None  ;g=[];  tim1 = datetime.timestamp(datetime.now())\n","    for _ in range(itr):\n","      data = eval(code)\n","      tim2 = datetime.timestamp(datetime.now())\n","      gap = tim2-tim1;      g.append(gap);      tim1 = tim2\n","    avg.append((j,sum(g)/len(g)))\n","    print('\\r-',alpha-i-1,j,eval(dcode),code,end='',flush=True)\n","  #Start\n","  for i,n in enumerate(range(alpha)):\n","    avg=[]\n","    for j,c in enumerate(code): load(c,beta)\n","    Tavg += f'{np.sort(np.array(avg,dtype=dtype),order=\"avg\")}\\n'\n","  print();print(Tavg)\n","\n","from PIL import Image\n","import cv2, imageio\n","from google.colab.patches import cv2_imshow\n","import matplotlib.pyplot as plt\n","import moviepy.editor\n","\n","\n","\n","# !wget 'http://bigmusic.cc/siteuploads/files/sfd58/28590/Ice%20Cream%20-%20BLACKPINK,%20Selena%20Gomez%201080p%20HD-(BigMusic).mp4'\n","# !wget 'https://dl.espressif.com/dl/audio/ff-16b-2c-44100hz.mp3'        #stereo\n","# !wget 'https://www.easygifanimator.net/images/samples/video-to-gif-sample.gif'              #animated\n","# !wget 'https://mathiasbynens.be/demo/animated-webp-supported.webp'\n","# !wget 'https://file-examples-com.github.io/uploads/2020/03/file_example_WEBP_1500kB.webp'\n","# !wget 'https://file-examples-com.github.io/uploads/2017/10/file_example_GIF_1MB.gif'\n","# !wget 'https://raw.githubusercontent.com/imageio/imageio-binaries/master/images/bricks.jpg'\n","\n","\n","files = glob.glob('./*.*')\n","file_name = files[-1];   print(files,'\\n',file_name.split('/')[-1])\n","MODEL_FREQUENCY=0.5; model_cycle_coundter = 0; AUDIO_SAMPLE_RATE=22050;FPS=30\n","\n","# get video and audo iterator Dict{}\n","def get_video(file_name,duration=0.5,fps=30,res=(480,480)):\n","  vclip = moviepy.editor.ImageClip(file_name,duration=duration).resize( (res) )\n","  # video frame iterator\n","  vitr = vclip.iter_frames(fps=fps)\n","  # if audio:return [{'vitr':vitr,'fps': vclip.fps,'duration': vclip.duration}, get_audio(vclip, as_itr=True)]\n","  return {'vitr':vitr,'duration': vclip.duration}\n","\n","# get frames or samples\n","def get_frames(itr,n_frames=-1):\n","  '''itr: either vclip or aclip object to fetch the frames form\n","    n_frames: number of frames to fetch per get_frames() method call\n","    returns: np.ndarray frames\n","  '''\n","  i=0;frames=[]\n","  while True:\n","    if i == n_frames:break#if number of frames are reached\n","    try:frames.append(next(itr))\n","    except:break\n","    i+=1\n","  return np.asarray(frames)\n","\n","# vitr2 = get_video2(file_name)\n","vitr = get_video(file_name)['vitr']\n","\n","# frames = get_frames(vitr)\n","\n","\n","# print(frames.shape, frames[0][0][0],type(frames[0][0][0][0]))\n","code = ['''get_frames(vitr)\n","''',\n","'''cv2.cvtColor(cv2.imread(file_name), cv2.COLOR_BGR2RGB)\n","'''\n","];dcode = '''data.shape'''\n","tc(7,10)#,dcode)\n","print(frames.shape)           ; plt.imshow(frames[0])         ;plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C8xw73zvskEp","cellView":"form"},"source":["# @title Image\n","# visit here for file names: https://imageio.readthedocs.io/en/stable/standardimages.html\n","# visit: https://filesamples.com/categories/image     # all formats\n","# http://techslides.com/sample-files-for-development\n","# !wget https://www.easygifanimator.net/images/samples/video-to-gif-sample.gif              #animated\n","# !wget https://mathiasbynens.be/demo/animated-webp-supported.webp\n","# !wget https://file-examples-com.github.io/uploads/2020/03/file_example_WEBP_1500kB.webp\n","# !wget https://www.animatedimages.org/data/media/202/animated-dog-image-0175.gif\n","# !wget https://file-examples-com.github.io/uploads/2017/10/file_example_GIF_1MB.gif\n","\n","from matplotlib import image as Img\n","  #load as array\n","    im = Img.imread('a.jpg')# 4rd fastest\n","  # show\n","  plt.imshow(im)\n","  plt.show()\n","from keras.preprocessing.image import load_img,array_to_img,img_to_array,save_img\n","  #load\n","    img = load_img('kolala.jpeg')\n","  #to array\n","    img_array = img_to_array(img)#less fast than cv2\n","  #to image\n","    img_pil = array_to_img(img_array)\n","  #save\n","    save_img('Keras_kolala.png', img_array)\n","import cv2\n","  # load image\n","    # The function determines the type of an image by the content, not by the file extension.\n","    # color images, the decoded images will have the channels stored in B G R order.\n","    im = cv2.imread('kolala.jpeg')# 1st fastest \n","  # BGR <-> RGB <-> RGBA\n","    img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)# 1st fastest still#  cv2.COLOR_RGBA2RGB or vice-versa\n","  # save or write\n","    cv2.imwrite('opncv_kolala.png', img)\n","  #load video\n","  # utils\n","    # get or split channels\n","    b_channel, g_channel, r_channel = cv2.split(img)\n","    # create a channel data\n","    alpha_channel = np.ones(b_channel.shape, dtype=b_channel.dtype) * 50 #creating a dummy alpha channel image.\n","    # merge channnels\n","    img_BGRA = cv2.merge((b_channel, g_channel, r_channel, alpha_channel))\n","  # capture video frames from device or video\n","    from google.colab.patches import cv2_imshow\n","    cap = cv2.VideoCapture('./path/video.mp4')#(0) #from device\n","    if not cap.isOpened(): print('object is not opened!!')\n","    #while obj cap is opened, keep capturing\n","    while(cap.isOpened):\n","        # Capture frame-by-frame\n","        ret, frame = cap.read()\n","        # ---- Our operations on the frame come here----\n","        #gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","        # Display the resulting frame\n","        cv2_imshow('frame_name',frame)\n","        #stop capturing when \"q\" is pressed\n","        if cv2.waitKey(1) & 0xFF == ord('q'): break\n","    # When everything done, release the capture\n","    cap.release()\n","    # Destroy all the windows \n","    cv2.destroyAllWindows() \n","from PIL import Image\n","  #load image\n","    # The open() function identifies files from their contents, not their names, \n","    #returns PIL.Image object\n","    im_obj = Image.open( 'image.jpg' )\n","    #webp\n","      def webp():\n","        '''reads and returns animated webp np.array frames'''\n","        im= Image.open(file_name)\n","        images=[]; i=0\n","        while True:\n","          try:im.seek(i)  ;images.append(np.array(im))\n","          except:break\n","          i+=1\n","        if np.array(images).shape[-1] >3:images = [cv2.cvtColor(img, cv2.COLOR_RGBA2RGB) for img in images]\n","        return  np.array(images)\n","  # to np.array\n","    im = np.asarray(Image.open('a.jpg'))# 2nd fastest\n","    im = np.array(Image.open('a.jpg'))# 3nd fastest \n","  # from np.array\n","    im = Image.fromarray(--numpy.ndarray--)\n","  # get info or properties\n","    height, width, channel = im.shape\n","    Bool = im.is_animated    #if image is gif or contains many images\n","    count = im.n_frames      #if image is gif or contains many images \n","    im.seek(Nth_frame_number) # get n-th  frame form the list of images in image object; returns nothing\n","    curr_fram_pos = im.tell()  #returns current frame number in list of frames of image object\n","    w, h = im.size # resolution, width - height\n","    w, h = im.width, im.height\n","    info = im.info  #returns a dict{}\n","  # Display image\n","    im.show()\n","  # resize\n","    im.resize(1290,700)\n","  #Applying a filter to the image\n","    im_sharp = im.filter( ImageFilter.SHARPEN )\n","  #save or write\n","    im_sharp.save( 'image_sharpened.jpg', 'JPEG' )\n","import imageio\n","  import visvis as vv#visualization\n","  # NOTE: goto spesific file foramte to know the parameters like for PNG got its link\n","  #to use predefined files in imageio server\n","    # visit here for file names: https://imageio.readthedocs.io/en/stable/standardimages.html\n","    file = 'imageio:astronaut.png' #or file_name or file_url\n","  # load image\n","    arr = imageio.imread(file) #np.array\n","  # load GIFs\n","    npList = imageio.mimread(file)#  list of numpy arrays,\n","  # load 3D images\n","    arr = imageio.volread(file, format=None)\n","  # load videos\n","    npList = imageio.mvolread(uri, format=None, memtest='1GB')# memtest: give error if loading try to take more than the given memory\n","  # load any type of data using \"mode\" argument\n","  # #'?': automode, 'i':image, 'I': gifs, 'v':3D image, 'V':video\n","    arr = imageio.get_reader(file, format=None, mode='?')\n","  # show\n","    vv.volshow(vol)\n","  # RGB -> BGR (35, 197, 350, 3)\n","    npList = [cv2.cvtColor(img, cv2.COLOR_RGB2BGR) for img in npList]      \n","  # save or write\n","    imageio.imwrite(file, np_array_data, format=None)\n","    # Write multiple images to the specified file.\n","      imageio.mimwrite(file, npList, format=None)\n","    # write 3D image\n","      imageio.volwrite(file, arr, format=None)\n","    # write video\n","      imageio.mvolwrite(file, arr, format=None)\n","    # write any type of data using 'mode' argument\n","    #'?': automode, 'i':image, 'I': gifs, 'v':3D image, 'V':video\n","      imageio.get_writer(file, format=None, mode='?')\n","      # example:\n","        reader = imageio.get_reader('imageio:cockatoo.mp4')\n","        fps = reader.get_meta_data()['fps']\n","        writer = imageio.get_writer('~/cockatoo_gray.mp4', fps=fps)\n","        for im in reader:\n","          writer.append_data(im[:, :, 1])\n","        writer.close()\n","  # screenshot or copied clipboard image\n","    im_screen = imageio.imread('<screen>')\n","    im_clipboard = imageio.imread('<clipboard>')\n","  # record from devie\n","    !pip install imageio-ffmpeg\n","    import visvis as vv\n","    reader = imageio.get_reader('<video0>')\n","    t = vv.imshow(reader.get_next_data(), clim=(0, 255))\n","    for im in reader:\n","        vv.processEvents()\n","        t.SetData(im)\n","  # utils\n","    formats = imageio.show_formats()# list of supported foramts\n","    info = obj.meta# dict{} of information\n","import webp\n","  # !pip install webp\n","  # load\n","    # non-animated\n","      arr = webp.imread(file_name)#1st fastest\n","      arr = webp.load_image(file_name ,'RGB')#3nd fastest\n","    # animated\n","      def webp():#1st fastest# 2nd in non-animated\n","      '''reads and returns animated webp np.array frames'''\n","      with open(file_name, 'rb') as f:\n","        webp_data = webp.WebPData.from_buffer(f.read())\n","        dec = webp.WebPAnimDecoder.new(webp_data)\n","        images = []\n","        for arr,_ in dec.frames():  images.append(arr)\n","      if np.array(images).shape[-1] >3:images = [cv2.cvtColor(img, cv2.COLOR_RGBA2RGB) for img in images]\n","      return np.array(images)\n","      def webp():#2nd fastest\n","        '''reads and returns animated webp np.array frames'''\n","        images = [np.array(im) for im in webp.mimread(file_name)]\n","        images = [cv2.cvtColor(img, cv2.COLOR_RGBA2RGB) for img in images]\n","        return np.array(images)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfRfgdqiFNn7","cellView":"form"},"source":["# @title Audio\n","#visit: https://filesamples.com/categories/audio    #all foramts\n","#photos,songs,muted videos sd to 4k https://pixabay.com\n","# http://techslides.com/sample-files-for-development\n","#https://docs.espressif.com/projects/esp-adf/en/latest/design-guide/audio-samples.html  #all types and channels\n","# http://www0.cs.ucl.ac.uk/teaching/GZ05/samples/\n","\n","#!wget https://file-examples-com.github.io/uploads/2017/11/file_example_MP3_5MG.mp3\n","#!wget https://file-examples-com.github.io/uploads/2017/11/file_example_WAV_10MG.wav  #stereo\n","#!wget http://www0.cs.ucl.ac.uk/teaching/GZ05/samples/mask-100-550.wav                #mono\n","# !wget https://dl.espressif.com/dl/audio/ff-16b-2c-44100hz.mp3        #stereo\n","# !wget https://dl.espressif.com/dl/audio/ff-16b-1c-44100hz.mp3         #mono\n","\n","#Offline\n","  #Load audio from a file\n","    !pip install moviepy# 3rd fastest\n","      import moviepy.editor\n","      # see https://zulko.github.io/moviepy/getting_started/videoclips.html\n","      # load audio\n","        audio = AudioFileClip(\"some_audiofile.mp3\")#from audio file\n","        audio = AudioFileClip(\"some_video.avi\")#from video file\n","        #or\n","        videoclip = VideoFileClip(\"some_video.avi\")\n","        audio = videoclip.audio\n","    !pip install torchaudio# 2nd fastest\n","      import torchaudio\n","      #load\n","      waveform, sample_rate = torchaudio.load('foo.mp3')  # load tensor from file\n","      #save\n","      torchaudio.save('foo_save.mp3', waveform, sample_rate)  # save tensor to file\n","      # waveform  to  db\n","      db = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=None)(waveform)\n","    !pip install soundfile# 1st fastest loading method\n","      import soundfile as sf\n","      # load\n","        #returns numpy ((frames,channel), sample_rate)\n","          sf.read(file,#string or file obj  f (with open() as 'f')\n","                frames=-1,#in:  The number of frames to read. If frames is -negative, the whole rest of the file is read. won't work if stop is given.\n","                start=0,#int: Where to start reading. A negative value counts from the end.\n","                stop=None,  #int: The index after the last frame to be read. A negative value counts from the end. won't work if frames is given.\n","                dtype='float64',# ('float64', 'float32', 'int32', 'int16', or optional) –Data type of the returned array, by default 'float64'. Floating point audio data is typically in the range from -1.0 to 1.0. Integer data is in the range from -2**15 to 2**15-1 for 'int16' and from -2**31 to 2**31-1 for 'int32'.\n","                always_2d=False,# True:stereo-> return (nframes,nchannels)  or Flase:mono -> (nframes) \n","                fill_value=None,# fill with this value if requested size is not enough  : [1,2,3,,, file_value,,,] otherwise return [1,2,3,,,]\n","                out=None,#np.array: np.zeros((5000000,2)) return 5000000 frames of channel 2 if fill_value is give or audio file has enough lengh; If out is specified, the data is written into the given array instead of creating a new array. In this case, the arguments dtype and always_2d are silently ignored! If frames is not given, it is obtained from the length of out.\n","                # used with '.raw' files mostly, no need to use these often\n","                  samplerate=None,#set rate\n","                  channels=None,#set channels\n","                  format=None,#'wav' etc,,file extension, default is read form the file name\n","                  subtype=None,#'PCM_24' etc...\n","                  endian=None,#default:'FILE\" ('FILE', 'LITTLE', 'BIG', 'CPU', )\n","                  closefd=True)# Whether to close the file descriptor on close(). Only applicable if the file argument is a file descriptor.\n","        # from url\n","          import io\n","          from six.moves.urllib.request import urlopen\n","          url = \"https://raw.githubusercontent.com/librosa/librosa/master/tests/data/test1_44100.wav\"\n","          data, samplerate = sf.read(io.BytesIO(urlopen(url).read()))\n","        # from numpy array\n","          data = np.random.uniform(-1, 1, size=(samplerate * 10, channel))#dummy\n","        # from audio file\n","          data, samplerate = sf.read('./file.wav')\n","        # raw files\n","          data, samplerate = sf.read('./file.raw', channels=1, samplerate=44100,subtype='FLOAT')\n","        # with open\n","          with open('filename.flac', 'rb') as f: data, samplerate = sf.read(f)\n","        # load large or any audio sequenctially in blocks\n","          soundfile.blocks(file,\n","                           blocksize=None,#frames per block\n","                           overlap=0, #frames to overlap \n","                           frames=-1,# n frames to read from file\n","                           start=0,# start reading from\n","                           stop=None,#  stop reading at\n","                           dtype='float64',# dtype of returned np.array\n","                           always_2d=False,#channels\n","                           fill_value=None,#fill with this value if exceeds the file length\n","                           out=None,# np.array for each block to fit into\n","                           samplerate=None, channels=None, format=None, subtype=None, endian=None, closefd=True)\n","                           \n","          blocks = [b for b in sf.blocks('myfile.wav', blocksize=1024, overlap=512)]\n","          #or\n","          with sf.SoundFile('myfile.wav', 'r+') as f:\n","            while f.tell() < f.frames:\n","                pos = f.tell()\n","                data = f.read(1024)\n","                f.seek(pos)\n","                f.write(data*2)\n","      #write\n","        soundfile.write(file,#file name to write audio into\n","                        data,#numpy array stereo:(frames,channel) or mono: (frames); supported dtype('flaot32','flaot64','int16','int32')\n","                        samplerate= 44100\n","                        subtype=['PCM_24',# 24bit\n","                                 'vorbis' # 16bit\n","                                 ],\n","                        endian=None,\n","                        format=['ogg','flac','wav'],\n","                        closefd=True)\n","        # Write audio as 24bit PCM WAV\n","          sf.write('stereo_file.wav', data, samplerate, subtype=subtype[0])\n","        # Write audio as 16bit vorbis Flac\n","          sf.write('stereo_file.flac', data, samplerate, format=format[1], subtype=subtype[1])\n","      #utils\n","        sf.info('./mask-100-550.wav',verbose=True)#returns inforamtion about file\n","        sf.available_formats()#list of suppoerted files: flac, wav etc..\n","        sf.available_subtypes(format=None)#supported subtype for given format, else rturnn list of all sutypes\n","        sf.check_format(format, subtype=None, endian=None)#Check if the combination of format/subtype/endian is valid.\n","        sf.default_subtype(format)#Return the default subtype for a given format.\n","\n","    from pydub import AudioSegment# 4th\n","      # load\n","        sound = AudioSegment.from_wav(\"never_gonna_give_you_up.wav\")\n","        sound = AudioSegment.from_mp3(\"never_gonna_give_you_up.mp3\")\n","        sound = AudioSegment.from_ogg(\"never_gonna_give_you_up.ogg\")\n","        sound = AudioSegment.from_flv(\"never_gonna_give_you_up.flv\",frame_rate=44100, channels=2, sample_width=2)\n","        with open(\"./sound.wav\", \"rb\") as wav_file: sound = AudioSegment.from_file(wav_file, format=\"wav\")\n","        sound = AudioSegment.from_file(\"never_gonna_give_you_up.mp4\", \"mp4\")\n","        sound1 = AudioSegment.from_file(\"never_gonna_give_you_up.wma\", \"wma\")\n","        sound2 = AudioSegment.from_file(\"never_gonna_give_you_up.aiff\", \"aac\")\n","      #save\n","        file_handle = sound.export(\"/path/to/output.mp3\",\n","                                  format=\"mp3\",\n","                                  bitrate=\"192k\",\n","                                  tags={\"album\": \"The Bends\", \"artist\": \"Radiohead\"},\n","                                  cover=\"/path/to/albumcovers/radioheadthebends.jpg\")\n","      # sound1 6 dB louder, then 3.5 dB quieter\n","        louder = sound1 + 6\n","        quieter = sound1 - 3.5\n","      # sound1, with sound2 appended\n","        combined = sound1 + sound2\n","      # sound1 repeated 3 times\n","        repeated = sound1 * 3\n","      # duration in ms and seconds\n","        duration_in_milliseconds = len(sound1)\n","        sound1.duration_seconds\n","      # first 5 seconds of sound1\n","        beginning = sound1[:5000]\n","      # last 5 seconds of sound1\n","        end = sound1[-5000:]\n","      # split sound1 in 5-second slices\n","        slices = sound1[::5000]\n","      # Advanced usage, if you have raw audio data:\n","        sound = AudioSegment(data=b'…',# raw audio data (bytes)\n","                            sample_width=2,# 2 byte (16 bit) samples\n","                            frame_rate=44100,# 44.1 kHz frame rate\n","                            channels=2# stereo\n","                            )\n","      # empty audio of duration 0ms\n","        empty = AudioSegment.empty()\n","      # silent audio of duration n\n","        ten_second_silence = AudioSegment.silent(duration=10000)\n","      # to multi-channel; stereo like\n","        stereo_sound = AudioSegment.from_mono_audiosegments(L_sound1, R_sound2)\n","      # channel count; 1:mono,2:stereo,...n\n","        channel_count = sound.channels\n","      #  sample width in bits\n","        # Number of bytes in each sample (1 means 8 bit, 2 means 16 bit, etc). CD Audio is 16 bit, (sample width of 2 bytes).\n","        bytes_per_sample = sound.sample_width\n","      # framte_rate or sample_rate\n","        # CD Audio has a 44.1kHz sample rate, which means frame_rate will be 44100 (same as sample rate, see frame_width).\n","        # 11025, 12000, 24000, 22050, 44100 (CD), 48000 (DVD).\n","        frames_per_second = sound.frame_rate\n","      # frame_width in bytes\n","        # Number of bytes for each \"frame\". A frame contains a sample for each channel (so for stereo you have 2 samples per frame, which are played simultaneously). frame_width is equal to channels * sample_width. For CD Audio it'll be 4 (2 channels times 2 bytes per sample).\n","        bytes_per_frame = sound.frame_width\n","      # frame_count\n","        # Returns the number of frames in the AudioSegment. Optionally you may pass in a ms keywork argument to retrieve the number of frames in that number of milliseconds of audio in the AudioSegment (useful for slicing, etc).\n","        number_of_frames_in_sound = sound.frame_count()\n","        number_of_frames_in_200ms_of_sound = sound.frame_count(ms=200)\n","      # overlay\n","        played_togther = sound1.overlay(sound2)\n","        sound2_starts_after_delay = sound1.overlay(sound2, position=5000)\n","        volume_of_sound1_reduced_during_overlay = sound1.overlay(sound2, gain_during_overlay=-8)\n","        sound2_repeats_until_sound1_ends = sound1.overlay(sound2, loop=true)\n","        sound2_plays_twice = sound1.overlay(sound2, times=2)\n","        # assume sound1 is 30 sec long and sound2 is 5 sec long:\n","        sound2_plays_a_lot = sound1.overlay(sound2, times=10000)\n","        len(sound1) == len(sound2_plays_a_lot)\n","      # pan or shiftings L or R\n","        # pan the sound 15% to the right\n","        panned_right = sound1.pan(+0.15)\n","        # pan the sound 50% to the left\n","        panned_left = sound1.pan(-0.50)\n","      # AudioSegment to array, or np.array \n","        samples = sound.get_array_of_samples()\n","        # stereo audio would look like [sample_1_L, sample_1_R, sample_2_L, sample_2_R, …].\n","        channel_sounds = seg.split_to_mono()#split to L and R if sound is stereo  \n","        samples = [s.get_array_of_samples() for s in channel_sounds]# to array.array\n","        fp_arr = np.array(samples).T.astype(np.float32)#to np.float32\n","        fp_arr /= 2**15\n","      # np.array to an AudioSegment\n","        import io, scipy.io.wavfile\n","        wav_io = io.BytesIO()\n","        scipy.io.wavfile.write(wav_io, 16000, fp_arr)\n","        wav_io.seek(0)\n","        sound = pydub.AudioSegment.from_wav(wav_io)\n","    import librosa# too low , very slow\n","      # only mono or only streo\n","      #to mono audio\n","      y_mono, sr = librosa.load(filepath, mono=True)\n","      #not mono audio\n","      y_stereo, sr = librosa.load(filepath, mono=False)\n","      # check validiy\n","      librosa.util.valid_audio(y_mono)\n","      librosa.util.valid_audio(y_stereo)\n","      # stereo to mono\n","      y = librosa.to_mono(y)\n","      # set or change sample_rate\n","      y = librosa.resample(y, sr, 8000)\n","      # get duration of file or spectorgram or y,sr in seconds\n","      secs = librosa.get_duration(y=y, sr=sr)# or (Spectorgram_S) or (filename)\n","      # write audio save \n","      librosa.output.write_wav('file_trim_5s.wav', y, sr)\n","\n","      # get all files audio music sound samples sample from directory folder\n","      # Get all audio files in a directory sub-tree\n","      files = librosa.util.find_files('~/Music')\n","      # Look only within a specific directory, not the sub-tree\n","      files = librosa.util.find_files('~/Music', recurse=False)\n","      # Only look for mp3 files\n","      files = librosa.util.find_files('~/Music', ext='mp3')#\n","      # Or just mp3 and ogg\n","      files = librosa.util.find_files('~/Music', ext=['mp3', 'ogg'])\n","      # Only get the first 10 files\n","      files = librosa.util.find_files('~/Music', limit=10)\n","      # Or last 10 files\n","      files = librosa.util.find_files('~/Music', offset=-10)\n","  #Play audio data or file\n","    from IPython.display import Audio\n","      # play ;Can also do stereo or more channels\n","      Audio([left_audio, right_audio],rate=framerate)\n","      Audio(path, autoplay=True)\n","      Audio(data= y, rate=sr)\n","      Audio(data= ,rate= ,filename= ,url= ,autoplay= , embed=)\n","      #or \n","      os.system(\"mpg321 welcome.mp3\")\n","  #resample audio data to new sample rate\n","    !pip install samplerate\n","    import samplerate\n","    data = np.array([[-0.9787, -0.564564],[0.234324, -0.5643543]])\n","    data_channel = data.shape[-1]\n","    rate = 44100\n","    new_rate = 16000\n","    resampler = samplerate.Resampler('sinc_best', channels=data_channel)\n","    data = resampler.process(data, rate/new_rate, end_of_input=True)\n","  #Record audio from local device directly\n","    import sounddevice as sd\n","      !pip install sounddevice\n","      !sudo apt-get install libportaudio2\n","      from scipy.io.wavfile import write\n","      fs = 44100  # Sample rate\n","      seconds = 3  # Duration of recording\n","      myrecording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)#start recording\n","      sd.wait()  # Wait until recording is finished\n","      write('output.wav', fs, myrecording)  # Save as WAV file \n","      sd.query_devices(device=None, kind=None)#return availabel devices\n","  #get info like channel, samplerate, duration\n","    !pip install audioread\n","    with audioread.audio_open('file_example_WAV_10MG.wav') as f:\n","      channel , rate, duration = f.channels, f.samplerate, f.duration\n","#Online\n","  #record audio from local device on webbrowser\n","  # sending data to a server consumes internet data\n","    # all imports\n","    from IPython.display import Javascript\n","    from google.colab.output import eval_js\n","    from base64 import b64decode\n","    import numpy as np\n","    #!pip install ffmpeg-python\n","    import ffmpeg\n","\n","    RECORD = \"\"\"\n","      const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n","      const b2text = blob => new Promise(resolve => {\n","        const reader = new FileReader()\n","        reader.onloadend = e => resolve(e.srcElement.result)\n","        reader.readAsDataURL(blob)\n","      })\n","\n","      var record = time => new Promise(async resolve => {\n","        stream = await navigator.mediaDevices.getUserMedia({ audio: {sampleRate:48000,\n","                                                                  channelCount:{ideal: 2, min:1},\n","                                                                  volume:1.0\n","                                                                  }\n","                                                          })\n","        recorder = new MediaRecorder(stream)\n","        chunks = []\n","        recorder.ondataavailable = e => chunks.push(e.data)\n","        recorder.start()\n","        await sleep(time)\n","        recorder.onstop = async ()=>{\n","          blob = new Blob(chunks)\n","          text = await b2text(blob)\n","          resolve(text)\n","        }\n","        recorder.stop()\n","      })\n","      \"\"\"\n","    # returns numpy audio data\n","    def record(duration=1000,fname=None):\n","      display(Javascript(RECORD))\n","      s = eval_js('record(%d)' % (duration))\n","      bin = b64decode(s.split(',')[1])\n","\n","      process = (ffmpeg.input('pipe:0').output('pipe:1', format='wav')\n","                .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n","                )\n","      output, err = process.communicate(input=bin)\n","      \n","      riff_chunk_size = len(output) - 8\n","      # Break up the chunk size into four bytes, held in b.\n","      q = riff_chunk_size;    b = []\n","      for i in range(4):\n","          q, r = divmod(q, 256)\n","          b.append(r)\n","\n","      # Replace bytes 4:8 in proc.stdout with the actual size of the RIFF chunk.\n","      riff = output[:4] + bytes(b) + output[8:]\n","      # to nparray type uint16 bit PCM\n","      data = np.frombuffer(riff, dtype=np.int16)\n","      #save\n","      if fname:\n","        with open(fname,'wb') as f: f.write(bin)\n","      \n","      return data/ 2**15,bin#normalized in range 1 to -1\n","    #time complexity\n","      from datetime import datetime\n","      tim1 = datetime.timestamp(datetime.now())\n","      g=[]\n","      for i in range(10):\n","        data,bin = record(500)\n","        tim2 = datetime.timestamp(datetime.now())\n","        gap = tim2-tim1\n","        g.append(gap)\n","        print(i,gap)\n","        tim1 = tim2\n","      sum(g)/len(g)\n","\n","\n","#timecomplexity\n","from datetime import datetime\n","if 'avg' not in globals():avg=[]\n","def load(code:str,itr=20):\n","  global avg\n","  data = None; rate = None  ;g=[];  tim1 = datetime.timestamp(datetime.now())\n","  for i in range(itr):\n","    data = eval(rf'{str(code)}')\n","    tim2 = datetime.timestamp(datetime.now())\n","    gap = tim2-tim1\n","    g.append(gap)\n","    tim1 = tim2\n","\n","  avg.append(sum(g)/len(g))\n","  print(f'''{i+1} : {gap}   {type(data)} rate: {rate}Hz\n","        avg: {avg[-1]}\n","        Total avg: {avg}\n","        min: {min(avg)}''')\n","\n","#!pip install librosa\n","import soundfile as sf\n","load('./file_example_WAV_10MG.wav',\n","     sf.read\n","     )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qHEFutzwbz56"},"source":["#neural networks"]},{"cell_type":"code","metadata":{"id":"Rs8HTMjUOyW4","cellView":"form"},"source":["#@title Neural network key points rnn cnn \n","_______generalization\n","'dataset': train,validation set |Dev set,test, always shuffle dataset\n","'overfitting': regularization,early stopping, weight contraints, dropout, batchnormalization\n","'batch_size' = memory = accuracy\n","'Architechtures':  not big not small....need to test out all possibilities\n","'capacity': if big : regularization"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zU2ex87WZ-l","cellView":"form"},"source":["#@title keras notes taining\n","# convolution output calculatoin: out_size = ((inp_size - kernel + 2*padding)/stride) + 1\n","# de-convolution output calculatoin:\n","  # padding='valid': out_size = (inp_size-1) * stride + kernel\n","  # padding='same' : out_size = inp_size * stride\n","# filters = number of samples from the image(featurs from the image)\n","# pooling = maxpooling(get maximum number out of the kernal), averagepooling\n","# stride = after how many steps the next kernal is\n","# padding =  add zeros at the edge of the image\n","#             'valid' no padding, 'same' 123 to 12300, 'casual' ???\n","# kernal = the filter resolution like 3x3\n","# Input_shape = (30,50,50,3)\n","#               30 images of 50x50 pixels in RGB (3 channels)\n","\n","# sample = (image and lable)\n","\n","# batch_size = [num_samples]  # no. of samples per gradient.   \n","\n","# steps_per_epoch = dataset_size/batch_size \n","\n","# num_epoch = infinity\n","\n","# Verbose =  specifies verbosity mode(0 = silent, 1= progress bar, 2 = one line per epoch). \n","\n","# callbacks = call the pre-defined | custom methods during trainig a model\n","\n","# overfitting = more data smaller network(becomes too much specific)(useless knowledge)(confused wich one to pick)(too much knowledge with no utilization)\n","\n","# underfitting = less data bigger network(becomes \"i dont care, what the prediction is\")(no knowledge at all)(dont pick at all)\n","\n","\n","#  y =  harmonic (tonal) and percussive (transient) portions of the signal. Each of y_harmonic and y_percussive have the same shape and duration as y.\n","#      percussive elements tend to be stronger indicators of rhythmic content, and can help provide more stable beat tracking results; \n","#      second, percussive elements can pollute tonal feature representations (such as chroma) by contributing energy across all frequency bands, so we’d be\n","#       better off without them. \n","\n","# tempo = beates/minute\n","\n","# beate_frame |  frame numbers =  array of frame numbers corresponding to detected beat events.\n","\n","# frame = short window of the signal (y), separated by hop_length = 512 samples. ( Since v0.3, librosa uses centered frames, so that the kth frame is \n","#     centered around sample k * hop_length.)\n","\n","# mfcc = ( Mel-frequency cepstral coefficients)(matrix ) numpy.ndarray of size (n_mfcc, T) (where T denotes the track duration in frames)\n","\n","# offset = start reading after this time set\n","\n","# duration = only load up this much audio data\n","\n","# dtype = to set datatype of 'y' if need\n","\n","# hop_length = number of audio samples between successive onset_envelope values\n","\n","# time series = Typically an audio signal, denoted by y, and represented as a one-dimensional numpy.ndarray of floating-point values. y[t] corresponds to amplitude of\n","#   the waveform at sample t.\n","\n","# sampling rate = The (positive integer) number of samples per second of a time series. This is denoted by an integer variable sr.\n","\n","# frame = A short slice of a time series used for analysis purposes. This usually corresponds to a single column of a spectrogram matrix.\n","\n","# window = A vector or function used to weight samples within a frame when computing a spectrogram.\n","\n","# frame length = The (positive integer) number of samples in an analysis window (or frame). This is denoted by an integer variable n_fft.\n","\n","# hop length = The number of samples between successive frames, e.g., the columns of a spectrogram. This is denoted as a positive integer hop_length.\n","\n","# window length = The length (width) of the window function (e.g., Hann window). Note that this can be smaller than the frame length used in a\n","#    short-time Fourier transform. Typically denoted as a positive integer variable win_length.\n","\n","# spectrogram = A matrix S where the rows index frequency bins, and the columns index frames (time). Spectrograms can be either real-valued or \n","#     complex-valued. By convention, real-valued spectrograms are denoted as numpy.ndarrays S, while complex-valued STFT matrices are denoted as D.\n","\n","# onset (strength) envelope = An onset envelope onset_env[t] measures the strength of note onsets at frame t. Typically stored as a one-dimensional\n","#      numpy.ndarray of floating-point values onset_envelope.\n","\n","# chroma = Also known as pitch class profile (PCP). Chroma representations measure the amount of relative energy in each pitch class (e.g., the 12 notes\n","#      in the chromatic scale) at a given frame/time.\n","\n","\n","# to_categorical = convert our numerical labels  >> binary \n","#   if 6 classes then 3rd class is defined as [0 0 1 0 0 0]\n","\n","# datData Augmentation =  a method of artificially creating a new dataset for training from the existing\n","#  training dataset to improve the performance of deep learning neural network with the amount of data available\n","#  . It is a form of regularization which makes our model generalize better than before.\n","# Here we have used a Keras ImageDataGenerator object to apply data augmentation for randomly\n","#  translating, resizing, rotating, etc the images. Each new batch of our data is randomly adjusting\n","#   according to the parameters supplied to ImageDataGenerator.\n","\n","# model.fit()___________[ all dataset goes into RAM][ no data augumentation]\n","#  used when the entire training dataset can fit into the memory and no data augmentation is applied.\n","#   The entire training set can fit into the Random Access Memory (RAM) of the computer.\n","# Calling the model. fit method for a second time is not going to reinitialize our already trained weights, which\n","#  means we can actually make consecutive calls to fit if we want to and then manage it properly.\n","# There is no need for using the Keras generators(i.e no data argumentation)\n","# Raw data is itself used for training our network and our raw data will only fit into the memory.\n","\n","# model.fit_generator()_________[batches of dataset goes into RAM][with data augumentaion]\n","\n","# used when either we have a huge dataset to fit into our memory or when data augmentation needs to be applied.\n","\n","#  generator function(dataAugmentaion) >> batch_size >> fit_generator >> backpropagation >>updates the weights>>Start Again for nxt epoch\n","\n","#  generator = a generator whose output must be a list of the form:\n","#                       - (inputs, targets)    \n","#                       - (input, targets, sample_weights)\n","# a single output of the generator makes a single batch and hence all arrays in the list \n","# must be having the length equal to the size of the batch. The generator is expected \n","# to loop over its data infinite no. of times, it should never return or exit.\n","\n","# validation_data =  can be either:\n","#                       - an inputs and targets list\n","#                       - a generator\n","#                       - an inputs, targets and sample_weights list which can be used to evaluate\n","#                         the loss and metrics for any model after any epoch has ended.\n","\n","# validation_steps = only if the validation_data is a generator then only this argument\n","# can be used. It specifies the total number of steps taken from the generator before it is \n","# stopped at every epoch and its value is calculated as the total number of training data points\n","# in your dataset divided by the batch size.\n","\n","\n","# GRU = fast training/performance, less memory, low accuracy (short term memory)\n","\n","# LSTM = slow taining/performance, more memory, high accuracy (long term memory)\n","\n","# bidirectional LSTM = imageCaptioning (use past and future inputs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fo83ll_1mMmF","cellView":"form"},"source":["# @title Activations\r\n","tf.keras.activations.relu(x, alpha=0.0, max_value=None, threshold=0) \r\n","  # range [0,+inf]\r\n","  # values returned based on prameters.\r\n","  # default: any value less than threshold will be set to 0.\r\n","tf.keras.layers.LeakyReLU(alpha=0.3, **kwargs)  # range [-inf,+inf]\r\n","\r\n","tf.keras.activations.sigmoid(x)           # range [0,1]\r\n","tf.keras.activations.softmax(x, axis=-1)  # range [0,1]\r\n","tf.keras.activations.softplus(x)\r\n","tf.keras.activations.softsign(x)\r\n","tf.keras.activations.tanh(x)              # range [-1,1]\r\n","tf.keras.activations.selu(x)\r\n","tf.keras.activations.elu(x, alpha=1.0)\r\n","tf.keras.activations.exponential(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L3yzbfrcBhen","cellView":"form"},"source":["#@title Layer Customization\n","#Use Layer class when:\n","  # your class is just a block in a bigger system.\n","  # if you are writing custom training & saving code.\n","\n","class CustomLayer(tf.keras.layers.Layer):\n","    def __init__(self,\n","                 units=32,\n","                 **kwargs#kernal,stride,name etc..\n","                 ):\n","        super(CustomLayer, self).__init__(**kwargs)\n","        #features\n","        self.units = units\n","        #layers           init the layers\n","        self.layer1 = AnotherLayer_or_Custom_layer(32)\n","        self.layer2 = AnotherLayer_or_Custom_layer(32)\n","    #initialization goes here; this is optional as all this can also be done in __init__() method.\n","    def build(self, input_shape=32):\n","        # method 1______________________\n","        #init the layer weights \n","        w_init = tf.random_normal_initializer()\n","        #weights\n","        self.w = tf.Variable(\n","            initial_value=w_init(shape=(input_shape, self.units), dtype=\"float32\"),\n","            trainable=True,\n","            )\n","        #init the layer bias or kernel\n","        b_init = tf.zeros_initializer()\n","        #bias or kernel\n","        self.b = tf.Variable(\n","            initial_value=b_init(shape=(self.units,), dtype=\"float32\"),\n","            trainable=True\n","            )\n","        # method 2_________________add_weight()\n","        #weights\n","        self.w = self.add_weight(\n","            shape=(input_shape[-1], self.units),\n","            initializer=\"random_normal\",\n","            trainable=True,\n","        )\n","        #bias or kernel\n","        self.b = self.add_weight(\n","            shape=(self.units,),\n","            initializer=\"random_normal\",\n","            trainable=True\n","        )\n","    #calcualtion goes here\n","    def call(self, inputs,\n","             training=None,     #whether the layer should behave in training mode and inference mode.\n","             mask=None,         #You will find it in all Keras RNN layers. A mask is a boolean tensor (one boolean value per timestep in the input) used to skip certain input timesteps when processing timeseries data.\n","                                #Keras will automatically pass the correct mask argument to __call__() for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the Embedding layer configured with mask_zero=True, and the Masking layer.\n","             **kwargs):\n","      #custom matrics (optional)\n","      #calculate accuracy matrics using custom or predifined metrics funcions()\n","      calc_acc = keras.metrics.BinaryAccuracy()(targets, logits, sample_weights)#this can be used is Gradiant_tap()\n","      self.add_metric(calc_acc, name=\"accuracy\")\n","      #custom loss (optional)\n","      #calculate loss using custom or predifined loss funcions()\n","      calc_loss = inputs * 1e-2# this can also be defined in __init__()\n","      self.add_loss(calc_loss)#this can be used in Gradiant_tap()\n","      #method 1______\n","      #call initialized layers\n","      x = self.layer1(inputs)\n","      x = tf.nn.relu(x)\n","      output = self.layer2(x)\n","      return output\n","      #method 2_________\n","      if training:  #during training\n","        \"\"\"y = w.x + b\"\"\"\n","        return tf.matmul(inputs, self.w) + self.b\n","      else:   #during inference or evalution or testing\n","        return inputs\n","      #method 3_________\n","      \"\"\"y = w.x + b\"\"\"\n","      return tf.matmul(inputs, self.w) + self.b\n","    \n","    \n","    #save/load a custom model or layer , or a subclassed model\n","    #saves layer or model configurations\n","    #A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration.\n","    def get_config(self):\n","        config = super(CustomLayer, self).get_config()#this addes the {tranable,layer_name,dtype} to the configuraton\n","        #dictionary maping to init. new and fresh layer or model\n","        config.update({\"units\": self.units})#this layer or model's variables to init.new layer or model\n","        return config\n","    #or optional\n","    #loads layer or model configurations\n","    @classmethod\n","    def from_config(cls, config):\n","      return cls(**config)\n","\n","layer = CustomLayer(64)#__init__() and build() get called\n","layer.add_loss(calc_loss)\n","layer.add_matrics(calc_matrics,name=\"std_of_activation\", aggregation=\"mean\"))\n","x = layer(input)#call() get called\n","\n","config = model_or_layer.get_config()#get_config() get called\n","#sets the layer or model init. values form the config.\n","new_layer = Dense.from_config(config, custom_objects={\"CustomLayer\": CustomLayer})\n","new_model = Model.from_config(config, custom_objects={\"CustomLayer\": CustomLayer})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUNACAyJCR0f","cellView":"form"},"source":["#@title Model Customization\n","#________ model as a layer and a set of multiple models is called \"ensembling\".\n","#model1\n","input = shape()\n","x = preprocess()(input)\n","output = (x)\n","model1 = Model(input,output)\n","#model2\n","input = shape()\n","x = model1(input)\n","output = (x)\n","model2 = Model(input,output)\n","\n","#custom model\n","#Use Model class when need to:\n","  # call fit() on it\n","  # call save() on it\n","class CustomModel(keras.Model):\n","    def __init__(self,\n","                 **kargs#inputs,outputs,name\n","                 ):\n","      super(CustomModel,self).__init__(**kargs)\n","      #layers goes here\n","      #same as layer class definiton\n","      layer1 = Dense(10)\n","      layer2 = Dense(20)\n","    def call(self,inputs,training=None):\n","      #computations goes here\n","      #same as layer class definiton\n","      x = layer1(inputs)\n","      #self.add_loss(calc_loss)#like in layer class\n","      #self.add_matrics(calc_matrics)#like in layer class\n","      return layer2(x)\n","    #custom compile\n","     def compile(optimizer,loss=None,metrics=None):\n","        super(CustomModel, self).compile()\n","        self.optimizer = optimizer\n","        self.compiled_loss = loss\n","        self.compiled_metrics = metrics\n","    #custom train_step() with GradientTape()\n","    def train_step(self, data):\n","        # Unpack the data. Its structure depends on your model and\n","        # on what you pass to `fit()`.\n","        sample_weight=None\n","        #if sample_weight is provided\n","        if len(data) == 3:\n","            x, y, sample_weight = data\n","        #if sample_weight is not provided and dataset have labels also\n","        elif len(data) == 2:\n","            x, y = data\n","        #if dataset doesnot have any labels\n","        else: x = y = data\n","        #GradientTap() scope\n","        #It enables you to retrieve the gradients of the trainable weights of the layer with respect to a loss value\n","        with tf.GradientTape() as tape:\n","            # Forward pass\n","            ## Compute predictions and training=True as we're trainig the layer or model on this data\n","            y_pred = self(x, training=True)\n","            # Compute the loss value.\n","            # use self.compiled_loss() when loss is compiled with compile(optimizer,loss,metrics).\n","\n","            loss = self.compiled_loss(y, y_pred,\n","                                      sample_weight=sample_weight,\n","                                      regularization_losses=self.losses)\n","            #add losses\n","            loss += sum(self.losses)\n","\n","        # Compute gradients\n","        gradients = tape.gradient(loss, self.trainable_variables)\n","\n","        # Update weights\n","        #Using an optimizer instance, you can use gradients to update the weight variables.\n","        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","\n","        # Update the metrics.\n","        # use self.compiled_metrics() when metrics is compiled with compile(optimizer,loss,metrics).\n","        self.compiled_metrics.update_state(y, y_pred, sample_weight=sample_weight)\n","\n","        # Return a dict mapping metric names to current value.\n","        # Note that it will include the loss (tracked in self.metrics).\n","        return {i.name: i.result() for i in self.metrics}\n","        #or for resulting values only\n","        return   self.metrics.result()\n","    #evalutaion\n","    def test_step(self, data):\n","      # Unpack the data\n","      x, y = data\n","      # Compute predictions and training=False as we're not trainig the layer or model on this data\n","      y_pred = self(x, training=False)\n","      # Compute the loss value.\n","      # Updates the metrics tracking the loss\n","      self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n","      # Update the metrics.\n","      self.compiled_metrics.update_state(y, y_pred)\n","      # Return a dict mapping metric names to current value.\n","      # Note that it will include the loss (tracked in self.metrics).\n","      return {i.name: i.result() for i in self.metrics}\n","      #or for resulting values only\n","      return   self.metrics.result()\n","\n","# Construct and compile an instance of CustomModel\n","inputs = keras.Input(shape=(32,))\n","outputs = keras.layers.Dense(1)(inputs)\n","model = CustomModel(inputs, outputs)#__init__() and build() get called\n","logits = model(x)#call() get called\n","model.add_loss(calc_loss)\n","model.add_matrics(calc_matrics,name=\"std_of_activation\", aggregation=\"mean\")\n","\n","model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n","model.fit(x, y, sample_weight=sw, epochs=10,shuffle=False)#train_step() get called\n","model.evaluate(x,y,batch_size=32)#test_step() get called\n","model.predict(x)\n","\n","#utils\n","  model.summary(line_length=None,# Total length of printed lines (e.g. set this to adapt the display to different terminal window sizes).\n","                positions=None,# Relative or absolute positions of log elements in each line. If not provided, defaults to [.33, .55, .67, 1.].\n","                print_fn=None# Print function to use. Defaults to print. It will be called on each line of the summary. You can set it to a custom function in order to capture the string summary.)\n","                )\n","  model.get_layer(name=None,#string name\n","                  index=None)#int index\n","    #If name and index are both provided, index will take precedence. Indices are based on order of horizontal graph traversal (bottom-up)."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kk7Hh0OgI_Qo","cellView":"form"},"source":["#@title Gradiant_Tap() or training_step()\n","#steps\n","# We open a for loop that iterates over epochs\n","# For each epoch, we open a for loop that iterates over the dataset, in batches\n","# For each batch, we open a GradientTape() scope\n","# Inside this scope, we call the model (forward pass) and compute the loss\n","# Outside the scope, we retrieve the gradients of the weights of the model with regard to the loss\n","# Finally, we use the optimizer to update the weights of the model based on the gradients\n","#init metrics,lass, optimizer\n","accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","#Use this when customising train_step()\n","@tf.function\n","def train_step(*data):\n","  x,y = data\n","  sample_weight=None\n","  # Open a GradientTape.\n","  with tf.GradientTape() as tape:\n","    # Forward pass.\n","    logits = layer_or_model(x)      #logists or predictions\n","\n","    # Loss value for this batch.\n","    loss = loss_fn(y, logits,sample_weight)\n","\n","    # add custom loss added  in layer_or_model() with self.add_loss() in call() or via layer_or_model.add_loss(calc_loss); see CustomLayer() for details.\n","    loss += sum(layer_or_model.losses)\n","\n","  # Update the state of the `accuracy` metric.\n","  accuracy.update_state(y, logits)\n","  \n","  # Compute or Get gradients of weights wrt the loss.\n","  gradients = tape.gradient(loss, layer_or_model.trainable_weights)\n","\n","  # Update the weights of our layer_or_model.\n","  #Apply gradients to variables.\n","  optimizer.apply_gradients(\n","      zip(gradients, layer_or_model.trainable_weights)#List of (gradient, variable) pairs.\n","      name=None,# Optional name for the returned operation. Default to the name passed to the Optimizer constructor.\n","      experimental_aggregate_gradients=True# Whether to sum gradients from different replicas in the presense of tf.distribute.Strategy. If False, it's user responsibility to aggregate the gradients. Default to True.\n","      )\n","  \n","  return loss, accuracy.result()\n","#Use this when not using model.fit()\n","for epoch in range(2):\n","  # Iterate over the batches of the dataset.\n","  for step, (x, y) in enumerate(dataset):\n","    loss,matric_accuracy = train_step(x,y)\n","    # Logging.\n","    if step % 100 == 0:\n","      print(\"Step:\", step, \"Loss:\", float(loss))\n","      print(\"Total running accuracy so far: %.3f\" % matric_accuracy)\n","  # ##WARNING##: Reset the metric's state at the end of epoch only when using custom train_step()\n","  accuracy.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_JaDvajDWJs","cellView":"form"},"source":["#@title Optimizer and Loss , Matrics Customization\n","#________________matric\n","#custom\n","class Custom(keras.metrics.Metric):\n","    # in which you will create state variables for your metric.\n","    #Instantiate the metric object:accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n","    def __init__(self, name=\"categorical_true_positives\", **kwargs):\n","        super(Custom, self).__init__(name=name, **kwargs)\n","        self.true_positives = self.add_weight(name=\"ctp\", initializer=\"zeros\")\n","    # which uses the targets y_true and the model predictions y_pred to update the state variables.\n","    #Call for each batch of data:accuracy.update_state(y, logits)\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","        y_pred = tf.reshape(tf.argmax(y_pred, axis=1), shape=(-1, 1))\n","        values = tf.cast(y_true, \"int32\") == tf.cast(y_pred, \"int32\")\n","        values = tf.cast(values, \"float32\")\n","        if sample_weight is not None:\n","            sample_weight = tf.cast(sample_weight, \"float32\")\n","            values = tf.multiply(values, sample_weight)\n","        self.true_positives.assign_add(tf.reduce_sum(values))\n","    # which uses the state variables to compute the final results.\n","    #Query its result : accuracy.result()\n","    def result(self): return self.true_positives\n","    # The state of the metric will be reset at the start of each epoch.\n","    #Reset the metric's state at the end of an epoch or at the start of an evaluation: accuracy.reset_states()\n","    def reset_states(self): self.true_positives.assign(0.0)\n","#predefind\n","accuracy_fn = keras.metrics.BinaryAccuracy()\n","#or\n","accuracy_fn = Custom()\n","#use\n","acc = accuracy_fn(targets, logits, sample_weights)\n","model_or_layer.add_metric(acc,\n","                          name=\"accuracy\",\n","                          aggregation=\"mean\")\n","#or\n","model.compile(metrics=[Custom()])\n","\n","#________________loss\n","#The purpose of loss functions is to compute the quantity that a model should seek to minimize during training.\n","#custom\n","class Custom(keras.losses.Loss):\n","    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n","        super().__init__(name=name)\n","        self.regularization_factor = regularization_factor\n","    #calculation\n","    def call(self, y_true, y_pred,sample_weight=None):\n","        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n","        reg = tf.math.reduce_mean(tf.square(0.5 - y_pred))\n","        return mse + reg * self.regularization_factor\n","#predefined\n","#if not using 'softmax' for 'Categorical Crossentropy' or 'SparseCategoricalCrossentropy' \n","# or not using 'sigmoid' for 'Binary Crossentropy'\n","# then use: from_logits=True \n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","#or\n","loss_fn=Custom()\n","#use\n","loss = loss_fn(targets, logits, sample_weights)\n","model_or_layer.add_loss(loss)\n","#or\n","model.compile(optimizer='Adam', loss=Custom())\n","#________________optimizer\n","#custom\n",">>>\n","#predefind\n","opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","#or\n","lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n","    initial_learning_rate=1e-2,\n","    decay_steps=10000,\n","    decay_rate=0.9)\n","opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n","#USE\n","model.compile(optimizer=opt,#by class reference\n","              #or\n","              optimizer='Adam')#by string name"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-XcNhSJcDlBo","cellView":"form"},"source":["#@title complie()\n","model.compile(\n","    #optimizer\n","      # controls the learning rate value\n","      # string name or optimizer instance\n","      optimizer=keras.optimizers.RMSprop(1e-3),#by class reference\n","      #or\n","      optimizer=\"adam\",#by string name\n","    #loss\n","      #string name of objective fun(), objective fun() or loss instance\n","      # If the model has multiple outputs, you can use a different loss on each output by passing a dictionary or a list of losses. The loss value that will be minimized by the model will then be the sum of all individual losses\n","      loss=['BinaryCrossentropy',#by string name\n","            keras.losses.CategoricalCrossentropy(from_logits=True),#by class reference\n","            None # no loss computation; only for prediction, not for training\n","          ],\n","      #or\n","      loss={\"output_1\": 'BinaryCrossentropy',#by string name\n","            \"output_2\": keras.losses.CategoricalCrossentropy(from_logits=True),#by class reference\n","            #\"output_3\" is omitted in this version for no loss computation; only for prediction, not for training\n","            },   \n","    #loss_weights\n","      #  scalar coefficients (Python floats) \n","      #  loss contributions of different model outputs\n","      loss_weights=[1.0,#give more importance to this\n","                    0.2,\n","                    ],\n","      #or\n","      loss_weights={\"output_1\": 2.0,#give more importance to this\n","                    \"output_2\": 1.0},\n","    #metrics\n","      # a list or dict of string name, fun(), or instance\n","      metrics=[['SparseCategoricalAccuracy'],#by string name\n","              [keras.metrics.MeanAbsolutePercentageError()]#by class reference\n","              ],    \n","      #or\n","      metrics = {\"output_1\":['SparseCategoricalAccuracy'],#by string name\n","                \"output_2\":[keras.metrics.MeanAbsolutePercentageError()]#by class reference\n","                },\n","    # weighted_metrics\n","      #  List of metrics to be evaluated and weighted by sample_weight or class_weight during training and testing.\n","      weighted_metrics=None,\n","    # run_eagerly\n","      # Bool. Defaults to False.\n","      # If True, this Model's logic will not be wrapped in a tf.function.\n","      run_eagerly=None,\n","    # steps_per_execution\n","      # Int. Defaults to 1.\n","      # The number of batches to run during each tf.function call.\n","      # Running multiple batches inside a single tf.function call can \n","      # greatly improve performance on TPUs or small models with a large\n","      # python overhead. At most, one full epoch will be run each execution.\n","      # If a number larger than the size of the epoch is passed, the execution\n","      # will be truncated to the size of the epoch.\n","      # Note that if steps_per_execution is set to N, Callback.on_batch_begin \n","      # and Callback.on_batch_end methods will only be called every \n","      # N batches (i.e. before/after each tf.function execution).\n","      steps_per_execution=None,\n","    # **kwargs\n","      **kwargs\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D_iWkIaDiBhp","cellView":"form"},"source":["#@title fit()\n","#for mapping the tf.data\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (\n","        {\"input_1\": data_1, \"input_2\": data_2,\"input_3\":data_3},\n","        {\"output_1\": label_1, \"output_2\": label_2},\n","    )\n",")\n","medel.fit(#data x\n","            # numpy array,tf.tensor,dict maping input names,tf.data, generator\n","            # a list of the above datatypes for multi-inputs\n","            x = {\"input_1\": data_1, \"input_2\": data_2, \"input_3\": data_3},\n","          #label y\n","            #same as  data x\n","            y = {\"output_1\": label_1, \"output_2\": label_2},\n","          #initial_epoch\n","            #start training from this epoch\n","            #Integer. Epoch at which to start training (useful for resuming a previous training run).\n","            initial_epoch=0,\n","          #epochs\n","            # end trainig at this epoch\n","            # an iteration over the entire x and y data provided\n","            epochs=1,\n","          #batch_size\n","            # Number of samples per gradient update\n","            # this arguments doesnot support tf.data,generator like data inputs\n","            batch_size=32,\n","          #verbose\n","            # 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended when not running interactively (eg, in a production environment).\n","            verbose = 1,\n","          #callbacks\n","            #list of callbacks\n","            callbacks=None,\n","          # shuffle\n","            #bool value shuffles data before each epoch\n","            #not used with any generator and steps_per_epoch \n","            shuffle=True,\n","          # steps_per_epoch = None\n","            # number of batches before declaring one epoch finished and starting the next epoch.\n","            # not used with np.array inputs\n","            # if input is tf.data then the value is None and will run untill the tf.data is exhausted. so,just use some value here.\n","            steps_per_epoch = len(data)//batch_size,\n","          # validation_steps\n","            #to ensure that same data is used every time.\n","            # only used with validation_data and of type tf.data\n","            # number of batches to be used for validation at end of epoch.\n","            #if None, then validation will use entire validation_data\n","            validation_steps = 10,\n","          # validation_data\n","            # validation_data will override validation_split.\n","            #runs at end of each epoch\n","            #(data,label)of numpy or tf.tensor or (,,sample_weight)\n","            validation_data = None,\n","          # validation_split\n","            # not used with tf.data or generator\n","            # value[0,1]\n","            #runs at end of each epoch\n","            validation_split=0.2,# percentage of data tobe used for validation\n","          # validation_batch_size\n","            # Number of samples per validation batch\n","            # not used with tf.data or any generator as they generate their own batches implicitly.\n","            validation_batch_size =None,\n","          # validation_freq\n","            #int: 2 means  runs validation every 2 epochs.\n","            #collections_abc.Container (list or tuple): (2,4,10)  runs validation (2nd,4th,10th) epoch.\n","            #only used with validation_data\n","            validation_freq=1,\n","          # class_weight\n","            #With the default settings the weight of a sample is decided by its frequency in the dataset.\n","            #{class_index:imporatance_value}\n","            #used during trainig only\n","            class_weight = {0:1.0,\n","                            1:1.0,\n","                            2:3.0#more importance to this class bcoz this class having less data\n","                            } ,\n","          # sample_weight\n","            #A \"sample weights\" array is an array of numbers that specify how much weight each sample in a batch should have in computing the total loss. It is commonly used in imbalanced classification problems (the idea being to give more weight to rarely-seen classes).\n","            # not used with tf.data or any generator\n","              #but this can be used as providing sample_weight as 3rd argument of the genrator.\n","              #dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train, sample_weight))\n","            #used during trainig only\n","            sample_weight = np.array[1.0,1.0,2.0,1.0,...1.0],#assiging importance to each sample of the entire dataset\n","          # max_queue_size\n","            #used with generator only\n","            #number of batches cached on cpu to feed gpu\n","            max_queue_size=10,\n","          # workers\n","            #used with generators only\n","            #Maximum number of processes to spin up when using process-based threading.\n","            #If unspecified, workers will default to 1. If 0, will execute the generator on the main thread.\n","            workers=1,\n","          # use_multiprocessing\n","            #used with generater only\n","            #If True, use process-based threading\n","            #this implementation relies on multiprocessing, you should not pass non-picklable arguments to the generator as they can't be passed easily to children processes.\n","            use_multiprocessing=False,\n","\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4cC7_TpOssSP","cellView":"form"},"source":["#@title evalute()\n","Model.evaluate(\n","    x=None,\n","      #same as input x to the model.fit()\n","    y=None,\n","      #same as label y in model.fit()\n","    batch_size=None,\n","      #not used with generator as they have their own batch_size\n","      #default:32 if not a generator\n","    verbose=1,\n","      #same as model.fit()\n","    sample_weight=None,\n","      #not used with genertors as generator has their own (x,y,sample_weight)\n","    steps=None,\n","      #number of batches before declaring the evalution step is finished.\n","      #not supported with np.array\n","    callbacks=None,\n","      #a list of callbacks\n","    max_queue_size=10,\n","      #used with generators only\n","      #\n","    workers=1,\n","      #used with generators\n","      #same as model.fit()\n","    use_multiprocessing=False,\n","      #same as model.fit()\n","    return_dict=False,\n","      #If True, loss and metric results are returned as a dict, with each key being the name of the metric.\n","      #If False, they are returned as a list.\n",")\n","#returns the loss and metrics values\n","model.metrics_names\n","  #will give you the display labels for the scalar outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uICSGOnzwivg","cellView":"form"},"source":["#@title preditc()\n","#Computation is done in batches. This method is designed for performance in large scale inputs.\n","#For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution\n","  model(x) or  model(x, training=False)# if you have layers such as tf.keras.layers.BatchNormalization that behaves differently during inference.\n","\n","Model.predict(\n","    x,\n","      #same as model.fit()\n","    batch_size=None,\n","      #same as model.fit()\n","    verbose=0,\n","      #0, or 1 ; none or progressBar\n","    steps=None,\n","      #If x is a tf.data dataset and steps is None, predict will use entire tf.data dataset.\n","    callbacks=None,\n","    max_queue_size=10,\n","      #use with genrators only\n","    workers=1,\n","      #same as model.fit()\n","    use_multiprocessing=False,\n","      #same as model.fit()\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-8I43tTD9qx","cellView":"form"},"source":["#@title CallBacks\n","#used in\n","  # Model.fit()\n","  # Model.evaluate()\n","  # Model.predict()\n","\n","#logs: A dict contains the loss value, and all the metrics at the end of a batch or epoch. \n","#self.model: callbacks have access to the model associated with the current round of training/evaluation/inference.\n","  #self.model.stop_training = True:to immediately interrupt training.\n","  #self.model.optimizer: Mutate hyperparameters of the optimizer like self.model.optimizer.learning_rate = 1e-3.\n","  #Save the model at period intervals.\n","  #Record the output of model.predict() on a few test samples at the end of each epoch, to use as a sanity check during training.\n","  #Extract visualizations of intermediate features at the end of each epoch, to monitor what the model is learning over time.\n","  #etc.\n","\n","class CustomCallback(keras.callbacks.Callback):\n","  # Global Methods; Called at the beginning or end of fit/evaluate/predict.\n","    def on_train_begin(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Starting training; got log keys: {}\".format(keys))\n","\n","    def on_train_end(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Stop training; got log keys: {}\".format(keys))\n","\n","    def on_test_begin(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Start testing; got log keys: {}\".format(keys))\n","\n","    def on_test_end(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Stop testing; got log keys: {}\".format(keys))\n","\n","    def on_predict_begin(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Start predicting; got log keys: {}\".format(keys))\n","\n","    def on_predict_end(self, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Stop predicting; got log keys: {}\".format(keys))\n","  # Batch-level methods; Called right before processing a batch or at the end of training/testing/predicting a batch. Within this method, logs is a dict containing the metrics results.\n","    def on_train_batch_begin(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n","\n","    def on_train_batch_end(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n","\n","    def on_test_batch_begin(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n","\n","    def on_test_batch_end(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n","\n","    def on_predict_batch_begin(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n","\n","    def on_predict_batch_end(self, batch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n","  # Epoch-level methods (training only); Called at the beginning and end of an epoch during training.\n","    def on_epoch_begin(self, epoch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n","\n","#USE\n","callbacks=[CustomCallback()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t_Y8B-a2Gpka","cellView":"form"},"source":["#@title debug eger_execution=True\n","tf.keras.Model.run_eagerly\n","  #Settable attribute indicating whether the model should run eagerly.\n","  #Running eagerly means that your model will be run step by step, like Python code. Your model might run slower, but it should become easier for you to debug it by stepping into individual layer calls.\n","  #By default, we will attempt to compile your model to a static graph to deliver the best execution performance."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgPLoS-kG8bM","cellView":"form"},"source":["#@title distrubuted trianig\n","#Sub-Types\n","#1. data parallism:   where a single model gets replicated on multiple devices or multiple machines. Each of them processes different batches of data, then they merge their results. There exist many variants of this setup, that differ in how the different model replicas merge results, in whether they stay in sync at every batch or whether they are more loosely coupled, etc.\n","      global_batch = num_devices*local_batch #8 GPUs * 32 sampels = 256\n","      device1[model(local_batch1)]\n","      device2[model(local_batch2)]\n","      device3[model(local_batch3)] \n","  \n","  #steps\n","    # Create a MirroredStrategy.\n","      strategy = tf.distribute.MirroredStrategy()\n","      print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n","    # Open a strategy scope.\n","      with strategy.scope():\n","        # Everything that creates variables should be under the strategy scope.\n","        # In general this is only model construction & `compile()`.\n","        model = get_model()\n","        model.compile(...)\n","\n","    # Train the model on all available devices.\n","      train_dataset, val_dataset, test_dataset = get_dataset()\n","      model.fit(train_dataset, epochs=2, validation_data=val_dataset)\n","\n","    # Test the model on all available devices.\n","      model.evaluate(test_dataset)\n","#2. model or device parallism:   where different parts of a single model run on different devices, processing a single batch of data together. This works best with models that have a naturally-parallel architecture, such as models that feature multiple branches.\n","      device1[model_part1(batch)]\n","      device2[model_part2(batch)]\n","      device3[model_part3(batch)]\n","  # Model where a shared LSTM is used to encode two different sequences in parallel\n","  input_a = keras.Input(shape=(140, 256))\n","  input_b = keras.Input(shape=(140, 256))\n","  shared_lstm = keras.layers.LSTM(64)\n","\n","  # input_a on gpu_0\n","  with tf.device_scope('/gpu:0'): encoded_a = shared_lstm(input_a)\n","  # input_b on gpu_1\n","  with tf.device_scope('/gpu:1'): encoded_b = shared_lstm(input_b)\n","  # Concatenate results on CPU\n","  with tf.device_scope('/cpu:0'): merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n","\n","# ___________________TPU\n","  # Make sure your dataset yields batches with a fixed static shape. A TPU graph can only process inputs with a constant shape.\n","  # Make sure you are able to read your data fast enough to keep the TPU utilized. Using the TFRecord format to store your data may be a good idea.\n","  # Consider running multiple steps of gradient descent per graph execution in order to keep the TPU utilized. You can do this via the experimental_steps_per_execution argument compile(). It will yield a significant speed up for small models.\n","\n","  # TPU detection\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n","    print('Running on TPU: ', tpu.cluster_spec().as_dict()['worker'])\n","    tf.config.experimental_connect_to_cluster(tpu)\n","    tf.tpu.experimental.initialize_tpu_system(tpu)\n","    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","    print('Replicas: ', strategy.num_replicas_in_sync)\n","\n","  with strategy.scope():\n","      # create and compile model in this scope here\n","#Types\n","# single machine multi-GPUs (single host, multi-device training)\n","  # On multiple GPUs (typically 2 to 8) installed on a single machine (single host, multi-device training). This is the most common setup for researchers and small-scale industry workflows.\n","# cluster of computers or multi-machines (multi-worker distributed training)\n","  #homogeneous cluster: same number of GPUs\n","  # On a cluster of many machines, each hosting one or multiple GPUs (multi-worker distributed training). This is a good setup for large-scale industry workflows, e.g. training high-resolution image classification models on tens of millions of images using 20-100 GPUs.\n","  # Distributed training is somewhat more involved than single-machine multi-device training. Roughly, you will need to launch a remote cluster of machines, then run your code on a \"chief\" machine that holds a TF_CONFIG environment variable that specifies how to communicate with the other machines in the cluster. From there, the workflow is similar to using single-machine multi-GPU training, with the main difference being that you will use MultiWorkerMirroredStrategy as your distribution strategy.\n","  # Importantly, you should:\n","    # Make sure your dataset is so configured that all workers in the cluster are able to efficiently pull data from it (e.g. if your custer in on GCP, it's a good idea to host your data on GCS).\n","    # Make sure your training is fault-tolerant (e.g. by configuring a ModelCheckpoint callback).\n","  #steps\n","    # Set up a cluster of machines or computes.\n","    # Set up an appropriate TF_CONFIG environment variable on each worker. This tells the worker what its role is and how to communicate with its peers.\n","      # The TF_CONFIG environment variable is a JSON string that specifies:\n","        # The cluster configuration, while the list of addresses & ports of the machines that make up the cluster\n","        # The worker's \"task\", which is the role that this specific machine has to play within the cluster.\n","        os.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': [\"localhost:12345\", \"localhost:23456\"]},\n","                                              'task': {'type': 'worker', 'index': 0}\n","                                              })\n","        #task types: \"worker\" or \"evaluator\"\n","        #For example, if you have 8 machines with 4 GPUs each, you could have 7 workers and 1 evaluator.\n","          # workers:\n","            # train the model, each one processing sub-batches of a global batch.\n","            # One of the workers (worker 0) will serve as \"chief\", a particular kind of worker that is responsible for saving logs and checkpoints for later reuse (typically to a Cloud storage location).\n","          # evaluator:\n","            # runs a continuous loop that loads the latest checkpoint saved by the chief worker, runs evaluation on it (asynchronously from the other workers) and writes evaluation logs (e.g. TensorBoard logs).\n","    # On each worker, run your model construction & compilation code within the scope of a MultiWorkerMirroredStrategy object, similarly to we did for single-host training.\n","    # Run evaluation code on a designated evaluator machine.\n","  #training code:________________________\n","    # on each worker (including the chief).\n","    # Each worker would run the same code , including the same callbacks except the Callbacks that save model checkpoints or logs should save to a different directory for each worker. It is standard practice that all workers should save to local disk (which is typically temporary), except worker 0, which would save TensorBoard logs checkpoints to a Cloud storage location for later access & reuse\n","    \n","    # On the chief (worker 0):\n","      # Set TF_CONFIG\n","      #chief workder\n","      worker_index = 0\n","      os.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': [\"localhost:12345\", \"localhost:23456\"]},\n","                                            'task': {'type': 'worker', 'index': worker_index}\n","                                            })\n","      # strategy scope.\n","      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n","      with strategy.scope():\n","        #create/restore the model\n","        model = make_or_restore_model()\n","      #callbacks\n","      callbacks = [# This callback saves a SavedModel every 100 batches\n","                   keras.callbacks.ModelCheckpoint(filepath='path/to/cloud/location/ckpt',\n","                                                   save_freq=100),#save after every 100 batches\n","                   keras.callbacks.TensorBoard('path/to/cloud/location/tb/')\n","                   ]\n","      #start training\n","      model.fit(train_dataset,\n","                callbacks=callbacks,\n","                )\n","    # On other workers:\n","      # Set TF_CONFIG\n","      worker_index = 1\n","      os.environ['TF_CONFIG'] = json.dumps({'cluster': {'worker': [\"localhost:12345\", \"localhost:23456\"]},\n","                                            'task': {'type': 'worker', 'index': worker_index}\n","                                            })\n","      # strategy scope\n","      strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n","      # You can restore from the checkpoint saved by the chief.\n","      with strategy.scope():\n","        # create/restore the model.\n","        model = make_or_restore_model()\n","      #callbacks\n","      callbacks = [keras.callbacks.ModelCheckpoint(filepath='local/path/ckpt', save_freq=100),\n","                   keras.callbacks.TensorBoard('local/path/tb/')\n","                   ]\n","      #start training\n","      model.fit(train_dataset,\n","                callbacks=callbacks,\n","                ) \n","  #evalutaion code:_________________________\n","    # on the evaluator only.\n","    # # The evaluator would simply use MirroredStrategy (since it runs on a single machine and does not need to communicate with other machines) and call model.evaluate(). It would be loading the latest checkpoint saved by the chief worker to a Cloud storage location, and would save evaluation logs to the same location as the chief logs.\n","\n","    # strategy scope\n","    strategy = tf.distribute.MirroredStrategy()\n","    with strategy.scope():\n","      # Restore from the checkpoint saved by the chief.\n","      model = make_or_restore_model()  \n","\n","    results = model.evaluate(val_dataset)\n","    # Then, log the results on a shared location, write TensorBoard logs, etc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y99-nXCJIk12","cellView":"form"},"source":["#@title Finding the best model configuration with hyperparameter tuning\n","Once you have a working model, you're going to want to optimize its configuration -- architecture choices, layer sizes, etc. Human intuition can only go so far, so you'll want to leverage a systematic approach: hyperparameter search.\n","\n","You can use Keras Tuner to find the best hyperparameter for your Keras models. It's as easy as calling fit().\n","\n","Here how it works.\n","\n","First, place your model definition in a function, that takes a single hp argument. Inside this function, replace any value you want to tune with a call to hyperparameter sampling methods, e.g. hp.Int() or hp.Choice():\n","\n","def build_model(hp):\n","    inputs = keras.Input(shape=(784,))\n","    x = layers.Dense(\n","        units=hp.Int('units', min_value=32, max_value=512, step=32),\n","        activation='relu'))(inputs)\n","    outputs = layers.Dense(10, activation='softmax')(x)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(\n","        optimizer=keras.optimizers.Adam(\n","            hp.Choice('learning_rate',\n","                      values=[1e-2, 1e-3, 1e-4])),\n","        loss='sparse_categorical_crossentropy',\n","        metrics=['accuracy'])\n","    return model\n","The function should return a compiled model.\n","\n","Next, instantiate a tuner object specifying your optimization objective and other search parameters:\n","\n","import kerastuner\n","\n","tuner = kerastuner.tuners.Hyperband(\n","  build_model,\n","  objective='val_loss',\n","  max_epochs=100,\n","  max_trials=200,\n","  executions_per_trial=2,\n","  directory='my_dir')\n","Finally, start the search with the search() method, which takes the same arguments as Model.fit():\n","\n","tuner.search(dataset, validation_data=val_dataset)\n","When search is over, you can retrieve the best model(s):\n","\n","models = tuner.get_best_models(num_models=2)\n","Or print a summary of the results:\n","\n","tuner.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lwsyn10n_oe1","cellView":"form"},"source":["# @title loading and preprocessing data\n","  # Text files need to be read into string tensors, then split into words. Finally, the words need to be indexed & turned into integer tensors.\n","  # Images need to be read and decoded into integer tensors, then converted to floating point and normalized to small values (usually between 0 and 1).\n","  # CSV data needs to be parsed, with numerical features converted to floating point tensors and categorical features indexed and converted to integer tensors. Then each feature typically needs to be normalized to zero-mean and unit-variance.\n","\n","  #data should be in the form of string/int/float.\n","  #Use NumpPy arrays for small data that can be fit in memory.\n","  #Use Dataset object or Python generator for big datasets that can not be fit in memory and in distributed training.\n","\n","  # Tokenization of string data, followed by token indexing.\n","  # Feature normalization.\n","  # Image rescaling, cropping, or image data augmentation;Rescaling the data to small values (in general, input values to a neural network should be close to zero -- typically we expect either data with zero-mean and unit-variance, or data in the [0, 1] range.\n","\n","#________________data loading from directory\n","  tf.keras.preprocessing.image.ImageDataGenerator(\n","    featurewise_center=False,\n","    samplewise_center=False,\n","    featurewise_std_normalization=False,\n","    samplewise_std_normalization=False,\n","    zca_whitening=False,\n","    zca_epsilon=1e-06,\n","    rotation_range=0,\n","    width_shift_range=0.0,\n","    height_shift_range=0.0,\n","    brightness_range=None,\n","    shear_range=0.0,\n","    zoom_range=0.0,\n","    channel_shift_range=0.0,\n","    fill_mode=\"nearest\",\n","    cval=0.0,\n","    horizontal_flip=False,\n","    vertical_flip=False,\n","    rescale=None,\n","    preprocessing_function=None,\n","    data_format=None,\n","    validation_split=0.0,\n","    dtype=None,\n","    )\n","#USE\n","  #load data\n","  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","  y_train = np_utils.to_categorical(y_train, num_classes)\n","  y_test  = np_utils.to_categorical(y_test, num_classes)\n","  #init generator fun()\n","  datagen = ImageDataGenerator(\n","      featurewise_center=True,\n","      featurewise_std_normalization=True,\n","      rotation_range=20,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      horizontal_flip=True)\n","  # compute quantities required for featurewise normalization\n","  # (std, mean, and principal components if ZCA whitening is applied)\n","  datagen.fit(x_train)\n","  #start trainig\n","  # fits the model on batches with real-time data augmentation:\n","  model.fit(datagen.flow(x_train, y_train, batch_size=32),\n","            steps_per_epoch=len(x_train) / 32, epochs=epochs)\n","  #custom training loop\n","  # here's a more \"manual\" example\n","  for e in range(epochs):\n","      print('Epoch', e)\n","      batches = 0\n","      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n","          model.fit(x_batch, y_batch)\n","          batches += 1\n","          if batches >= len(x_train) / 32:\n","              # we need to break the loop by hand because\n","              # the generator loops the indefinitely\n","              break\n","#________________flow_from_direcoty\n","  ImageDataGenerator.flow_from_directory(\n","    directory,\n","    target_size=(256, 256),\n","    color_mode=\"rgb\",\n","    classes=None,\n","    class_mode=\"categorical\",\n","    batch_size=32,\n","    shuffle=True,\n","    seed=None,\n","    save_to_dir=None,\n","    save_prefix=\"\",\n","    save_format=\"png\",\n","    follow_links=False,\n","    subset=None,\n","    interpolation=\"nearest\",)\n","#USE\n","  train_datagen = ImageDataGenerator(\n","        rescale=1./255,\n","        shear_range=0.2,\n","        zoom_range=0.2,\n","        horizontal_flip=True)\n","  validation_datagen = ImageDataGenerator(rescale=1./255)\n","  train_generator = train_datagen.flow_from_directory(\n","          'data/train',\n","          target_size=(150, 150),\n","          batch_size=32,\n","          class_mode='binary')\n","  validation_generator = validation_datagen.flow_from_directory(\n","          'data/validation',\n","          target_size=(150, 150),\n","          batch_size=32,\n","          class_mode='binary')\n","  model.fit(\n","          train_generator,\n","          steps_per_epoch=2000,\n","          epochs=50,\n","          validation_data=validation_generator,\n","          validation_steps=800)\n","#image loading\n","  tf.keras.preprocessing.image.load_img(image_path, color_mode=\"rgb\", target_size=None, interpolation=\"nearest\")\n","  tf.keras.preprocessing.image.img_to_array(loaded_image,\n","                                            data_format=None,#None:using the default system settings\n","                                            # \"channels_first\" or \"channels_last\"\n","                                            dtype=None#None:using the default system settings\n","                                            #\"flaot32\" etc.\n","                                            )\n","  tf.keras.preprocessing.image.array_to_img(img_array)#array should be in shape WHC\n","#________________directory structure should be in this format\n","  #optional\n","    #trainig/dataset/\n","    #validation/dataset/\n","  [dataset/class_name/sampleindex.fileExtension]\n","  dataset/dog/\n","  dataset/dog/dog1.jpg\n","  dataset/dog/dog2.jpg\n","  dataset/cat/\n","  dataset/cat/cat1.jpg\n","  dataset/cat/cat2.jpg\n","  #or for text files.txt\n","  dataset/greetings/\n","  dataset/greetings/greetings1.txt\n","  dataset/greetings/greetings2.txt\n","  dataset/male_names/\n","  dataset/male_names/male_names1.txt\n","  dataset/male_names/male_names2.txt\n","#________________image_dataset_from_directory\n","  # tf.keras.preprocessing.image_dataset_from_directory : turns image files sorted into class-specific folders into a labeled dataset of image tensors.\n","  tf.keras.preprocessing.image_dataset_from_directory(\n","    # formats: jpeg, png, bmp, gif. Animated gifs are truncated to the first frame.\n","    directory,\n","      #./dataset\n","    labels=\"inferred\",\n","      # \"inferred\" (labels are generated from the directory structure),\n","      #or a list/tuple of integer labels of the same size as the number of image files found in the directory. Labels should be sorted according to the alphanumeric order of the image file paths (obtained via os.walk(directory) in Python).\n","    label_mode=\"int\",\n","      #'int':\n","        #means that the labels are encoded as int32 integers tensor of shape (batch_size,)\n","        #e.g. for 'sparse_categorical_crossentropy' loss\n","      #'categorical':\n","        # means that the labels are encoded as float32 tensor of shape (batch_size, num_classes) categorical vector \n","        #e.g. for 'categorical_crossentropy' loss\n","        # representing a one-hot encoding of the class index.\n","      # 'binary':\n","        # means that the labels are encoded as float32 scalars tensor with values 0s or 1s of shape (batch_size, 1)\n","        #e.g. for 'binary_crossentropy' loss\n","        #there can be only 2 classes\n","      #None: means no labels, probabaly the case of GAN.\n","    class_names=None,\n","      #Only used if labels = \"inferred\".\n","      #This is the explict list of class names (must match names of subdirectories).\n","      #Used to control the order of the classes (otherwise alphanumerical order is used).\n","    color_mode=\"rgb\",\n","      # Whether the images will be converted to have 1, 3, or 4 channels.\n","      # \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\"\n","    batch_size=32,\n","    image_size=(256, 256),#to resize images\n","    shuffle=True,\n","      #False: sorts the data in alphanumeric order.\n","    seed=None,\n","      #Optional random seed for shuffling and transformations.\n","    validation_split=None,\n","      #Optional float between 0 and 1, fraction of data to reserve for validation.\n","    subset=None,\n","      #One of \"training\" or \"validation\". Only used if validation_split is set.\n","    interpolation=\"bilinear\",\n","      #to resize images method\n","      # bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic.\n","    follow_links=False,\n","      #Whether to visits subdirectories pointed to by symlinks. Defaults to False.\n","    )\n","    #if label_mode = None then it retruns x otherwise it retus (x,y)\n","#USE\n","  from tensorflow import keras\n","  from tensorflow.keras.preprocessing.image import image_dataset_from_directory\n","\n","  train_ds = image_dataset_from_directory(\n","      directory='training_data/',\n","      labels='inferred',\n","      label_mode='categorical',\n","      batch_size=32,\n","      image_size=(256, 256))\n","  validation_ds = image_dataset_from_directory(\n","      directory='validation_data/',\n","      labels='inferred',\n","      label_mode='categorical',\n","      batch_size=32,\n","      image_size=(256, 256))\n","\n","  model = keras.applications.Xception(weights=None, input_shape=(256, 256, 3), classes=10)\n","  model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","  model.fit(train_ds, epochs=10, validation_data=validation_ds)\n","  # For demonstration, iterate over the batches yielded by the dataset.\n","    for data, labels in train_ds:\n","      print(data.shape)  # (32, 256, 256, 3)\n","      print(data.dtype)  # float32\n","      print(labels.shape)  # (32,10)#one-hot\n","      print(labels.dtype)  # int32\n","#________________text_dataset_from_directory \n","  # tf.keras.preprocessing.text_dataset_from_directory : turns text files sorted into class-specific folders into a labeled dataset of text tensors.\n","  tf.keras.preprocessing.text_dataset_from_directory(\n","      #Generates a tf.data.Dataset from text files in a directory.\n","      #Only .txt files are supported at this time.\n","    directory,\n","    #./dataset\n","    labels=\"inferred\",\n","    label_mode=\"int\",\n","    class_names=None,\n","    batch_size=32,\n","    max_length=None,\n","      #Maximum size of a text string. Texts longer than this will be truncated to max_length.\n","    shuffle=True,\n","    seed=None,\n","    validation_split=None,\n","    subset=None,\n","    follow_links=False,\n","    )\n","\n","# tf.data tips\n","  dataset.cache()\n","    # If you call .cache() on a dataset, its data will be cached after running through the first iteration over the data. Every subsequent iteration will use the cached data. The cache can be in memory (default) or to a local file you specify.\n","      # Your data is not expected to change from iteration to iteration\n","      # You are reading data from a remote distributed filesystem\n","      # You are reading data from local disk, but your data would fit in memory and your workflow is significantly IO-bound (e.g. reading & decoding image files).\n","  dataset.prefetch(buffer_size)\n","    # You should almost always call .prefetch(buffer_size) after creating a dataset. It means your data pipeline will run asynchronously from your model, with new samples being preprocessed and stored in a buffer while the current batch samples are used to train the model. The next batch will be prefetched in GPU memory by the time the current batch is over.\n","#image batch Augmentation\n","  tf.keras.layers.experimental.preprocessing.Resizing(\n","    height,#output height\n","    width,#output width\n","    interpolation=\"bilinear\",\n","      # String, the interpolation method. Defaults to bilinear. Supports bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic\n","    name=None,#layer name\n","    **kwargs)\n","    #input should be in shape NHWC\n","  tf.keras.layers.experimental.preprocessing.Rescaling(\n","      #Multiply inputs by scale and adds offset\n","        # To rescale form [0, 255] to  [0, 1] range, you would pass scale=1./255.\n","        # To rescale from [0, 255] to [-1, 1] range, you would pass scale=1./127.5, offset=-1.\n","        # The rescaling is applied both during training and inference.\n","      scale,#float\n","      offset=0.0,#float\n","      name=None,#layer name\n","      **kwargs)\n","  tf.keras.layers.experimental.preprocessing.CenterCrop(\n","    # Crop the central portion of the images to target height and width.\n","    #input should be in shape NHWC\n","    height,#int\n","    width,#int\n","    name=None, **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomCrop(\n","      # Randomly crop the images to target height and width.\n","      #This layer will crop all the images in the same batch to the same cropping location. By default, random cropping is only applied during training. At inference time, the images will be first rescaled to preserve the shorter side, and center cropped. If you need to apply random cropping at inference time, set training to True when calling the layer.\n","    height,#output height\n","    width,#output width\n","    seed=None,#integer used to create random seed\n","    name=None, **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomFlip(\n","      #Randomly flip each image horizontally and vertically.\n","      #This layer will flip the images based on the mode attribute. During inference time, the output will be identical to input. Call the layer with training=True to flip the input.\n","      #input shape NHWC\n","    mode=\"horizontal_and_vertical\",\n","      #String indicating which flip mode to use. Can be \"horizontal\", \"vertical\", or \"horizontal_and_vertical\". Defaults to \"horizontal_and_vertical\". \"horizontal\" is a left-right flip and \"vertical\" is a top-bottom flip.\n","    seed=None,#to create random seed\n","    name=None, **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomTranslation(\n","      #Randomly translate each image during training.\n","      #input shape NHWC\n","    height_factor,\n","      #a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting vertically. A negative value means shifting image up, while a positive value means shifting image down. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, height_factor=(-0.2, 0.3) results in an output shifted by a random amount in the range [-20%, +30%]. height_factor=0.2 results in an output height shifted by a random amount in the range [-20%, +20%].\n","    width_factor,\n","      #a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for shifting horizontally. A negative value means shifting image left, while a positive value means shifting image right. When represented as a single positive float, this value is used for both the upper and lower bound. For instance, width_factor=(-0.2, 0.3) results in an output shifted left by 20%, and shifted right by 30%. width_factor=0.2 results in an output height shifted left or right by 20%.\n","    fill_mode=\"reflect\",\n","      # Points outside the boundaries of the input are filled according to the given mode (one of {'constant', 'reflect', 'wrap', 'nearest'}).\n","      # reflect: (d c b a | a b c d | d c b a) The input is extended by reflecting about the edge of the last pixel.\n","      # constant: (k k k k | a b c d | k k k k) The input is extended by filling all values beyond the edge with the same constant value k = 0.\n","      # wrap: (a b c d | a b c d | a b c d) The input is extended by wrapping around to the opposite edge.\n","      # nearest: (a a a a | a b c d | d d d d) The input is extended by the nearest pixel.\n","    interpolation=\"bilinear\",#or\"nearest\"\n","    seed=None,#to create random seed\n","    name=None,\n","    **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(\n","      #Randomly rotate each image.\n","      #By default, random rotations are only applied during training. At inference time, the layer does nothing. If you need to apply random rotations at inference time, set training to True when calling the layer.\n","    factor,\n","      #a float represented as fraction of 2pi, or a tuple of size 2 representing lower and upper bound for rotating clockwise and counter-clockwise. A positive values means rotating counter clock-wise, while a negative value means clock-wise. When represented as a single float, this value is used for both the upper and lower bound. For instance, factor=(-0.2, 0.3) results in an output rotation by a random amount in the range [-20% * 2pi, 30% * 2pi]. factor=0.2 results in an output rotating by a random amount in the range [-20% * 2pi, 20% * 2pi].\n","    fill_mode=\"reflect\",\n","      # Points outside the boundaries of the input are filled according to the given mode (one of {'constant', 'reflect', 'wrap', 'nearest'}).\n","      # reflect: (d c b a | a b c d | d c b a) The input is extended by reflecting about the edge of the last pixel.\n","      # constant: (k k k k | a b c d | k k k k) The input is extended by filling all values beyond the edge with the same constant value k = 0.\n","      # wrap: (a b c d | a b c d | a b c d) The input is extended by wrapping around to the opposite edge.\n","      # nearest: (a a a a | a b c d | d d d d) The input is extended by the nearest pixel.\n","    interpolation=\"bilinear\",#'nearest'\n","    seed=None, name=None, **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomZoom(\n","      #Randomly zoom each image during training.\n","    height_factor,\n","      #a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming vertically. When represented as a single float, this value is used for both the upper and lower bound. A positive value means zooming out, while a negative value means zooming in. For instance, height_factor=(0.2, 0.3) result in an output zoomed out by a random amount in the range [+20%, +30%]. height_factor=(-0.3, -0.2) result in an output zoomed in by a random amount in the range [+20%, +30%].\n","    width_factor=None,\n","      #a float represented as fraction of value, or a tuple of size 2 representing lower and upper bound for zooming horizontally. When represented as a single float, this value is used for both the upper and lower bound. For instance, width_factor=(0.2, 0.3) result in an output zooming out between 20% to 30%. width_factor=(-0.3, -0.2) result in an output zooming in between 20% to 30%. Defaults to None, i.e., zooming vertical and horizontal directions by preserving the aspect ratio.\n","    fill_mode=\"reflect\",\n","      #Points outside the boundaries of the input are filled according to the given mode (one of {'constant', 'reflect', 'wrap', 'nearest'}).\n","      # reflect: (d c b a | a b c d | d c b a) The input is extended by reflecting about the edge of the last pixel.\n","      # constant: (k k k k | a b c d | k k k k) The input is extended by filling all values beyond the edge with the same constant value k = 0.\n","      # wrap: (a b c d | a b c d | a b c d) The input is extended by wrapping around to the opposite edge.\n","      # nearest: (a a a a | a b c d | d d d d) The input is extended by the nearest pixel.\n","    interpolation=\"bilinear\",#or\"nearest\"\n","    seed=None,name=None,**kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomHeight(\n","      #Randomly vary the height of a batch of images during training.\n","      #Adjusts the height of a batch of images by a random factor. The input should be a 4-D tensor in the \"channels_last\" image data format.\n","      #By default, this layer is inactive during inference.\n","    factor,\n","      #A positive float (fraction of original height), or a tuple of size 2 representing lower and upper bound for resizing vertically. When represented as a single float, this value is used for both the upper and lower bound. For instance, factor=(0.2, 0.3) results in an output with height changed by a random amount in the range [20%, 30%]. factor=(-0.2, 0.3) results in an output with height changed by a random amount in the range [-20%, +30%].factor=0.2results in an output with height changed by a random amount in the range[-20%, +20%]`.\n","    interpolation=\"bilinear\",\n","      # Defaults to bilinear. Supports bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic\n","    seed=None, name=None, **kwargs)\n","  tf.keras.layers.experimental.preprocessing.RandomWidth(\n","      # Randomly vary the width of a batch of images during training.\n","      # Adjusts the width of a batch of images by a random factor. The input should be a 4-D tensor in the \"channels_last\" image data format.\n","      # By default, this layer is inactive during inference.\n","    factor,\n","      #A positive float (fraction of original height), or a tuple of size 2 representing lower and upper bound for resizing vertically. When represented as a single float, this value is used for both the upper and lower bound. For instance, factor=(0.2, 0.3) results in an output with width changed by a random amount in the range [20%, 30%]. factor=(-0.2, 0.3) results in an output with width changed by a random amount in the range [-20%, +30%].factor=0.2results in an output with width changed by a random amount in the range[-20%, +20%]`.\n","    interpolation=\"bilinear\",\n","      #Defaults to bilinear. Supports bilinear, nearest, bicubic, area, lanczos3, lanczos5, gaussian, mitchellcubic\n","    seed=None, name=None, **kwargs)\n","  #USE\n","  output = tf.keras.layers.experimental.preprocessing.RandomWidth(args)(input)\n","#Utilties\n","  tf.keras.utils.to_categorical(\n","      #Converts a class vector (integers) to binary class matrix.\n","      #it is same as one_hot; see 'label encoding methods' section\n","      #returns  matrics, each vector representing corresponding class index in binary; 1 = [0,1,0,0]\n","      y,#vector of [0,1,2,3] integer class indices\n","      num_classes=None,#number of classes. If None, this would be inferred as the (largest number in y) + 1.\n","      dtype=\"float32\")#the data type expected by the input. Default: 'float32'.\n","  tf.keras.backend.clear_session()\n","    #clear old model and its layers with their names in memory that are not in used anymore.\n","    #otherwise old models and layers will consume memeory.\n","    #it stops the layers to not to produce new name every time same model is created.\n","      #creation 1: Dense_01; creation 2: Dense_02; creation 3: Dense_03\n","  tf.keras.backend.floatx()\n","    #Returns the default float type, as a string.\n","    #E.g. 'float16', 'float32', 'float64'.\n","    #default float type is :'float32'\n","  tf.keras.backend.set_floatx('float64')\n","    #sets the default float type to 'float64'\n","  tf.keras.backend.image_data_format()\n","    #Returns the default image data format convention in string.\n","    #'channels_first' or 'channels_last'\n","    #default:'channels_last'\n","  tf.keras.backend.set_image_data_format('channels_first')\n","    #sets the default image_data_foramt convension in string\n","#label encoding methods\n","  # Integer Encoding:\n","    #Where each unique label is mapped to an integer.\n","    #this is only used with sparsecategoricalcorossentropyloss\n","    #then these integer vector can be used to make one_hot encodings\n","    # ['cat','dog','rat','car'] to [0,1,2,3]\n","    #drawback: it make the model to assume that highest integer has highest priority than other labels\n","  # One Hot Encoding or to_categorical:\n","    #Where each label is mapped to a binary vectors.\n","    #this is only used with cataegorcalcrossentropyloss\n","    # [1,2,3] to [[1,0,0],[0,1,0],[0,0,1]]\n","    tf.one_hot(indices,\n","                #integer vector\n","               depth,\n","                #integer type,  if 3, then output will be [[1,0,0]], if 2 then [[1,0]]\n","               on_value=None,\n","                #default:1,can be any value\n","               off_value=None,\n","                #default:0, can be any value\n","               axis=None,\n","                # If the input indices is rank 1D, the output will have rank 2D. The new axis is created at dimension axis (default: the new axis is appended at the end).\n","                # If indices is a scalar =1 the output shape will be a vector of length depth[1,0,0]\n","                # If indices is a vector of length features=[1,2,3], the output shape will be:\n","                #   features x depth if axis == -1   [[1,0,0],[0,1,0],[0,0,1]]\n","                #   depth x features if axis == 0    ??\n","               dtype=None,\n","                #default:tf.float32,type of output tensor\n","                #for tf.string, tf.bool, etc. , both on_value and off_value must be provided\n","               name=None)\n","  # Learned Embedding:\n","    #Where a distributed representation of the categories is learned.\n","    #[1,2,3] to [[.23,.35,.32,.14,.53],[.44,.67,.38,.59],[.76,.49,.98,.88]]\n","# split data into train and test\n","  from sklearn.model_selection import train_test_split\n","  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WosRd8oDqU1","cellView":"form"},"source":["#@title RNN LSTM GRU Bidirectional TimeDistributed\n","input_shape = (batch, timesteps, feature)\n","\n","return_state = True\n","  # Returns (output_vector,hidden_state,current_state) for the given input to the rnn layer,\n","  # these vlues can be used to init. the next rnn layer with inintial_state=(hidden_state,curent_state).\n","  #encoder-decoder (sequence-to-sequence model), where the encoder final state is used for initial state of the decoder.\n","  #Note that LSTM has 2 state tensors, but GRU only has one.\n","return_sequence = True\n","  # return the entire sequence of outputs for each sample\n","    # one vector per timestep per sample of shape (batch_size, timesteps, units).\n","  # Used with seq2seq models\n","stateful=True # Cross-batch statefulness\n","  # RNN resets its states for every new sample and forgets the stated or knowlege of past sample.\n","  # If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.\n","  # processing very long sequences (possibly infinite).\n","  # If you have very long sequences though, it is useful to break them into shorter sequences, and to feed these shorter sequences sequentially into a RNN layer without resetting the layer's state. That way, the layer can retain information about the entirety of the sequence, even though it's only seeing one sub-sequence at a time.\n","  # When you want to clear the state, you can use layer.reset_states().\n","  # Note: the batch format should be as;\n","    # batch_1[sentece_A_part1,sentence_B_part1],\n","    # batch_2[sentece_A_part2,sentence_B_part2]\n","  #USE\n","    lstm_layer = layers.LSTM(64, stateful=True)\n","    for part in paragraphs: output = lstm_layer(part)\n","    # reset_states() will reset the cached state to the original initial_state.\n","    # If no initial_state was provided, zero-states will be used by default.\n","    lstm_layer.reset_states()\n","initial_state=None\n","  # List of initial state tensors to be passed to the first call of the cell (optional, defaults to None which causes creation of zero-filled initial state tensors).\n","  # can use initial_state=(hidden_state,curent_state) returned by return_state=True\n","  # can use initial_state=prev_layer.states\n","  # shape of the state needs to match the unit size of the layer\n","go_backwards=False\n","  # If True, process the input sequence backwards and return the reversed sequence.\n","unroll = False\n","  #Loop unrolling can lead to a large speedup when processing short sequences on CPU\n","TimeDistributed(\n","    #input should be at least 3D [batch_size,timesteps,features] = (32, 10, 28,28,3)\n","    layer,\n","      #Dense,LSTM,CONV2D3D etc\n","     **kwargs)\n","Bidirectional(\n","    # process the input by both sides forward and backward\n","    #The output of the Bidirectional RNN will be, by default, the sum of the forward layer output and the backward layer output. If you need a different merging behavior, e.g. concatenation, change the merge_mode parameter in the Bidirectional wrapper constructor.\n","    layer,\n","      #SimpleRNN,LSTM,GRU or custom layer that meets:\n","        # Be a sequence-processing layer (accepts 3D+ inputs).\n","        # Have a go_backwards, return_sequences and return_state attribute (with the same semantics as for the RNN class).\n","        # Have an input_spec attribute.\n","        # Implement serialization via get_config() and from_config(). Note that the recommended way to create new RNN layers is to write a custom RNN cell and use it with keras.layers.RNN, instead of subclassing keras.layers.Layer directly.\n","    merge_mode=\"concat\",\n","      # Mode by which outputs of the forward and backward RNNs will be combined.\n","      #'sum', 'mul', 'concat', 'ave', None\n","      #if None then the output will be the list \n","    weights=None,\n","    backward_layer=None,\n","      # Optional keras.layers.RNN, or keras.layers.Layer instance to be used to handle backwards input processing.\n","      # If backward_layer is not provided, the layer instance passed as the layer argument will be used to generate the backward layer automatically.\n","      #  Note that the provided backward_layer layer should have properties matching those of the layer argument, \n","      # in particular it should have the same values for stateful, return_states, return_sequence, etc.\n","      #  In addition, backward_layer and layer should have different go_backwards argument values.\n","      #  A ValueError will be raised if these requirements are not met.\n","    **kwargs)\n","#RNN Cells(SimpleRNNCell ,LSTMCell,GRUCell)\n","  #Processes whole batches of input sequences\n","  #  RNN(LSTMCell(10)) ==  LSTM(10)\n","#RNN(SimpleRNN,LSTM,GRU)\n","  #only processes a single timestep.\n","  \n","#CuDNN           #The NVIDIA \"CUDA® Deep Neural Network\" library (cuDNN) is a GPU-accelerated library of primitives for deep neural networks. cuDNN provides highly tuned implementations for standard routines such as forward and backward convolution, pooling, normalization, and activation layers.\n","  # this is by default works automatically without any explicit implementation.\n","  #this wont work if:\n","    # Changing the activation function from tanh to something else.\n","    # Changing the recurrent_activation function from sigmoid to something else.\n","    # Using recurrent_dropout > 0 <.\n","    # Setting unroll to True, which forces LSTM/GRU to decompose the inner tf.while_loop into an unrolled for loop.\n","    # Setting use_bias to False.\n","    # Using masking when the input data is not strictly right padded (if the mask corresponds to strictly right padded data, CuDNN can still be used. This is the most common case).\n","  # CuDNN is only available at the layer level, and not at the cell level.\n","    # This means `LSTM(units)` will use the CuDNN kernel,\n","    # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n","#RNNs with list/dict inputs, or nested inputs\n","  # like video having video and audio, then the input shape would be:\n","    #[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency],\"text\":[character]}]\n","  # like handwrinting useing x,y cordinated and presseure on that point\n","    #[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pnPb4um0PLa0","cellView":"form"},"source":["# @title Masking & Padding\n","#Masking:\n","  # to tell the layer the data is missing and should be skipped.\n","  #eg. [[[True,True,True,False,False]]] of shape(1,5) is a mask tensor and can be feeded to mask consuming layer\n","  #3 ways to so:\n","    # Add a keras.layers.Masking() layer.\n","    # Configure a keras.layers.Embedding( mask_zero=True).\n","    # Pass a mask argument manually when calling layers that support this argument (e.g. RNN layers).\n","    print(layer_output._keras_mask)\n","  #use of mask tensor\n","    x = lstm(x, mask=mask)\n","\n","#Padding:\n","  #eg. [[10,23,45,0,0]]\n","  # to pad the missing data to make sure the data is in standard size-and-shape for the layer\n","  # Note that you could \"pre\" padding (at the beginning) or\n","  # \"post\" padding (at the end).\n","  # We recommend using \"post\" padding when working with RNN layers\n","  # (in order to be able to use the CuDNN implementation of the layers).\n","  tf.keras.preprocessing.sequence.pad_sequences(\n","      sequences,\n","      #a list or np.array\n","      maxlen=None,\n","        #the maximum length the all sequences will be padded to\n","        #default will take the length of longest sequence\n","      dtype=\"int32\",\n","        #type of output sequences\n","      padding=\"pre\",\n","      #'pre': pad before [0,0,1,2]\n","      #'post': pad after [1,2,0,0]\n","      truncating=\"pre\",\n","      #'pre': remove before [1,2,3,0,0]\n","      #'post': remove after [3,4,5,0,0]\n","      value=0.0\n","      #pad with this value of one of the types string or float\n","      )\n","  padded_inputs = tf.keras.preprocessing.sequence.pad_sequences(raw_inputs,padding=\"post\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYNMkrRaOyfT","cellView":"form"},"source":["#@title Saving models\n","#1. Whole-model saving (configuration + weights)____________\n","  # the architecture/config of the model, allowing to re-create the model\n","  # the leaned weights of the model\n","  # the training configuration (loss, optimizer)\n","  # the state of the optimizer, allowing to resume training exactly where you left off.\n","\n","  #Subclassed models can only be saved with the SavedModel format.\n","#save\n","\n","model.save(\n","    filepath,\n","      #'./model_name'\n","    overwrite=True,\n","      #Whether to silently overwrite any existing file at the target location, or provide the user with a manual prompt.\n","    include_optimizer=True,\n","      #If True, save optimizer's state together.\n","    save_format=None,#default: 'tf'\n","      #'tf' or 'h5'\n","    signatures=None,\n","      #Applicable to the 'tf' format only\n","    options=None,\n","      #Optional tf.saved_model.SaveOptions object that specifies options for saving to SavedModel.\n",")\n","model.save('./model_name')#default\n","#or\n","model.save('./your_file_path/my_model', save_format='tf')# or 'h5'\n","#or\n","model.save('my_model.h5')# or '.keras'\n","\n","#load\n","tf.keras.models.load_model(\n","    filepath,#'./model.name'\n","    custom_objects=None,\n","      # Optional dictionary mapping names (strings) to custom classes or functions to be considered during deserialization.\n","    compile=True,\n","      #Boolean, whether to compile the model after loading.\n","    options=None\n","      #Optional tf.saved_model.LoadOptions object that specifies options for loading from SavedModel.\n",")\n","model = load_model('./model_name')#default\n","#or\n","model = load_model('./my_model.h5')#or 'my_model.keras\n","##If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument\n","model = load_model('./my_model.h5',custom_objects={'AttentionLayer': AttentionLayer})\n","\n","#2. Weights-only saving____________\n","#if you have code to instantiating your model, you can then load the weights you saved into a model with the same architecture:\n","#if you need model for prediction or inference only or  for transfer learning with no need of compilation then use save_weights only.\n","#if as long as two models have the same architecture, they are able to share the same checkpoint.\n","#save\n","model.save_weights('./my_model_weights')#default\n","#or save_foramt surpasses any other nameing convension like 'name.h5'  wont work here\n","model.save_weights('./my_model_weights', save_format='tf')# or 'h5'\n","#or\n","model.save_weights('./my_model_weights.h5')# or '.hdf5'\n","\n","#load\n"," #layers that don't have weights are not taken into account in the topological ordering, so adding or removing layers is fine as long as they don't have weights.\n","Model.load_weights(filepath,#'./model_weights'\n","                   by_name=False,\n","                    #Boolean, whether to load weights by name or by topological order. Only topological loading is supported for weight files in TensorFlow format.\n","                   skip_mismatch=False,\n","                    #Boolean, whether to skip loading of layers where there is a mismatch in the number of weights, or a mismatch in the shape of the weight (only valid when by_name=True).\n","                   options=None\n","                    #Optional tf.train.CheckpointOptions object that specifies options for loading weights.\n","                   )\n","model.load_weights('./my_model_weights')#defalult\n","#or\n","model.load_weights('./my_model_weights.h5')#or '.hdf5'\n","\n","#check successful load of weights\n","load_status.assert_consumed()\n","\n","#weights transfering\n","#you can also load a chekcpoint of one model to another model with same architechture.\n","#If you need to load the weights into a different architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load them by layer name:\n","# If you switch between Sequential and Functional or subclassed, etc., then always rebuild the pre-trained model and load the pre-trained weights to that model.\n","# old model\n","model = Sequential()\n","model.add(Dense(2, input_dim=3, name='dense_1'))\n","model.add(Dense(3, name='dense_2'))\n","model.save_weights(old_model)\n","# new model with diff architecture\n","model = Sequential()\n","model.add(Dense(2, input_dim=3, name='dense_1'))  # weights will be loaded\n","model.add(Dense(10, name='new_dense'))  # weights will not be loaded\n","# load weights from old model; will only affect the first layer, dense_1.\n","model.load_weights(old_model, by_name=True)\n","\n","# one layer to another layer\n","layer_2.set_weights(layer_1.get_weights())\n","# One model to another model with a same architecture , also woeks with models with layer that do not effect weights like, dropout() layer\n","customModel.set_weights(api_based_model.get_weights())\n","#3. Configuration-only saving (serialization)___________\n","#If you only need to save the architecture of a model, and not its weights or its training configuration.\n","#Note this only applies to models defined using the functional or Sequential apis not subclassed models.\n","\n","# save as JSON\n","#The generated JSON file is human-readable and can be manually edited if needed.\n","json_string = model.to_json()\n","with open('./my_model.txt','b') as f:\n","  f.write(json_string)\n","\n","#load from JSON\n","#You can then build a fresh model from this data:\n","from tensorflow.keras.models import model_from_json\n","with open('./my_model.txt','b') as f:\n","  json_string = f.read()\n","model = model_from_json(json_string)\n","                        #If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument\n","                        #,custom_objects={'AttentionLayer': AttentionLayer})\n","\n","# custom layers (or other custom objects) in saved models________________\n","# If the model you want to load includes custom layers or other custom classes or functions, you can pass them to the loading mechanism via the custom_objects argument:\n","# Assuming your model includes instance of an \"AttentionLayer\" class\n","# you can use a custom object scope:\n","from tensorflow.keras.utils import CustomObjectScope\n","custom_objs = {'AttentionLayer': AttentionLayer,\n","               #'CustomMethodUsedInLayerOrModel',CustomMethodUsedInLayerOrModel\n","               }\n","with CustomObjectScope(custom_objs):\n","    model = load_model('my_model.h5')\n","    #or\n","    model = model_from_json(json_string)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqh4gofL37mC","cellView":"form"},"source":["#@title any method to use in the middel of training using LambdaCallback()\n","def p(epoch):\n","  if epoch%2 is 0:print('hello')\n","# using lambdacallback()\n","epoch_print_callback = LambdaCallback(\n","    on_epoch_begin=lambda epoch,logs: p(epoch)\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oHHakCzzcNbB"},"source":["##models and approaches"]},{"cell_type":"code","metadata":{"id":"yBWD7ossnY2p","cellView":"form"},"source":["#@title GAN\n"," def train_step(self, real_images):\n","        if isinstance(real_images, tuple):\n","            #get first batch\n","            real_images = real_images[0]\n","        # Sample random points in the latent space\n","        #get batch_size\n","        batch_size = tf.shape(real_images)[0]\n","        #___________Phase-I\n","        #Train the discriminator. - Sample a batch of random points in the latent space. - Turn the points into fake images via the \"generator\" model. - Get a batch of real images and combine them with the generated images. - Train the \"discriminator\" model to classify generated vs. real images.\n","        #get (batch_size,letent_vector) shape random vectors for each represeting and image \n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # Decode them to fake images\n","        generated_images = self.generator(random_latent_vectors)\n","\n","        # Combine them with real images\n","        combined_images = tf.concat([generated_images, real_images], axis=0)\n","\n","        # create labels discriminating real(1s) vs fake(0s) images\n","        labels = tf.concat([tf.ones((batch_size, 1)),#real image labels\n","                           tf.zeros((batch_size, 1))],#fake image labels\n","                           axis=0\n","                           )\n","        # Add random noise to the labels - important trick!\n","        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n","\n","        # Train the discriminator\n","        with tf.GradientTape() as tape:\n","          #compute real vs fake\n","            predictions = self.discriminator(combined_images)\n","            #compute loss\n","            d_loss = self.loss_fn(labels, predictions)\n","        #compute discriminator gradients\n","        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n","        #update discriminator gradients with d_optimizer\n","        self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","        #___________Phase-II\n","        #Train the generator. - Sample random points in the latent space. - Turn the points into fake images via the \"generator\" network. - Get a batch of real images and combine them with the generated images. - Train the \"generator\" model to \"fool\" the discriminator and classify the fake images as real.\n","        #get (batch_size,letent_vector) shape random vectors for each represeting and image \n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        # create labels that say \"all real images\"\n","        misleading_labels = tf.zeros((batch_size, 1))\n","\n","        # Train the generator\n","        #note that we should *not* update the weights  of the discriminator !\n","        with tf.GradientTape() as tape:\n","          #get predictions\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            #compute loss\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        #get generator gradients\n","        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n","        #update generator gradients with g_optimizer\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","        \n","        return d_loss, g_loss, generated_images\n","\n","d_loss, g_loss, generated_images = train_step(batch)\n","#get first image array\n","image_array = generated_images[0]\n","#convert genrated images array to images\n","img = tf.keras.preprocessing.image.array_to_img(image_array[0] * 255.0, scale=False)\n","img.save('./first_generate_image.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s23y13RwxzCT","cellView":"form"},"source":["#@title Embedding\n","# embedding is a matrix of  smallest elements like (char_dim x char_dim) or actual elements like  (words_dim x words_dim)\n","# human brain has multidimensional embeddings\n","# possible colors =  16777216 = [256,256,256] = 24bit color = 2**24\n","# possible elements = 150994944 = [16777216,3,3]\n","# smallest image element consiting of 3*3 pixles\n","# 480/3 = 160\n","# possible elements in 480x480 image = 25,600 = 160*160\n","# text embedding [ 143,859 ]\n","# there are 86 billion neurons in brain and 173 billion neurons in GPT-3\n","#(TSNE is a manifold learning technique which means that it tries to map high-dimensional data to a lower-dimensional manifold, creating an embedding that attempts to maintain local structure within the data. It’s almost exclusively used for visualization because the output is stochastic and it does not support transforming new data. An up and coming alternative is Uniform Manifold Approximation and Projection, UMAP, which is much faster and does support transform new data into the embedding space).\n","#TensorFlow developed projector, an online application that lets us visualize and interact with embeddings. \n","#vect_A = index_A (   (n max indices in space T) x (n max indices in space I)   )\n","#vect_T = index_T (   (n max indices in space A) x (n max indices in space I)   )\n","#vect_I = index_I (   (n max indices in space A) x (n max indices in space T)   )\n","#vect_ATI = vect_A x vect_T x vect_I\n","# embeddings_ATI = shape(len(A space), len(T space), len(I space))\n","#index_lookup = (index_A, max_n at index 0, max_n at index 0)\n","#vect_A = (index_A, index_lookup_T indices joins corresponding index_lookup_I indices)\n","\n","from __future__ import absolute_import\n","\n","from .. import backend as K\n","from .. import initializers\n","from .. import regularizers\n","from .. import constraints\n","from ..engine import Layer\n","from ..legacy import interfaces\n","\n","class Embedding(Layer):\n","    \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n","      eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n","\n","      This layer can only be used as the first layer in a model.\n","\n","      # Example\n","\n","      ```python\n","        model = Sequential()\n","        model.add(Embedding(1000, 64, input_length=10))\n","        # the model will take as input an integer matrix of size (batch, input_length).\n","        # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n","        # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n","\n","        input_array = np.random.randint(1000, size=(32, 10))\n","\n","        model.compile('rmsprop', 'mse')\n","        output_array = model.predict(input_array)\n","        assert output_array.shape == (32, 10, 64)\n","      ```\n","\n","      # Arguments\n","        input_dim: int > 0. Size of the vocabulary,\n","            i.e. maximum integer index + 1.\n","        output_dim: int >= 0. Dimension of the dense embedding.\n","        embeddings_initializer: Initializer for the `embeddings` matrix\n","            (see [initializers](../initializers.md)).\n","        embeddings_regularizer: Regularizer function applied to\n","            the `embeddings` matrix\n","            (see [regularizer](../regularizers.md)).\n","        embeddings_constraint: Constraint function applied to\n","            the `embeddings` matrix\n","            (see [constraints](../constraints.md)).\n","        mask_zero: Whether or not the input value 0 is a special \"padding\"\n","            value that should be masked out.\n","            This is useful when using [recurrent layers](recurrent.md)\n","            which may take variable length input.\n","            If this is `True` then all subsequent layers\n","            in the model need to support masking or an exception will be raised.\n","            If mask_zero is set to True, as a consequence, index 0 cannot be\n","            used in the vocabulary (input_dim should equal size of\n","            vocabulary + 1).\n","        input_length: Length of input sequences, when it is constant.\n","            This argument is required if you are going to connect\n","            `Flatten` then `Dense` layers upstream\n","            (without it, the shape of the dense outputs cannot be computed).\n","\n","      # Input shape\n","          2D tensor with shape: `(batch_size, sequence_length)`.\n","\n","      # Output shape\n","          3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n","\n","      # References\n","          - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n","      \"\"\"\n","\n","    @interfaces.legacy_embedding_support\n","    def __init__(self, input_dim, output_dim,\n","                 embeddings_initializer='uniform',\n","                 embeddings_regularizer=None,\n","                 activity_regularizer=None,\n","                 embeddings_constraint=None,\n","                 mask_zero=False,\n","                 input_length=None,\n","                 **kwargs):\n","        if 'input_shape' not in kwargs:\n","            if input_length:kwargs['input_shape'] = (input_length,)\n","            else:kwargs['input_shape'] = (None,)\n","        super(Embedding, self).__init__(**kwargs)\n","\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","        self.embeddings_initializer = initializers.get(embeddings_initializer)\n","        self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","        self.embeddings_constraint = constraints.get(embeddings_constraint)\n","        self.mask_zero = mask_zero\n","        self.input_length = input_length\n","\n","    def build(self, input_shape):\n","        self.embeddings = self.add_weight(\n","            # embedding space, a 2D tensor\n","            shape=(self.input_dim, self.output_dim),\n","            initializer=self.embeddings_initializer,\n","            name='embeddings',\n","            regularizer=self.embeddings_regularizer,\n","            constraint=self.embeddings_constraint,\n","            dtype=self.dtype)\n","        self.built = True\n","\n","    def compute_mask(self, inputs, mask=None):\n","        if not self.mask_zero:return None\n","        else:return K.not_equal(inputs, 0)\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.input_length is None:return input_shape + (self.output_dim,)\n","        else:\n","            # input_length can be tuple if input is 3D or higher\n","            if isinstance(self.input_length, (list, tuple)):in_lens = list(self.input_length)\n","            else:in_lens = [self.input_length]\n","            if len(in_lens) != len(input_shape) - 1:\n","                ValueError('\"input_length\" is %s, but received input has shape %s' %\n","                           (str(self.input_length), str(input_shape)))\n","            else:\n","                for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n","                    if s1 is not None and s2 is not None and s1 != s2:\n","                        ValueError('\"input_length\" is %s, but received input has shape %s' %\n","                                   (str(self.input_length), str(input_shape)))\n","                    elif s1 is None:in_lens[i] = s2\n","            return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n","\n","    def call(self, inputs):\n","      #input should be integer vector representing indices\n","        if K.dtype(inputs) != 'int32':\n","            inputs = K.cast(inputs, 'int32')\n","        #get embedding vectors for corresponding integer indices\n","        return K.gather(self.embeddings, inputs)\n","\n","    def get_config(self):\n","        config = {'input_dim': self.input_dim,\n","                  'output_dim': self.output_dim,\n","                  'embeddings_initializer': initializers.serialize(self.embeddings_initializer),\n","                  'embeddings_regularizer': regularizers.serialize(self.embeddings_regularizer),\n","                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n","                  'embeddings_constraint': constraints.serialize(self.embeddings_constraint),\n","                  'mask_zero': self.mask_zero,\n","                  'input_length': self.input_length}\n","        base_config = super(Embedding, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","\n","# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","# ==============================================================================\n","\"\"\"Embedding layer.\n","\"\"\"\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","from tensorflow.python.distribute import sharded_variable\n","from tensorflow.python.eager import context\n","from tensorflow.python.framework import ops\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.keras import constraints\n","from tensorflow.python.keras import initializers\n","from tensorflow.python.keras import regularizers\n","from tensorflow.python.keras.engine.base_layer import Layer\n","from tensorflow.python.keras.utils import tf_utils\n","from tensorflow.python.ops import embedding_ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.util.tf_export import keras_export\n","\n","\n","@keras_export('keras.layers.Embedding')\n","class Embedding(Layer):\n","  \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n","\n","  e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n","\n","  This layer can only be used as the first layer in a model.\n","\n","  Example:\n","\n","  >>> model = tf.keras.Sequential()\n","  >>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n","  >>> # The model will take as input an integer matrix of size (batch,\n","  >>> # input_length), and the largest integer (i.e. word index) in the input\n","  >>> # should be no larger than 999 (vocabulary size).\n","  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n","  >>> # dimension.\n","  >>> input_array = np.random.randint(1000, size=(32, 10))\n","  >>> model.compile('rmsprop', 'mse')\n","  >>> output_array = model.predict(input_array)\n","  >>> print(output_array.shape)\n","  (32, 10, 64)\n","\n","  Arguments:\n","    input_dim: Integer. Size of the vocabulary,\n","      i.e. maximum integer index + 1.\n","    output_dim: Integer. Dimension of the dense embedding.\n","    embeddings_initializer: Initializer for the `embeddings`\n","      matrix (see `keras.initializers`).\n","    embeddings_regularizer: Regularizer function applied to\n","      the `embeddings` matrix (see `keras.regularizers`).\n","    embeddings_constraint: Constraint function applied to\n","      the `embeddings` matrix (see `keras.constraints`).\n","    mask_zero: Boolean, whether or not the input value 0 is a special \"padding\"\n","      value that should be masked out.\n","      This is useful when using recurrent layers\n","      which may take variable length input.\n","      If this is `True`, then all subsequent layers\n","      in the model need to support masking or an exception will be raised.\n","      If mask_zero is set to True, as a consequence, index 0 cannot be\n","      used in the vocabulary (input_dim should equal size of\n","      vocabulary + 1).\n","    input_length: Length of input sequences, when it is constant.\n","      This argument is required if you are going to connect\n","      `Flatten` then `Dense` layers upstream\n","      (without it, the shape of the dense outputs cannot be computed).\n","\n","  Input shape:\n","    2D tensor with shape: `(batch_size, input_length)`.\n","\n","  Output shape:\n","    3D tensor with shape: `(batch_size, input_length, output_dim)`.\n","  \"\"\"\n","\n","  def __init__(self,\n","               input_dim,\n","               output_dim,\n","               embeddings_initializer='uniform',\n","               embeddings_regularizer=None,\n","               activity_regularizer=None,\n","               embeddings_constraint=None,\n","               mask_zero=False,\n","               input_length=None,\n","               **kwargs):\n","    if 'input_shape' not in kwargs:\n","      if input_length:\n","        kwargs['input_shape'] = (input_length,)\n","      else:\n","        kwargs['input_shape'] = (None,)\n","    if input_dim <= 0 or output_dim <= 0:\n","      raise ValueError('Both `input_dim` and `output_dim` should be positive, '\n","                       'found input_dim {} and output_dim {}'.format(\n","                           input_dim, output_dim))\n","    dtype = kwargs.pop('dtype', K.floatx())\n","    # We set autocast to False, as we do not want to cast floating- point inputs\n","    # to self.dtype. In call(), we cast to int32, and casting to self.dtype\n","    # before casting to int32 might cause the int32 values to be different due\n","    # to a loss of precision.\n","    kwargs['autocast'] = False\n","    super(Embedding, self).__init__(dtype=dtype, **kwargs)\n","\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","    self.embeddings_initializer = initializers.get(embeddings_initializer)\n","    self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n","    self.activity_regularizer = regularizers.get(activity_regularizer)\n","    self.embeddings_constraint = constraints.get(embeddings_constraint)\n","    self.mask_zero = mask_zero\n","    self.supports_masking = mask_zero\n","    self.input_length = input_length\n","\n","  @tf_utils.shape_type_conversion\n","  def build(self, input_shape):\n","    # Note: most sparse optimizers do not have GPU kernels defined. When\n","    # building graphs, the placement algorithm is able to place variables on CPU\n","    # since it knows all kernels using the variable only exist on CPU.\n","    # When eager execution is enabled, the placement decision has to be made\n","    # right now. Checking for the presence of GPUs to avoid complicating the\n","    # TPU codepaths which can handle sparse optimizers. But if we are within\n","    # a tf.function, we go back the graph mode logic and rely on the placer.\n","    if context.executing_eagerly() and context.context().num_gpus():\n","      with ops.device('cpu:0'):\n","        self.embeddings = self.add_weight(\n","            shape=(self.input_dim, self.output_dim),\n","            initializer=self.embeddings_initializer,\n","            name='embeddings',\n","            regularizer=self.embeddings_regularizer,\n","            constraint=self.embeddings_constraint)\n","    else:\n","      self.embeddings = self.add_weight(\n","          shape=(self.input_dim, self.output_dim),\n","          initializer=self.embeddings_initializer,\n","          name='embeddings',\n","          regularizer=self.embeddings_regularizer,\n","          constraint=self.embeddings_constraint)\n","    self.built = True\n","\n","  def compute_mask(self, inputs, mask=None):\n","    if not self.mask_zero:\n","      return None\n","\n","    return math_ops.not_equal(inputs, 0)\n","\n","  @tf_utils.shape_type_conversion\n","  def compute_output_shape(self, input_shape):\n","    if self.input_length is None:\n","      return input_shape + (self.output_dim,)\n","    else:\n","      # input_length can be tuple if input is 3D or higher\n","      if isinstance(self.input_length, (list, tuple)):\n","        in_lens = list(self.input_length)\n","      else:\n","        in_lens = [self.input_length]\n","      if len(in_lens) != len(input_shape) - 1:\n","        raise ValueError('\"input_length\" is %s, '\n","                         'but received input has shape %s' % (str(\n","                             self.input_length), str(input_shape)))\n","      else:\n","        for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n","          if s1 is not None and s2 is not None and s1 != s2:\n","            raise ValueError('\"input_length\" is %s, '\n","                             'but received input has shape %s' % (str(\n","                                 self.input_length), str(input_shape)))\n","          elif s1 is None:\n","            in_lens[i] = s2\n","      return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n","\n","  def call(self, inputs):\n","    dtype = K.dtype(inputs)\n","    if dtype != 'int32' and dtype != 'int64':\n","      inputs = math_ops.cast(inputs, 'int32')\n","    if isinstance(self.embeddings, sharded_variable.ShardedVariable):\n","      out = embedding_ops.embedding_lookup_v2(self.embeddings.variables, inputs)\n","    else:\n","      out = embedding_ops.embedding_lookup_v2(self.embeddings, inputs)\n","    return out\n","\n","  def get_config(self):\n","    config = {\n","        'input_dim': self.input_dim,\n","        'output_dim': self.output_dim,\n","        'embeddings_initializer':\n","            initializers.serialize(self.embeddings_initializer),\n","        'embeddings_regularizer':\n","            regularizers.serialize(self.embeddings_regularizer),\n","        'activity_regularizer':\n","            regularizers.serialize(self.activity_regularizer),\n","        'embeddings_constraint':\n","            constraints.serialize(self.embeddings_constraint),\n","        'mask_zero': self.mask_zero,\n","        'input_length': self.input_length\n","    }\n","    base_config = super(Embedding, self).get_config()\n","    return dict(list(base_config.items()) + list(config.items()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ujoT6wAOHSkA","cellView":"form"},"source":["# @title Transformer\n","# http://jalammar.github.io/illustrated-transformer/\n","# Transformers do not require that the sequential data be processed in order. \n","# For example, if the input data is a natural language sentence, the Transformer\n","# does not need to process the beginning of it before the end. Due to this\n","# feature, the Transformer allows for much more parallelization than RNNs and\n","# therefore reduced training times.\n","#taken from tensorflow.com\n","# Positional encoding\n","  def positional_encoding(position, d_model):\n","    def get_angles(pos, i, d_model):\n","      angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","      return pos * angle_rates\n","    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :],  d_model)\n","    \n","    # apply sin to even indices in the array; 2i\n","    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","    # apply cos to odd indices in the array; 2i+1\n","    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","      \n","    pos_encoding = angle_rads[np.newaxis, ...]\n","      \n","    return tf.cast(pos_encoding, dtype=tf.float32)\n","# Masking\n","  def create_padding_mask(seq):\n","    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","    # add extra dimensions to add the padding to the attention logits.\n","    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n","  def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask  # (seq_len, seq_len)\n","# Scaled dot product attention\n","  def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","      q, k, v must have matching leading dimensions.\n","      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","      The mask has different shapes depending on its type(padding or look ahead) \n","      but it must be broadcastable for addition.\n","      \n","      Args:\n","        q: query shape == (..., seq_len_q, depth)\n","        k: key shape == (..., seq_len_k, depth)\n","        v: value shape == (..., seq_len_v, depth_v)\n","        mask: Float tensor with shape broadcastable \n","              to (..., seq_len_q, seq_len_k). Defaults to None.\n","        \n","      Returns:\n","        output, attention_weights\n","    \"\"\"\n","    #matmul(Q,K)\n","    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (batch_size, seq_len_q, seq_len_k)\n","    #scale()\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","    #mask()\n","    # add the mask to the scaled tensor.\n","    if mask is not None: scaled_attention_logits += (mask * -1e9)\n","    #softmax()\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (batch_size, seq_len_q, seq_len_k)\n","    #matmul(scale,V)\n","    output = tf.matmul(attention_weights, v)  # (batch_size, seq_len_q, depth_v)\n","    return output, attention_weights\n","# Multi-head attention\n","  class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model,#output units\n","                num_heads=8):\n","      super(MultiHeadAttention, self).__init__()\n","      self.num_heads = num_heads\n","      self.d_model = d_model\n","      \n","      assert d_model % self.num_heads == 0,f'output unit \"d_model\" : {d_model} must be divisible by number of heads \"num_heads\": {num_heads}'\n","      \n","      self.depth = d_model // self.num_heads\n","      #Linear() In\n","      self.wq = tf.keras.layers.Dense(d_model)\n","      self.wk = tf.keras.layers.Dense(d_model)\n","      self.wv = tf.keras.layers.Dense(d_model)\n","      #Linear() Out\n","      self.dense = tf.keras.layers.Dense(d_model)\n","    #heads\n","    def split_heads(self, QKV, batch_size):\n","      \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","      \"\"\"\n","      QKV = tf.reshape(QKV, (batch_size, -1, self.num_heads, self.depth))#(batch_size, seq_len, num_heads, depth)\n","      return tf.transpose(QKV, perm=[0, 2, 1, 3]) #(batch_size, num_heads, seq_len, depth)\n","      \n","    def call(self, q, k, v, mask):\n","      batch_size = tf.shape(q)[0]\n","      #Linear() In\n","      q = self.wq(q)  # (batch_size, seq_len, d_model)\n","      k = self.wk(k)  # (batch_size, seq_len, d_model)\n","      v = self.wv(v)  # (batch_size, seq_len, d_model)\n","      #heads()\n","      q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","      k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","      v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","      #attention()\n","      # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","      # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","      scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","      #concat()\n","      scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","      concat_attention = tf.reshape(scaled_attention,(batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","      #Linear() Out\n","      output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","          \n","      return output, attention_weights\n","# Point wise feed forward network\n","  def point_wise_feed_forward_network(d_model,#output units\n","                                      dff):#hidden units\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])\n","# Encoder layer\n","  class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model,#output units\n","                num_heads,#parallel heads\n","                dff,#feedforward hidden units\n","                rate=0.1):#dropout rate\n","      super(EncoderLayer, self).__init__()\n","\n","      self.mha = MultiHeadAttention(d_model, num_heads)\n","      self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","      self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","      self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","      \n","      self.dropout1 = tf.keras.layers.Dropout(rate)\n","      self.dropout2 = tf.keras.layers.Dropout(rate)\n","      \n","    def call(self, inputs, training, mask):\n","      #Multi-Head-Attention\n","      attn_output, _ = self.mha(inputs,inputs,inputs, mask)  # (batch_size, input_seq_len, d_model)\n","      attn_output = self.dropout1(attn_output, training=training)\n","      #add and Normalization\n","      out1 = self.layernorm1(inputs + attn_output)  # (batch_size, input_seq_len, d_model)\n","      #feedforwardNetwork\n","      ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","      ffn_output = self.dropout2(ffn_output, training=training)\n","      #add and Normalization\n","      out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","      \n","      return out2\n","# Decoder layer\n","  class DecoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, d_model,#output units\n","                num_heads,#parallel heads\n","                dff,#feedforward hidden units\n","                rate=0.1):#dropout\n","      super(DecoderLayer, self).__init__()\n","\n","      self.mha = MultiHeadAttention(d_model, num_heads)\n","      self.ffn = point_wise_feed_forward_network(d_model, dff)\n","  \n","      self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","      self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","      \n","      self.dropout1 = tf.keras.layers.Dropout(rate)\n","      self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def block(self, q, k, v, mask):\n","      #Multi-Head-Attention\n","      attn, attn_weights = self.mha(q, k, v, mask)# (batch_size, target_seq_len, d_model)\n","      attn = self.dropout1(attn, training=training)\n","      #add and Normalization\n","      out = self.layernorm1(attn + v)# (batch_size, target_seq_len, d_model)\n","      return out, attn_weights\n","\n","    def call(self, inputs, enc_output, training, look_ahead_mask, padding_mask):\n","      # enc_output.shape == (batch_size, input_seq_len, d_model)\n","      #block-1\n","      out1, attn_weights_block1 = self.block(inputs,inputs,inputs, look_ahead_mask)\n","      #block-2\n","      out2, attn_weights_block2 = self.block(enc_output, enc_output, out1, padding_mask)    \n","      #feedforwardnetwork\n","      ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","      ffn_output = self.dropout2(ffn_output, training=training)\n","      #add and Normalization\n","      out3 = self.layernorm2(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","      \n","      return out3, attn_weights_block1, attn_weights_block2\n","# Encoder\n","  class Encoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers,#encoder layers\n","                d_model,#output units\n","                num_heads,#parallel heads\n","                dff,#feedforward hidden units\n","                input_vocab_size,#embedding input size\n","                maximum_position_encoding,#max timesteps or words\n","                rate=0.1):#dropout\n","      super(Encoder, self).__init__()\n","\n","      self.num_layers = num_layers\n","      self.d_model = d_model\n","      #Embedding\n","      self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","      #positional encoding\n","      self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)    \n","      #list of encoding layers\n","      self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","      #dropout layer\n","      self.dropout = tf.keras.layers.Dropout(rate)\n","          \n","    def call(self, inputs, training, mask):\n","\n","      seq_len = tf.shape(inputs)[1]\n","      \n","      # embedding + position encoding\n","      inputs = self.embedding(inputs)  # (batch_size, input_seq_len, d_model)\n","      inputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","      inputs += self.pos_encoding[:, :seq_len, :]\n","      #dropout\n","      inputs = self.dropout(inputs, training=training)\n","      #encoding layers\n","      for i in range(self.num_layers): inputs = self.enc_layers[i](inputs, training, mask)\n","      \n","      return inputs  # (batch_size, input_seq_len, d_model)\n","# Decoder\n","  class Decoder(tf.keras.layers.Layer):\n","    def __init__(self, num_layers,#decoding layers\n","                d_model,#output units\n","                num_heads,#parallel heads\n","                dff,#feedforwardnetword hidden units\n","                target_vocab_size,#embedding input size\n","                maximum_position_encoding,#max timesteps or words\n","                rate=0.1):#dropout\n","      super(Decoder, self).__init__()\n","      \n","      self.num_layers = num_layers\n","      self.d_model = d_model   \n","      #Embedding\n","      self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","      #positional encoding\n","      self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","      #decoding layers\n","      self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","      #dropout\n","      self.dropout = tf.keras.layers.Dropout(rate)\n","      \n","    def call(self, targets, enc_output, training, look_ahead_mask, padding_mask):\n","\n","      seq_len = tf.shape(targets)[1]\n","      attention_weights = {}\n","      # embeddings + positional encodings\n","      targets = self.embedding(targets)  # (batch_size, target_seq_len, d_model)\n","      targets *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","      targets += self.pos_encoding[:, :seq_len, :]\n","      #dropout\n","      targets = self.dropout(targets, training=training)\n","      #decoding layer\n","      for i in range(self.num_layers):\n","        targets, block1, block2 = self.dec_layers[i](targets, enc_output,\n","                                                    training,\n","                                                    look_ahead_mask,\n","                                                    padding_mask)\n","        \n","        attention_weights[f'decoder_layer{i+1}_block1'] = block1\n","        attention_weights[f'decoder_layer{i+1}_block2'] = block2\n","      \n","      # targets.shape == (batch_size, target_seq_len, d_model)\n","      return targets, attention_weights\n","# Transformer\n","  class Transformer(tf.keras.Model):\n","    def __init__(self, num_layers, #encoder and decoder layers\n","                d_model,#outupt units\n","                num_heads,#parallel heads\n","                dff,#feedforwardnetwork hidden units\n","                input_vocab_size, #embedding input size\n","                target_vocab_size,#embedding input size\n","                pe_input,#max timesteps or words\n","                pe_target,#max timesteps or words\n","                rate=0.1):#dropout\n","      super(Transformer, self).__init__()\n","      #encoder\n","      self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","      #decoder\n","      self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","      #linear output layer\n","      self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","      \n","    def call(self, inp, tar, training, enc_padding_mask, dec_padding_mask, look_ahead_mask):\n","      #encoder\n","      enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","      #decoder\n","      # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","      dec_output, attention_weights = self.decoder(tar, enc_output,\n","                                                  training, look_ahead_mask,dec_padding_mask)\n","      #output linear layer\n","      final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","      \n","      return final_output, attention_weights\n","# hyperparameters\n","  num_layers = 4#encoder and decoder layers\n","  d_model = 128#output units\n","  dff = 512#feedforward hidden units\n","  num_heads = 8#parallel heads\n","\n","  input_vocab_size = tokenizer_pt.vocab_size + 2#embeddings input size\n","  target_vocab_size = tokenizer_en.vocab_size + 2#embeddings input size\n","  dropout_rate = 0.1\n","# Optimizer\n","  class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model,#output units\n","                warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","\n","  learning_rate = CustomSchedule(d_model)\n","  optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","# Loss and metrics\n","  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","  def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    loss_ = loss_object(real, pred)\n","    # Since the target sequences are padded, \n","    # it is important to apply a padding mask when calculating the loss.\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    loss_ *= mask\n","    \n","    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n","  train_loss = tf.keras.metrics.Mean(name='train_loss')\n","  train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","# create mask\n","  def create_masks(inp, tar):\n","    # Encoder padding mask\n","    enc_padding_mask = create_padding_mask(inp)\n","    \n","    # Used in the 2nd attention block in the decoder.\n","    # This padding mask is used to mask the encoder outputs.\n","    dec_padding_mask = create_padding_mask(inp)\n","    \n","    # Used in the 1st attention block in the decoder.\n","    # It is used to pad and mask future tokens in the input received by \n","    # the decoder.\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","    \n","    return enc_padding_mask, combined_mask, dec_padding_mask\n","# chekpoint manager\n","  checkpoint_path = \"./checkpoints/train\"\n","  ckpt = tf.train.Checkpoint(transformer=transformer,optimizer=optimizer)\n","  ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","  # if a checkpoint exists, restore the latest checkpoint.\n","  if ckpt_manager.latest_checkpoint:\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","    print ('Latest checkpoint restored!!')\n","transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)\n","'''Exception occured-----\n","  The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. tar_real is that same input shifted by 1: At each location in tar_input, tar_real contains the next token that should be predicted.\n","  For example, sentence = \"SOS A lion in the jungle is sleeping EOS\"\n","  tar_inp = \"SOS A lion in the jungle is sleeping\"\n","  tar_real = \"A lion in the jungle is sleeping EOS\"'''\n","# train_step\n","  # The @tf.function trace-compiles train_step into a TF graph for faster\n","  # execution. The function specializes to the precise shape of the argument\n","  # tensors. To avoid re-tracing due to the variable sequence lengths or variable\n","  # batch sizes (the last batch is smaller), use input_signature to specify\n","  # more generic shapes.\n","\n","  train_step_signature = [\n","      tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","      tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","  ]\n","\n","  @tf.function(input_signature=train_step_signature)\n","  def train_step(inp, tar):\n","    tar_inp = tar[:, :-1]\n","    tar_real = tar[:, 1:]\n","    \n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","    \n","    with tf.GradientTape() as tape:\n","      predictions, _ = transformer(inp, tar_inp, \n","                                  True, \n","                                  enc_padding_mask, \n","                                  combined_mask, \n","                                  dec_padding_mask)\n","      loss = loss_function(tar_real, predictions)\n","\n","    gradients = tape.gradient(loss, transformer.trainable_variables)    \n","    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","    \n","    train_loss(loss)\n","    train_accuracy(tar_real, predictions)\n","# training loop\n","  for epoch in range(EPOCHS):\n","    start = time.time()\n","    \n","    train_loss.reset_states()\n","    train_accuracy.reset_states()\n","    \n","    # inp -> portuguese, tar -> english\n","    for (batch, (inp, tar)) in enumerate(train_dataset):\n","      train_step(inp, tar)\n","      \n","      if batch % 50 == 0:\n","        print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","            epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","        \n","    if (epoch + 1) % 5 == 0:\n","      ckpt_save_path = ckpt_manager.save()\n","      print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                          ckpt_save_path))\n","      \n","    print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                  train_loss.result(), \n","                                                  train_accuracy.result()))\n","\n","    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n","# Evaluate\n","  # The following steps are used for evaluation:\n","    # Encode the input sentence using the Portuguese tokenizer (tokenizer_pt). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n","    # The decoder input is the start token == tokenizer_en.vocab_size.\n","    # Calculate the padding masks and the look ahead masks.\n","    # The decoder then outputs the predictions by looking at the encoder output and its own output (self-attention).\n","    # Select the last word and calculate the argmax of that.\n","    # Concatentate the predicted word to the decoder input as pass it to the decoder.\n","    # In this approach, the decoder predicts the next word based on the previous words it predicted.\n","  def evaluate(inp_sentence):\n","    start_token = [tokenizer_pt.vocab_size]\n","    end_token = [tokenizer_pt.vocab_size + 1]\n","    \n","    # inp sentence is portuguese, hence adding the start and end token\n","    inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n","    encoder_input = tf.expand_dims(inp_sentence, 0)\n","    \n","    # as the target is english, the first word to the transformer should be the\n","    # english start token.\n","    decoder_input = [tokenizer_en.vocab_size]\n","    output = tf.expand_dims(decoder_input, 0)\n","      \n","    for i in range(MAX_LENGTH):\n","      enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n","          encoder_input, output)\n","    \n","      # predictions.shape == (batch_size, seq_len, vocab_size)\n","      predictions, attention_weights = transformer(encoder_input, \n","                                                  output,\n","                                                  False,\n","                                                  enc_padding_mask,\n","                                                  combined_mask,\n","                                                  dec_padding_mask)\n","      \n","      # select the last word from the seq_len dimension\n","      predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n","\n","      predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","      \n","      # return the result if the predicted_id is equal to the end token\n","      if predicted_id == tokenizer_en.vocab_size+1:\n","        return tf.squeeze(output, axis=0), attention_weights\n","      \n","      # concatentate the predicted_id to the output which is given to the decoder\n","      # as its input.\n","      output = tf.concat([output, predicted_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights\n","# plot attension weights\n","  def plot_attention_weights(attention, sentence, result, layer):\n","    fig = plt.figure(figsize=(16, 8))\n","    \n","    sentence = tokenizer_pt.encode(sentence)\n","    \n","    attention = tf.squeeze(attention[layer], axis=0)\n","    \n","    for head in range(attention.shape[0]):\n","      ax = fig.add_subplot(2, 4, head+1)\n","      \n","      # plot the attention weights\n","      ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","      fontdict = {'fontsize': 10}\n","      \n","      ax.set_xticks(range(len(sentence)+2))\n","      ax.set_yticks(range(len(result)))\n","      \n","      ax.set_ylim(len(result)-1.5, -0.5)\n","          \n","      ax.set_xticklabels(\n","          ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n","          fontdict=fontdict, rotation=90)\n","      \n","      ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n","                          if i < tokenizer_en.vocab_size], \n","                        fontdict=fontdict)\n","      \n","      ax.set_xlabel('Head {}'.format(head+1))\n","    \n","    plt.tight_layout()\n","    plt.show()\n","# translate\n","  def translate(sentence, plot=''):\n","    result, attention_weights = evaluate(sentence)\n","    \n","    predicted_sentence = tokenizer_en.decode([i for i in result \n","                                              if i < tokenizer_en.vocab_size])  \n","\n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(predicted_sentence))\n","    \n","    if plot:\n","      plot_attention_weights(attention_weights, sentence, result, plot)\n","  #use\n","  translate(\"este é um problema que temos que resolver.\")\n","  #or You can pass different layers and attention blocks of the decoder to the plot parameter.\n","  translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dudjSHgLZvT-","cellView":"form"},"source":["# @title Reinforcement Learning DDPG1\n","  # steps:\n","    # the Agent observing the Environment and receiving a state and a reward.\n","    # The Agent uses this state and reward to decide the next action to take .\n","    # The Agent then sends an action to the Environment in an attempt to control it in a favorable way.\n","    # the environment transitions and its internal state changes as a consequence of the previous state and the Agent’s action\n","  # episode:\n","    # The task the Agent is trying to solve may or may not have a natural ending.\n","    # Tasks that have a natural ending, such as a game, are called \"episodic tasks\".\n","    # Conversely, tasks that do not are called \"continuing tasks\", such as learning forward motion.\n","    # The sequence of time steps from the beginning to the end of an episodic task is called an \"episode\".\n","  # Return(delayed reward):\n","    # As we will see, Agents may take several time steps and episodes to learn how to solve a task.\n","    # The sum of rewards collected in a single episode is called a return.\n","    # Agents are often designed to maximize the return.\n","  # Exploration and Exploitation:\n","    #an Agent has to exploit what it has already experienced in order to obtain as much reward as possible,\n","    # but at the same time, it also has to explore in order to make select better action in the future. \n","\n","  from collections import deque\n","  import random, gym, tflearn, argparse\n","  from gym import wrappers\n","  import pprint as pp\n","\n","class ReplayBuffer(object):# return a randomly chosen batch of experiences when queried.\n","  def __init__(self, buffer_size, random_seed=123):\n","    \n","    self.buffer_size = buffer_size# experience lenght or size\n","    self.count = 0  #total experience in buffer\n","    self.buffer = deque()  #[old ,..., new]#experience bucket\n","    random.seed(random_seed)  #random experience choice\n","\n","  #add experience to bucket or buffer\n","  def add(self, s, a, r, t, s2):\n","    experience = (s, a, r, t, s2)\n","    if self.count < self.buffer_size: #if bucket not full\n","      self.buffer.append(experience)\n","      self.count += 1\n","    else:# remove old experience\n","      self.buffer.popleft()\n","      self.buffer.append(experience)\n","\n","  def size(self): return self.count\n","  #fetch a sample of lenght batch_size\n","  def sample_batch(self, batch_size):# number of experiences to  fetch form buffer bucket\n","    batch = []\n","    if self.count < batch_size:   # if buffer is less than batch_size\n","        batch = random.sample(self.buffer, self.count)\n","    else:# randomly fetch batch_size experience from buffer\n","        batch = random.sample(self.buffer, batch_size)\n","    #separate experience into catogaries\n","      s_batch = np.array([_[0] for _ in batch])#state\n","      a_batch = np.array([_[1] for _ in batch])#action\n","      r_batch = np.array([_[2] for _ in batch])#reward\n","      t_batch = np.array([_[3] for _ in batch])#terminal\n","      s2_batch = np.array([_[4] for _ in batch])#next state\n","\n","    return s_batch, a_batch, r_batch, t_batch, s2_batch\n","\n","  #clear all experience\n","  def clear(self):\n","    self.buffer.clear()\n","    self.count = 0\n","\n","class ActorNetwork(object):# policy function\n","    \"\"\"Inputs:state\n","    outputs: action\n","    a = deterministicPolicy(s) \"\"\"\n","\n","    def __init__(self, sess, state_dim, action_dim, action_bound, hidden_units, learning_rate, tau, batch_size):\n","        #parameters\n","          self.sess = sess# The tensorflow session\n","          self.s_dim = state_dim # int: denoting the dimensionality of the states in the current problem\n","          self.a_dim = action_dim# int: denoting the dimensionality of the actions in the current problem\n","          self.action_bound = action_bound  # range [-2,2]\n","          self.hidden_units=hidden_units, # hidden layer units\n","          self.learning_rate = learning_rate\n","          self.tau = tau# rate or speed at which the target model will track the main model.\n","          self.batch_size = batch_size\n","\n","        # Actor Network\n","          self.inputs, self.out, self.scaled_out = self.create_actor_network()\n","          self.model_weights = tf.trainable_variables()#weights\n","        # Target Network\n","          self.target_inputs, self.target_out, self.target_scaled_out = self.create_actor_network()\n","          self.target_model_weights = tf.trainable_variables() [len(self.model_weights):]#weights\n","        # updating target network with actor network weights\n","          self.update_target_model_weights = \\\n","              [self.target_model_weights[i].assign(tf.multiply(self.model_weights[i], self.tau) +\n","                                                  tf.multiply(self.target_model_weights[i], (1. - self.tau))\n","                                                  )for i in range(len(self.target_model_weights))]\n","        #get gradients\n","          # This gradient will be provided by the critic network\n","          self.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n","          # Combine the gradients here\n","          self.unnormalized_actor_gradients = tf.gradients(\n","              self.scaled_out, self.model_weights, -self.action_gradient)\n","          self.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n","          self.gradients = zip(self.actor_gradients, self.model_weights)\n","        # Optimizer\n","          self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(self.gradients)\n","        #total trainable weights\n","          self.num_trainable_vars = len(self.model_weights) + len(self.target_model_weights)\n","    #actor and target model definition\n","    def create_actor_network(self):\n","        inputs = Input(shape=[None, self.s_dim])# state\n","        net = Dense(self.hidden[0], activation='relu')(inputs)\n","        net = BatchNormalization()(net)\n","        net = Dense(self.hidden[1], activation='relu')(net)\n","        net = BatchNormalization()(net)\n","        # Final layer weights are init to Uniform[-3e-3, 3e-3]\n","        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n","        # \"tanh\": to map continues actions ranging [-2,2]\n","        outputs = Dense(self.a_dim, activation='tanh', weights_init=w_init, name= 'outputs')(net)# action\n","        # Scale output to -action_bound to action_bound\n","        scaled_out = tf.multiply(outputs, self.action_bound)\n","        return inputs, outputs, scaled_out\n","    # Updates the weights of the actor network\n","    def train(self, states, a_gradient):  self.sess.run(self.optimize, feed_dict={self.inputs: states,self.action_gradient: a_gradient})\n","    def predict(self, inputs):  return self.sess.run(self.scaled_out, feed_dict={self.inputs: inputs})\n","    def predict_target(self, inputs): return self.sess.run(self.target_scaled_out, feed_dict={self.target_inputs: inputs})\n","    # Updates the weights of the target network to track the main network at the speed defined by tau\n","    def update_target_network(self):  self.sess.run(self.update_target_model_weights)\n","    def get_num_trainable_vars(self): return self.num_trainable_vars\n","\n","class CriticNetwork(object):# value function\n","  \"\"\"Inputs:state and action (action is obtained from the output of the Actor network), output: Q(s,a).\"\"\"\n","\n","  def __init__(self, sess, state_dim, action_dim, hidden_units, learning_rate, tau, gamma, num_actor_vars):\n","      \n","      #parameters\n","          self.sess = sess# The tensorflow session\n","          self.s_dim = state_dim # int: denoting the dimensionality of the states in the current problem\n","          self.a_dim = action_dim# int: denoting the dimensionality of the actions in the current problem\n","          #self.action_bound = action_bound  # range [-2,2]\n","          self.hidden_units=hidden_units, # hidden layer units\n","          self.learning_rate = learning_rate\n","          self.tau = tau# rate or speed at which the target model will track the main model.\n","          self.gamma = gamma  #discount factor\n","          self.num_actor_vars = num_actor_vars\n","          # self.batch_size = batch_size\n","\n","      # Create the critic network\n","        self.inputs, self.action, self.out = self.create_critic_network()\n","        self.model_weights = tf.trainable_variables()[num_actor_vars:]  #weights\n","      # Target Network\n","        self.target_inputs, self.target_action, self.target_out = self.create_critic_network()\n","        self.target_model_weights = tf.trainable_variables()[(len(self.model_weights) + num_actor_vars):] #weights\n","      # periodically updating target network with online network  weights with regularization\n","        self.update_target_model_weights = \\\n","            [self.target_model_weights[i].assign(tf.multiply(self.model_weights[i], self.tau) +\n","                                                tf.multiply(self.target_model_weights[i], (1. - self.tau))\n","                                                )for i in range(len(self.target_model_weights))]\n","      # Network target (y_i)\n","        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n","      #loss\n","        self.loss = tflearn.mean_square(self.predicted_q_value, self.out)\n","      # optmizer\n","        self.optimize = tf.train.AdamOptimizer(self.learning_rate)   .minimize(self.loss)\n","      # get action gradients\n","        self.action_grads = tf.gradients(self.out, self.action)\n","  # critit and target network\n","  def create_critic_network(self):\n","      inputs = Input(shape=[None, self.s_dim])\n","      action = Input(shape=[None, self.a_dim])\n","      net = Dense(self.hidden[1], activation='relu')(inputs)\n","      net = BatchNormalization()(net)\n","      # Use two temp layers to get the corresponding weights and biases\n","        t1 = Dense(self.hidden[0])(net)\n","        t2 = Dense(self.hidden[0])(action)\n","      # Add the action tensor in the 2nd hidden layer\n","        net = tflearn.activation(   tf.matmul(net, t1.W) + tf.matmul(action, t2.W) + t2.b, activation='relu'    )\n","      # Weights are init to Uniform[-3e-3, 3e-3]\n","        w_init = tflearn.initializations.uniform(minval=-0.003, maxval=0.003)\n","      # linear layer connected to 1 output representing Q(s,a)\n","        out = Dense(1, weights_init=w_init)(net)\n","      return inputs, action, out\n","  # train ciritc network\n","  def train(self, inputs, action, predicted_q_value):\n","      return self.sess.run([self.out, self.optimize], feed_dict={\n","                                                                    self.inputs: inputs,\n","                                                                    self.action: action,\n","                                                                    self.predicted_q_value: predicted_q_value\n","                                                                })\n","\n","  def predict(self, inputs, action):\n","      return self.sess.run(self.out, feed_dict={  self.inputs: inputs,    self.action: action   })\n","\n","  def predict_target(self, inputs, action):\n","      return self.sess.run(self.target_out, feed_dict={ self.target_inputs: inputs,   self.target_action: action  })\n","\n","  def action_gradients(self, inputs, actions):\n","      return self.sess.run(self.action_grads, feed_dict={ self.inputs: inputs,  self.action: actions  })\n","\n","  def update_target_network(self):  self.sess.run(self.update_target_model_weights)\n","# for Exploration\n","class ActionNoise():# \"OrnsteinUhlenbeckActionNoise\" action noise\n","    def __init__(self, mu, sigma=0.3, theta=.15, dt=1e-2, x0=None):\n","      #parameters\n","        self.theta = theta\n","        self.mu = mu\n","        self.sigma = sigma\n","        self.dt = dt\n","        self.x0 = x0\n","        self.reset()\n","\n","    def reset(self):  self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n","\n","    def __call__(self):\n","        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n","                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n","        self.x_prev = x\n","        return x\n","\n","    def __repr__(self): return 'ActionNoise(mu={}, sigma={})'.format(self.mu, self.sigma)\n","\n","def build_summaries():# summary operation\n","    episode_reward = tf.Variable(0.)\n","    tf.summary.scalar(\"Reward\", episode_reward)\n","    episode_ave_max_q = tf.Variable(0.)\n","    tf.summary.scalar(\"Qmax Value\", episode_ave_max_q)\n","\n","    summary_vars = [episode_reward, episode_ave_max_q]\n","    summary_ops = tf.summary.merge_all()\n","\n","    return summary_ops, summary_vars\n","\n","# hyperparameters\n","  # agent parameters\n","  actor_lr  =0.0001\n","  critic_lr  =0.001\n","  gamma  =0.99\n","  tau  =0.001\n","  buffer_size =1000000\n","  minibatch_size=64 #samples of replay_buffer(s,a,r,t,s2)\n","\n","  # run parameters\n","  env  ='Pendulum-v0'\n","  random_seed =1234\n","  max_episodes =50000\n","  max_episode_len =1000\n","  monitor_dir  ='./results/gym_ddpg'\n","  summary_dir  ='./results/tf_ddpg'\n","  render_env=False\n","  use_gym_monitor=True\n","\n","# model or start point\n","  with tf.Session() as sess:\n","    #initialization\n","      #environment\n","        env = gym.make(env)\n","      #random seeding\n","        np.random.seed(random_seed)\n","        tf.set_random_seed(random_seed)\n","        env.seed(random_seed)\n","      # environment dimensions\n","        state_dim = env.observation_space.shape[0]\n","        action_dim = env.action_space.shape[0]\n","        action_bound = env.action_space.high\n","      # Ensure action bound is symmetric\n","        assert (env.action_space.high == -env.action_space.low)\n","      # Actor and critic Networks\n","        actor = ActorNetwork(sess, state_dim, action_dim, action_bound, float(actor_lr), float(tau),  minibatch_size)\n","        critic = CriticNetwork(sess, state_dim, action_dim, float(critic_lr), float(tau), float(gamma), actor.get_num_trainable_vars())\n","      #Action Noise\n","        actor_noise = ActionNoise(mu=np.zeros(action_dim))\n","      # Set up summary Ops\n","        summary_ops, summary_vars = build_summaries()\n","        sess.run(tf.global_variables_initializer())\n","        writer = tf.summary.FileWriter(summary_dir, sess.graph)\n","      # Initialize target network weights\n","        actor.update_target_network()\n","        critic.update_target_network()\n","      # Initialize replay memory\n","        replay_buffer = ReplayBuffer(buffer_size, random_seed)\n","      # Needed to enable BatchNorm. \n","        tflearn.is_training(True)\n","    if use_gym_monitor:\n","      if not render_env:  env = wrappers.Monitor( env, monitor_dir, video_callable=False, force=True)\n","      else: env = wrappers.Monitor(env, monitor_dir, force=True)\n","    # Training loop\n","      for i in range(max_episodes):# epochs\n","        #reset environment to initial state\n","          s = env.reset() \n","        #per episode reward and # per episode max reward q_value\n","          ep_reward = 0;        ep_ave_max_q = 0\n","        for j in range(max_episode_len):  #trials per episode\n","          #show environment\n","            if render_env:env.render()\n","          # Added exploration noise\n","            #                               (1,   3)        )) + (1. / (1. + i))\n","            a = actor.predict(np.reshape(s, (1, actor.s_dim))) + actor_noise()\n","          # act on environment and get next state and reward etc..\n","            s2, r, terminal, info = env.step(a[0])\n","          # update experience in replay_buffer\n","            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,  terminal, np.reshape(s2, (actor.s_dim,)))\n","          # get a sample_batach\n","            if replay_buffer.size() > minibatch_size: s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(minibatch_size)\n","          # Calculate targets and labels\n","            target_q = critic.predict_target( s2_batch, actor.predict_target(s2_batch))\n","            #labels\n","              y_i = []\n","              for k in range(minibatch_size):\n","                if t_batch[k]:  y_i.append(r_batch[k])#if terminal: then append reward\n","                else: y_i.append(r_batch[k] + critic.gamma * target_q[k])#append (raward + gamma * target)\n","          # Update the critic given the targets: q_values of all actions\n","            predicted_q_value, _ = critic.train(  s_batch, a_batch, np.reshape(y_i, (minibatch_size, 1)))\n","          # update per episode max reward: get optimal ation from list of actions\n","            ep_ave_max_q += np.amax(predicted_q_value)\n","          # Update the actor policy using the sampled gradient\n","            a_outs = actor.predict(s_batch) # a = actorPolicy(s)\n","            grads = critic.action_gradients(s_batch, a_outs)  #gradients\n","            actor.train(s_batch, grads[0])  # train or update\n","          # Update target networks\n","            actor.update_target_network()\n","            critic.update_target_network()\n","          # current state = next state\n","            s = s2\n","          # update per episode reward\n","            ep_reward += r\n","          # if episode terminal reached \n","            if terminal:\n","              # update summary\n","                summary_str = sess.run(summary_ops,\n","                                       feed_dict={summary_vars[0]: ep_reward,\n","                                                  summary_vars[1]: ep_ave_max_q / float(j)})\n","                writer.add_summary(summary_str, i)\n","                writer.flush()\n","              # print status and exit current episode\n","                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f}'%(int(ep_reward), i, (ep_ave_max_q / float(j))))\n","                break\n","\n","    if use_gym_monitor: env.monitor.close()\n","\n","#_______________for high lvl API\n","def loop(agent, world):\n","  # get state or input\n","  state = world.get_state()\n","  # get action or output\n","  action = agent.get_action(state)\n","  # act on world or get next input\n","  next_state, reward, done = world.act(action)\n","  # get experience\n","  agent.remember(state, action, reward, done, next_state)\n","  # update agent\n","  agent.train()\n","\n","def main():\n","    world = World()\n","    agent = DDPG(state_dim=world.state_dim, action_dim=world.action_dim)\n","    while True: loop(agent, world)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qYv_JDVs9fx","cellView":"form"},"source":["# @title DDPG2\n","# with 4 models( actor, actor_target, critic, critic_target)\n","\n","problem = \"Pendulum-v0\"\n","env = gym.make(problem)\n","\n","num_states = env.observation_space.shape[0] #3\n","num_actions = env.action_space.shape[0] #1\n","\n","\n","upper_bound = env.action_space.high[0]  #2\n","lower_bound = env.action_space.low[0] #-2\n","# ActionNoise\n","class OUActionNoise:\n","    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n","        self.theta = theta\n","        self.mean = mean\n","        self.std_dev = std_deviation\n","        self.dt = dt\n","        self.x_initial = x_initial\n","        self.reset()\n","\n","    def __call__(self):\n","        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n","        x = (\n","            self.x_prev\n","            + self.theta * (self.mean - self.x_prev) * self.dt\n","            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n","        )\n","        # Store x into x_prev\n","        # Makes next noise dependent on current one\n","        self.x_prev = x\n","        return x\n","\n","    def reset(self):\n","        if self.x_initial is not None:\n","            self.x_prev = self.x_initial\n","        else:\n","            self.x_prev = np.zeros_like(self.mean)\n","#replay buffer\n","class Buffer:\n","    def __init__(self, buffer_capacity=100000, batch_size=64):\n","        # Number of \"experiences\" to store at max\n","        self.buffer_capacity = buffer_capacity\n","        # Num of tuples to train on.\n","        self.batch_size = batch_size\n","\n","        # Its tells us num of times record() was called.\n","        self.buffer_counter = 0\n","\n","        # Instead of list of tuples as the exp.replay concept go\n","        # We use different np.arrays for each tuple element\n","        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n","        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n","        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n","        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n","\n","    # Takes (s,a,r,s') obervation tuple as input\n","    def record(self, obs_tuple):\n","        # Set index to zero if buffer_capacity is exceeded,\n","        # replacing old records\n","        index = self.buffer_counter % self.buffer_capacity\n","\n","        self.state_buffer[index] = obs_tuple[0]\n","        self.action_buffer[index] = obs_tuple[1]\n","        self.reward_buffer[index] = obs_tuple[2]\n","        self.next_state_buffer[index] = obs_tuple[3]\n","\n","        self.buffer_counter += 1\n","\n","    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n","    # TensorFlow to build a static graph out of the logic and computations in our function.\n","    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n","    @tf.function\n","    def update(self, state_batch, action_batch, reward_batch, next_state_batch,):\n","        #update critic network\n","        with tf.GradientTape() as tape:\n","            target_actions = target_actor(next_state_batch, training=True)\n","            y = reward_batch + gamma * target_critic( [next_state_batch, target_actions], training=True)\n","            critic_value = critic_model([state_batch, action_batch], training=True)\n","            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n","\n","        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n","        critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n","        # update actro network\n","        with tf.GradientTape() as tape:\n","            actions = actor_model(state_batch, training=True)\n","            critic_value = critic_model([state_batch, actions], training=True)\n","            # Used `-value` as we want to maximize the value given by the critic for our actions\n","            actor_loss = -tf.math.reduce_mean(critic_value)\n","\n","        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n","        actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n","\n","    # We compute the loss and update parameters\n","    def learn(self):\n","        # Get sampling range\n","        record_range = min(self.buffer_counter, self.buffer_capacity)\n","        # Randomly sample indices\n","        batch_indices = np.random.choice(record_range, self.batch_size)\n","\n","        # Convert to tensors\n","        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n","        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n","        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n","        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n","        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n","\n","        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n","\n","# This updates the target parameters slowly\n","# Based on rate `tau`, which is much less than one.\n","@tf.function\n","def update_target(target_weights, weights, tau):\n","    for (a, b) in zip(target_weights, weights):   a.assign(b * tau + a * (1 - tau))\n","\n","def get_actor():\n","    # Initialize weights between -3e-3 and 3-e3\n","    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n","\n","    inputs = layers.Input(shape=(num_states,))\n","    out = layers.Dense(256, activation=\"relu\")(inputs)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n","\n","    # Our upper bound is 2.0 for Pendulum.\n","    outputs = outputs * upper_bound\n","    model = tf.keras.Model(inputs, outputs)\n","    return model\n","\n","def get_critic():\n","    # State as input\n","    state_input = layers.Input(shape=(num_states))\n","    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n","    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n","\n","    # Action as input\n","    action_input = layers.Input(shape=(num_actions))\n","    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n","\n","    # Both are passed through seperate layer before concatenating\n","    concat = layers.Concatenate()([state_out, action_out])\n","\n","    out = layers.Dense(256, activation=\"relu\")(concat)\n","    out = layers.Dense(256, activation=\"relu\")(out)\n","    outputs = layers.Dense(1)(out)\n","\n","    # Outputs single value for give state-action\n","    model = tf.keras.Model([state_input, action_input], outputs)\n","\n","    return model\n","\n","# policy() returns an action sampled from our Actor network plus some noise for exploration.\n","def policy(state, noise_object):\n","    sampled_actions = tf.squeeze(actor_model(state))\n","    noise = noise_object()\n","    # Adding noise to action\n","    sampled_actions = sampled_actions.numpy() + noise\n","\n","    # We make sure action is within bounds\n","    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n","\n","    return [np.squeeze(legal_action)]\n","#hyperparameters\n","  std_dev = 0.2\n","  ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n","\n","  actor_model = get_actor()\n","  critic_model = get_critic()\n","\n","  target_actor = get_actor()\n","  target_critic = get_critic()\n","\n","  # Making the weights equal initially\n","  target_actor.set_weights(actor_model.get_weights())\n","  target_critic.set_weights(critic_model.get_weights())\n","\n","  # Learning rate for actor-critic models\n","  critic_lr = 0.002\n","  actor_lr = 0.001\n","\n","  critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n","  actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n","\n","  total_episodes = 100\n","  # Discount factor for future rewards\n","  gamma = 0.99\n","  # Used to update target networks\n","  tau = 0.005\n","\n","  buffer = Buffer(50000, 64)\n","\n","# Now we implement our main training loop, and iterate over episodes. We sample actions using policy() and train with learn() at each time step, along with updating the Target networks at a rate tau.\n","\n","# To store reward history of each episode\n","ep_reward_list = []\n","# To store average reward history of last few episodes\n","avg_reward_list = []\n","\n","for ep in range(total_episodes):\n","\n","    prev_state = env.reset()\n","    episodic_reward = 0\n","\n","    while True:\n","        # Uncomment this to see the Actor in action\n","        # But not in a python notebook.\n","        # env.render()\n","\n","        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n","\n","        action = policy(tf_prev_state, ou_noise)\n","        # Recieve state and reward from environment.\n","        state, reward, done, info = env.step(action)\n","\n","        buffer.record((prev_state, action, reward, state))\n","        episodic_reward += reward\n","\n","        buffer.learn()\n","        update_target(target_actor.variables, actor_model.variables, tau)\n","        update_target(target_critic.variables, critic_model.variables, tau)\n","\n","        # End this episode when `done` is True\n","        if done:\n","            break\n","\n","        prev_state = state\n","\n","    ep_reward_list.append(episodic_reward)\n","\n","    # Mean of last 40 episodes\n","    avg_reward = np.mean(ep_reward_list[-40:])\n","    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n","    avg_reward_list.append(avg_reward)\n","\n","# Plotting graph\n","# Episodes versus Avg. Rewards\n","plt.plot(avg_reward_list)\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Avg. Epsiodic Reward\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tMhQSsLndPpC"},"source":["##utilties"]},{"cell_type":"code","metadata":{"id":"AekWjvQy4rHx","cellView":"form"},"source":["#@title plot training results\n","# Plot training & validation accuracy values\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjTydSB72k5X","cellView":"form"},"source":["#@title tensorborad bord\n","!tensorboard --logdir /content/foldername"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QBp4SQZkVFeW","cellView":"form"},"source":["#@title get perdictions in words not in floating number\n","\n","#class probabilities\n","  predictions = classifier.predict(x_test)\n","  print(predictions)\n","\n","  [0.0128037 ]\n","  [0.01182843]\n","  [0.01042355]\n","  [0.00906552]\n","  [0.00820154]\n","  [0.00726516]......\n","\n","  logloss_score = log_loss(y_test, predictions)\n","  print(logloss_score)\n","  0.047878393431377855\n","\n","#To convert these to class labels you can take a threshold:\n","  import numpy as np\n","  #example\n","  probas = np.array([[0.4],[0.7],[0.2]])\n","  labels = (probas < 0.5).astype(np.int)\n","  print(labels)\n","  [[1]\n","  [0]\n","  [1]]\n","#For multiclass classification where you want to assign one class from multiple possibilities you can use argmax:\n","  #example\n","  probas = np.array([[0.4, 0.1, 0.5],[0.7, 0.2, 0.1],[0.3, 0.4, 0.3]])\n","  labels = np.argmax(probas, axis=-1)    \n","  print(labels)\n","  [2 0 1]\n","#to get these as one-hot encoded arrays you can use LabelBinarizer:\n","  from sklearn import preprocessing\n","\n","  lb = preprocessing.LabelBinarizer()\n","  lb.fit_transform(labels)\n","  array([[0, 0, 1],\n","        [1, 0, 0],\n","        [0, 1, 0]])\n","#for multilabel classification where you can have multiple output classes per example you can use thresholding again:\n","  #example\n","  probas = np.array([[0.6, 0.1, 0.7],[0.7, 0.2, 0.1],[0.8, 0.9, 0.6]])\n","  labels = (probas > 0.5).astype(np.int)\n","  print(labels)\n","  [[1 0 1]\n","  [1 0 0]\n","  [1 1 1]]\n","\n","# Some packages provide separate methods for getting probabilities and labels,\n","# so there is no need to do this manually, but it looks like you are using Keras\n","# which only gives you probabilities.\n","# As a sidenote, this is not called \"normalization\" for neural networks.\n","# Normalization typically describes scaling your input data to fit in a nice range like [-1,1]."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NWlZ65HHyk2j","cellView":"form"},"source":["#@title gpu cpu scopes\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","# using cpu\n","def cpu():\n","  with tf.device('/cpu:0'):\n","    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n","    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n","    return tf.math.reduce_sum(net_cpu)\n","# using gpu\n","def gpu():\n","  with tf.device('/device:GPU:0'):\n","    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n","    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n","    return tf.math.reduce_sum(net_gpu)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80XzpC-Da6bE"},"source":["#miscellaneous"]},{"cell_type":"code","metadata":{"id":"OWYwEZ4Z0I7_","cellView":"form"},"source":["#@title restart colab machine and resets variables\n","!kill -9 -1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Rlz7C0PMO16","cellView":"form"},"source":["#@title class methods ,functions operations, decorators, private, protectd, public\n","#list the possible operation an object can support\n","dir(obj)\n","# returns name of the class\n","type(self)._name__\n","#class special methods\n","@total_ordering    # for Comparision operations like __eq__,__lt__\n","class myclass:\n","  super(myclass, self).__init__()                              #call to parent class method without knowing or typing the parent class name\n","  def __init__(self, a):self.a = a                             #set attributs with values and it does not return anything\n","  #conversion to list,tuple,dict,set etc\n","  def __getitem__(self, key):return self.a[key]                     #CALL> obj[0] or obj[:]          >>[1,2,23,4,5,6.....]\n","  def __repr__(self):return 'hello'                            #CALL> myclass() or repr(obj)  >> hello  #automatic calls\n","  def __call__(self):return 'hello'                           #CALL> obj()       >> hello\n","  \n","  def __len__(self):return 10                                 #CALL> len(obj)                  >> 10\n","  def __str__(self):return 'hello'                           #CALL> str(obj) or print(obj) >> hello\n","  def __reversed__(self):return self[::-1]                    #CALL> list(reversed(obj))  >>[3,2,1]\n","  def __unicode__(self):return unicode(self.a)\n","  def __format__(self, formatstr): return format(self.a)\n","  def __hash__(self):return hash(self.a)\n","  def __nonzero__(self):return bool(self.a)\n","  def __dir__(self):return dir(self.a)\n","  def __sizeof__(self):return sys.getsizeof(self.a)\n","  def __contains__(self, item): return item in self.a         #CALL>  b in obj >>bool\n","  def __delitem__(self, key): del self.a[key]\n","  #logical operations among objects\n","    #Comparison \n","  def __eq__(self,a):return self.a == a                       #CALL> obj1 == obj2 >> bool value(true|false)\n","  def __lt__(self, a):return self.a < a                       #CALL> obj1 < obj2 >> bool value(true|false)\n","  def __le__(self, a):return self.a <= a                       #CALL> obj1 <= obj2 >> bool value(true|false)\n","  def __ne__(self, a):return self.a != a                       #CALL> obj1 != obj2 >> bool value(true|false)\n","  def __gt__(self, a):return self.a > a                       #CALL> obj1 > obj2 >> bool value(true|false)\n","  def __ge__(self, a):return self.a >= a                       #CALL> obj1 >= obj2 >> bool value(true|false)\n","  #arithematic operation among objects\n","    #Binary operations\n","  def __add__(self, a):return self.a + a                      #CALL> obj1 + obj2 >> bool value(true|false)\n","  def __sub__(self, other): return self.a - a                      #CALL> obj1 - obj2 >> bool value(true|false)\n","  def __mul__(self, other): return self.a * a                      #CALL> obj1 + obj2 >> bool value(true|false)\n","  def __truediv__(self, other): return self.a / a                      #CALL> obj1 / obj2 >> bool value(true|false)\n","  def __floordiv__(self, other): return self.a // a       \n","  def __mod__(self, other): return self.a % a             \n","  def __pow__(self, other[, modulo]): return self.a ** a  \n","  def __lshift__(self, other): return self.a << a         \n","  def __rshift__(self, other): return self.a >> a         \n","  def __and__(self, other): return self.a & a             \n","  def __xor__(self, other): return self.a ^ a             \n","  def __or__(self, other): return self.a | a    \n","    #reversed binary operations     \n","  def __radd__(self, other): return a + self.a\n","  def __rsub__(self, other): return a - self.a            \n","  def __rmul__(self, other): return a * self.a\n","  def __rtruediv__(self, other): return a / self.a\n","  def __rfloordiv__(self, other): return a // self.a       \n","  def __rmod__(self, other): return a % self.a            \n","  def __rpow__(self, other[, modulo]): return a ** self.a \n","  def __rlshift__(self, other): return a << self.a         \n","  def __rrshift__(self, other): return a >> self.a         \n","  def __rand__(self, other): return a & self.a            \n","  def __rxor__(self, other): return a ^ self.a          \n","  def __ror__(self, other): return a | self.a             \n","    #Extended|Augmented Assignment\n","  def __iadd__(self, other): return self.a += a           \n","  def __isub__(self, other): return self.a -= a           \n","  def __imul__(self, other): return self.a *= a           \n","  def __itruediv__(self, other): return self.a /= a       \n","  def __ifloordiv__(self, other): return self.a //= a     \n","  def __imod__(self, other): return self.a %= a           \n","  def __ipow__(self, other[, modulo]): return self.a **= a\n","  def __ilshift__(self, other): return self.a <<= a    \n","  def __irshift__(self, other): return self.a >>= a    \n","  def __iand__(self, other): return self.a $= a        \n","  def __ixor__(self, other): return self.a ^= a        \n","  def __ior__(self, other): return self.a |= a         \n","    #Unary Operators\n","  def __neg__(self): return -self.a                    \n","  def __pos__(self): return +self.a                    \n","  def __abs__(self): return abs(self.a)                 \n","  def __invert__(self): return ~self.a                  \n","  def __complex__(self): return complex(self.a)         \n","  def __int__(self): return int(self.a)                 \n","  def __float__(self): return float(self.a)             \n","  def __oct__(self): return oct(self.a)\n","  def __hex__(self): return hex(self.a)\n","  def __round__(self[, ndigits]): return round(self.a) \n","  def __trunc__(self): return math.trunc(self.a)\n","  def __floor__(self): return math.floor(self.a)\n","  def __ceil__(self): return math.ceil(self.a) \n","  \n","  def method(self):return 'instance method called', self                        #availabel everywhere; either inside or output of a class\n","  #class decoratores\n","  @classmethod                                       #avilabel as class.method(); cant use inside of a class\n","  def classmethod(cls):return 'class method called', cls\n","  @staticmethod                                      #availabel only inside class as its private use as self.method(); cant use outside of a class\n","  def staticmethod():return 'static method called'\n","  @property                                          #method as a variable with no paramters and argumetns\n","  def getName(self):return 'this is my class'               #CALL> obj.getName >> this is my class\n","\n","  #public\n","  def method():pass\n","  #protected\n","  def _method():pass\n","  #private\n","  def __method():pass\n","\n","#deorators\n","def twice(func):                                                                 #main name\n","  def must():                                                                   #must include a method to be returned\n","    # do anything with fun()\n","    for _ in range(2):\n","      func()\n","  return must                                                                   #must return a function\n","\n","@twice                                                                           # feed fun() to twice\n","def fun():                                                                      #noraml function\n","  print('hello')\n","fun()\n","# method special fucntions\n","def a(a=2,b=1):\n","  '''fdsfsdfdsf'''\n","a.__doc__                                                  #'''fdsfsdfdsf'''\n","a._name__                                                 #a                   #name of a()\n","a.__qualname__                                             #a                   #qualified name of a()\n","a.__module__                                               #__main__            #modul name\n","a.__defaults__                                             #(2,1)               #default parameteric values\n","a.__code__                                          \n","a.__globals__                                              #{'--global name--':'--value--',...}       #big Dict of key:value pairs of globals\n","a.__dict__                                          \n","a.__closure__\n","a.__annotations__\n","a.__kwdefaults__\n","#or empty class\n","class b:\n","  pass\n","#add attribures to empty class\n","b.new1 = None;  b.new2 = None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d5Thu-LvMpua","cellView":"form"},"source":["#@title method as switch\n","# switch method for switching b/w 2 paths\n","def Path():\n","  global count\n","  if count is 0:\n","    path = 0\n","    count=1\n","    return path\n","  if count is 1:\n","    path=1\n","    count=0\n","    return path\n","\n","\n","count=0\n","for i in range(10):\n","  print(Path())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Eq7-W1ZbBH-W","cellView":"form"},"source":["#@title get unique elements with counts, reverse pairs\n","a = np.unique(labels, return_counts=True)\n","#or \n","b = set(labels)\n","# get unique elements and their total counts in a list\n","db, count = np.unique(y, return_counts=True)\n","#or faster\n","db, count = np.bincount(choices, minlength=np.size(choices))\n","#make dic of db, count pairs\n","c = dict(zip(db, count))\n","#or dic\n","c = collections.Counter(y)\n","print(c)\n","#or\n","c =  np.count(y,db)\n","#or\n","c =  list(y).count(db)\n","\n","# get ele:count pairs of a list\n","Dic = dict( (l, List.count(l) ) for l in set(List))\n","\n","# get [count,el] pairs of a list\n","count_el = [ [List.count(el), el] for el in set(List)]\n","# reverse the [count,el] to [el, count], works with any k,v \n","el_count = [[el,count] for count, el in count_el]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UwRZ3ih-2gh3","cellView":"form"},"source":["#@title calculate the size of storage  n_images takes\n","print(\n","f'''\n","{300*60*1280*704*3 /1024} KB\n","{300*60*1280*704*3 /1024/1024} MB\n","{300*60*1280*704*3 /1024/1024/1024} GB\n","'''\n",")\n","# storage size of image in kb mb gb\n","\n","image = 1280*704*3\n","fps = 30\n","sec = 1   # 1 - 60\n","min = 10  #  \n","\n","time = 60*10    # sec x min x hrs\n","kb = mb = gb = 1024\n","image * fps * 60*10 / kb/mb/gb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyoBy-YRGtae","cellView":"form"},"source":["#@title argument parser cli cmd\n","import argparse\n","parser = argparse.ArgumentParser()\n","#set values\n","parser.add_argument('--var_name ='default var value', choices=['choice a','choice b',],type=str)\n","#global args\n","args = parser.parse_args()\n","#use values\n","print(args.var_name)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1XgBqT1BJ9p","cellView":"form"},"source":["#@title eel python <-> html,javascript\n","!pip install Eel\n","\n","# python.py\n","import eel \n","eel.init('/content/drive/My Drive/web')\n","\n","\n","# from javascript\n","eel.my_javascript_function(1, 2, 3, 4)  # This calls the Javascript function\n","# to javascript\n","@eel.expose\n","def my_python_function(a, b):\n","    print(a, b, a + b)\n","\n","# synchronous process\n","# do something with returned value\n","# (It works exactly the same the other way around).\n","def print_num(n): print('Got this from Javascript:', n)\n","# Call Javascript function, and pass explicit callback function\n","eel.js_random()(print_num)\n","#  to hold the returned value from javascript fun()\n","n = eel.js_random()()  # This immediately returns the value\n","\n","# asynchronous process\n","def my_other_thread():\n","    while True:\n","        print(\"I'm a thread\")\n","        eel.sleep(1.0)                  # Use eel.sleep(), not time.sleep()\n","\n","eel.spawn(my_other_thread)\n","\n","eel.start('main.html', block=False)     # Don't block on this call\n","\n","while True:\n","    print(\"I'm a main loop\")\n","    eel.sleep(1.0)                      # Use eel.sleep(), not time.sleep()\n","\n","# eel.start('main.html', mode='chrome-app', port=8080, cmdline_args=['--start-fullscreen', '--browser-startup-dialog'])\n","eel.start('main.html')\n","\n","\n","# script.js\n","%%javascript\n","// # from python\n","console.log(\"Calling Python...\");\n","eel.my_python_function(1, 2); // This calls the Python function that was decorated\n","\n","\n","// fun() definition\n","function my_javascript_function(a, b, c, d) {\n","  if (a < b) {\n","    console.log(c * d);\n","  }\n","}\n","\n","// to python\n","eel.expose(my_javascript_function);\n","//  or fun() as other name\n","eel.expose(someFunction, \"my_javascript_function\");\n","\n","// synchronous process\n","// to hold the returned value from python fun()\n","async function run() {\n","  // Inside a function marked 'async' we can use the 'await' keyword.\n","  let n = await eel.py_random()(); // Must prefix call with 'await', otherwise it's the same syntax\n","  console.log(\"Got this from Python: \" + n);\n","}\n","\n","run();\n","\n","\n","// main.html\n","%%html\n","<!-- eel.js must be included in the html code -->\n","<script type=\"text/javascript\" src=\"/eel.js\"></script>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9tvgr657zi3","cellView":"form"},"source":["#@title lambda function is very small fun() short simple tiny  \n","x = lambda a, b, c : a + b + c\n","print(x(5, 6, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULfprJDXVrgK","cellView":"form"},"source":["#@title read and write list,dict, tuple etc. into a file.txt \n","with open('set10', 'r') as f:\n","    set10 = ast.literal_eval(f)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXAeQ4OThg3j","cellView":"form"},"source":["#@title emojis, char int ascii unicode utf8\n","\n","#reverse flip exchange dict\n","uchar = {v: k for k, v in uchar.items()}\n","\n","uchar = {176: '░', 177: '▒', 178: '▓', 186: '║', 196: '─', 197: '┼', 219: '█', 220: '▄', 221: '▌', 222: '▐', 223: '▀', 224: 'α', 225: 'ß', 226: 'Γ', 227: 'π', 228: 'Σ', 229: 'σ', 230: 'µ', 231: 'τ', 232: 'Φ', 233: 'Θ', 234: 'Ω', 235: 'δ', 236: '∞', 237: 'φ', 238: 'ε', 239: '∩', 240: '≡', 241: '±', 242: '≤', 243: '⌠', 244: '⌡', 245: '÷', 246: '≈', 247: '°', 248: '∙', 251: '√', 252: 'ⁿ', 253: '²', 254: '■'}\n","\n","!pip install emoji\n","import emoji\n","print(emoji.emojize('Python is :thumbs_up:'))\n","#Python is 👍\n","print(emoji.emojize('Python is :thumbsup:', use_aliases=True))\n","#Python is 👍\n","print(emoji.demojize('Python is 👍'))\n","#Python is :thumbs_up:\n","\n","#or \n","\n","print(ord('A') ,#65\n","      chr(65))#A\n","#From the list of unicodes, replace “+” with “000”. For example – “U+1F600” will become “U0001F600” and prefix the unicode with “\\” and print it.\n","# grinning face \n","print(\"\\U0001f600\")        #                \n","print (u'\\U0001f600')      #😀\n","print(\"\\N{grinning face}\") #😀\n","\n","# or \n","\n","\n","%%html\n","<meta charset=\"UTF-8\">\n","<body>\n","\n","<h1>Sized Emojis</h1>\n","\n","<p style=\"font-size:48px\">\n","&#128512; &#128516; &#128525; &#128151;\n","</p>"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B4_Yi9iRBX4u","cellView":"form"},"source":["#@title file to spectrogram to image to save\n","\n","cmap = plt.get_cmap('inferno')\n","\n","plt.figure(figsize=(10,10))\n","# feature columns' names\n","genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n","for g in genres:\n","    i = 0# progress status count\n","    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n","    for filename in os.listdir(f'/content/Music/genres/{g}'):\n","        songname = f'/content/Music/genres/{g}/{filename}'\n","        y, sr = librosa.load(songname, mono=True, duration=5)\n","        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n","        plt.axis('off');\n","        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n","        plt.clf()\n","        #progress status\n","        i += 1;  output.clear()\n","        print(i,'/',len(os.listdir(f'/content/Music/genres/{g}')),'/',len(genres))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"STHPYrOwwyfM","cellView":"form"},"source":["#@title generate dummy multi-label,dataset,featrues\n","from sklearn import datasets\n","a = datasets.make_multilabel_classification(n_samples=2000, n_features=20, n_classes=10, n_labels=3, length=50, allow_unlabeled=False, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\n","Multi_label = a[1]\n","\n","# Images\n","images = np.empty((0,20,20,1))\n","for i in range(2000):\n","  images = np.append(images,[np.random.rand(20,20,1) * 255], axis=0)\n","images.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6ErRxDTMlhw","cellView":"form"},"source":["#@title download and unzip the zipped or .tar file\n","import zipfile\n","                             #path of the zip file \n","zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/img_align_celeba.zip\", 'r')\n","zip_ref.extractall(\"/tmp\")  #optional path to extract to that path\n","zip_ref.close()\n","\n","# tar.gz\n","import tarfile\n","with tarfile.open('/content/genres.tar.gz', 'r:gz') as tar:\n","    tar.extractall('/content/Music/')\n","#or\n","!wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n","!mkdir /content/a\n","!tar -xvzf /content/Question_Answer_Dataset_v1.2.tar.gz -C /content/QA\n","#or\n","from keras.utils.data_utils import get_file\n","path = get_file('/content/sample_data/file.mp4', #storage path and filename\n","                origin='http://mirrors.standaloneinstaller.com/video-sample/jellyfish-25-mbps-hd-hevc.3gp')  #url of file\n","#or \n","import requests\n","url = \"https://pytorch.org/tutorials//_static/img/steam-train-whistle-daniel_simon-converted-from-mp3.wav\"\n","r = requests.get(url)\n","with open('steam-train-whistle-daniel_simon-converted-from-mp3.wav', 'wb') as f:\n","    f.write(r.content)\n","#or \n","!apt install unzip\n","!unzip  source.zip -d /folder\n","#download to local machine using browser download system from cloud\n","from google.colab import files\n","files.download('./googleDrive/a.txt')\n","# create direct link so file could be downloaded by just clicking the link rather than visiting the website\n","# past link of drive file \n","https://sites.google.com/site/gdocs2direct/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHmatvivBMlm","cellView":"form"},"source":["#@title print(string_formats)\n","a,b,c = 1,2,3\n","print('%d is %d and %d'%(a,b,b))\n","print('{0} is {1} and {2}'.format(a,b,b))\n","print(f'{a} is {b} and {c}')\n","print(r'this is \\n raw \\n text')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPAE-1KtR_jm","cellView":"form"},"source":["#@title read and write dataset to file.csv\n","import pandas as pd\n","##________________________________write dataset to file.csv\n","#[image1.png, appel, red]\n","vision = {'filepath': ['/path1','/path2','/path3','/path4'],\n","        'object': ['apple','mango','apple','car'],\n","        'mean': ['']\n","        }\n","\n","df = pd.DataFrame(vision, columns= ['filepath', 'object'])\n","\n","df.to_csv (r'file.csv', index = False, header=True) #Don't forget to add '.csv' at the end of the path\n","\n","print (df)\n","#________________________________read dataset from file.csv\n","f = pd.read_csv(\n","    r'file.csv',      # absolute python path to subdirectory\n","    #sep='\\t'           # Tab-separated value file.\n","    #quotechar=\"'\",        # single quote allowed as quote character\n","    #dtype={\"salary\": int},             # Parse the salary column as an integer \n","    #usecols=['filepath'],   # Only load the three columns specified.\n","    #parse_dates=['birth_date'],     # Intepret the birth_date column as a date\n","    #skiprows=10,         # Skip the first 10 rows of the file\n","    #na_values=['.', '??']       # Take any '.' or '??' values as NA\n","           )\n","\n","#print(f.head(2))                                                               # to get 2 rows of dataset\n","print(f['filepath'][0])                                                         # to get value of file[columnName[index]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dtP766ajfyrP","cellView":"form"},"source":["#@title pbar progressbar progress bar tqdm output.clear(output_tags='hello')\n","from tqdm import tqdm\n","# default\n","for _ in tqdm('i am iterable'):\n","\n","# custom\n","with tqdm(total=200, desc=\"Adding Users\", bar_format=\"{l_bar}{bar}|  [ time left: {remaining} ]\") as pbar:\n","    for i in range(100):\n","        pbar.update(2)  #multiple of a unit update\n","\n","#custom 2\n","from google.colab import output\n","for i in range(1000):\n","  if i%10==0:\n","    print(i)\n","    output.clear()\n","\n","#or \n","pBar(ID='1')    #to clear only this output\n","output.clear(output_tags='1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9R_XVqyEMmHh","cellView":"form"},"source":["#@title CLI command cmd\n","# https://linuxize.com/post/bash-check-if-file-exists/#:~:text=a%20default%20shell.-,Check%20if%20File%20Exists,a%20directory%20or%20a%20device).\n","# https://ubuntu.com/tutorials/command-line-for-beginners#3-opening-a-terminal\n","# https://www.tecmint.com/linux-commands-cheat-sheet/\n","# https://linuxconfig.org/linux-commands\n","# https://www.hostinger.in/tutorials/linux-commands\n","# https://www.geeksforgeeks.org/linux-commands/\n","# https://www.howtogeek.com/412055/37-important-linux-commands-you-should-know/\n","# https://maker.pro/linux/tutorial/basic-linux-commands-for-beginners\n","#         shell scripting\n","# https://www.tutorialspoint.com/unix/shell_scripting.htm\n","# https://www.shellscript.sh/\n","# https://tldp.org/LDP/abs/html/\n","\n","# list the files in directory\n","!ls /tmp/img_align_celeba \n","# list files in directory if direcory name includes space\n","!ls /content/gdrive/My\\ Drive/*.py\n","# read file data\n","!cat '/content/gdrive/My Drive/mylib.py'\n","!tail logs.txt\n","# move|copy file to another destination( src -> dest)\n","!mv /tmp/img_align_celeba /content/sample_data/celeba\n","!cp /src/  -d /dest/\n","#download url\n","!wget fileURL -O /optional_fpath_fname    #file will be saved at given path with given filename\n","!wget fileURL -P /optional_fpath          #file will be saved at given path with default filename\n","#copy hello to clipboard\n","!echo hello|clip\n","# pip install dependencies from a text file\n","!pip install -r requirements.txt\n","# write outputs or logs to a file \n","!python3 main.py > logs.txt 2>&1 &'\n","# get response of a url or website\n","!curl http://localhost:8000\n","# execute python code\n","!python3 -c \"print('python code is executed here.')\"\n","# make directory\n","!mkdir -p /path/\n","# list of commands and shortcuts\n","!ls /bin\n","#list directories\n","%ldir\n","#command history\n","%history\n","#list of all % related commands\n","%lsmagic    #https://ipython.readthedocs.io/en/stable/interactive/magics.html\n","#ram information\n","!cat /proc/meminfo\n","#use variable in cmd  and store output of cmd to a variable\n","message = 'A Great Tutorial on Colab by Tutorialspoint!'\n","greeting = !echo -e '$message\\n$message'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnqkR_jdlsNS"},"source":["**bold**\n","*italic*\n","~strikethrough~\n","$\\sqrt{3x-1}+(1+x)^2$\n"]},{"cell_type":"code","metadata":{"id":"xXqghznR_jt3","cellView":"form"},"source":["#@title add custom modul into colab, reload modul, create package\n","import sys\n","sys.path.append('/content/gdrive/My Drive')\n","# Now we can import the library and use the function.\n","import mylib as my\n","\n","import importlib\n","# reload and refresh  updates modul\n","importlib.reload(my)\n","\n","#package              each dir must have __init__.py file\n","from dir1.dir2.dirN import dir\n","from dir1.dir2.dirN import mylib\n","from dir1.dir2.dirN.mylib import method\n","\n","#modul path\n","my.__file__\n","#atrributes, methods with their definitions and vlues\n","my.__dict__\n","# main function call\n","if __name__ == '__main__': \n","  '''default calls during modul imports"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WI-qWZaMeB7W","cellView":"form"},"source":["#@title torch tensor\n","torch.from_numpy(y_percussive)\n","torch.eye(2)\n","torch.zeros(2,2)\n","torch.ones(2,2)\n","torch.rand(2,2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4II8GKbIMomr","cellView":"form"},"source":["#@title  matplotlib.pyplot as plt methods\n","# Note: image(h,w,1) should be (h,w), channel 1 is not supported\n","plt.imshow(img)\n","plt.plot(beat_chroma)\n","plt.show()\n","plt.gray()\n","plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n","\n","# plot multi many graphs graph\n","plt.figure(figsize=(10, 8))# (width x hight)\n","\n","plt.subplot(3, 1, 1)  #1\n","librosa.display.specshow(chroma, y_axis='chroma')\n","plt.colorbar()\n","plt.title('Unfiltered')\n","\n","plt.subplot(3, 1, 2)  #2\n","librosa.display.specshow(chroma_med, y_axis='chroma')\n","plt.colorbar()\n","plt.title('Median-filtered')\n","\n","plt.subplot(3, 1, 3)  #3\n","librosa.display.specshow(chroma_nlm, y_axis='chroma')\n","plt.colorbar()\n","plt.title('Non-local means')\n","\n","plt.tight_layout()\n","plt.show()\n","# or\n","\n","plt.figure(figsize=(10,50))\n","for i in range(5):\n","  plt.subplot(5,5,i+1)\n","  plt.imshow(input[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CBdNgHR9nnB6","cellView":"form"},"source":["#@title get or set values of layer or moduls\n","keras.backend.get_value(Fashion.optimizer.lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlDnmPM7zS9z","cellView":"form"},"source":["#@title html javascript json in colab cell\n","import IPython\n","from google.colab import output\n","\n","# works in any code-cell except the cell including %%   statement\n","display(IPython.display.HTML('''  --<html> code gere'''))\n","display(IPython.display.JSON('''  --jason code gere'''))\n","display(IPython.display.Javascript('''  --Javascript code gere'''))\n","\n","# works in their own code-cell\n","%%javascript\n","--code--\n","%%html\n","--code--\n","%%JSON\n","--code--\n","%%writefile /filename.extension\n","--file content to be written--\n","%mkdir directoryname -p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuwGMAraXz21","cellView":"form"},"source":["#@title time complexity\n","\n","start = datetime.timestamp(datetime.now())\n","fun()\n","end = datetime.timestamp(datetime.now())\n","result = end - start\n","#or\n","%timeit fun()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gt4sjnwo-qk-","cellView":"form"},"source":["#@title stop or remove warnings in colab\n","import warnings\n","warnings.filterwarnings('ignore')\n","warnings.simplefilter('ignore')\n","warnings.filterwarnings(action='once')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZXx4Kbih4gY","cellView":"form"},"source":["#@title glob.glob operatons\n","# get files only starts with 'startswith' and ends with '.txt'\n","glob.glob('./startswith?.txt')\n","# get files only have digit in range 0 to 9\n","glob.glob('./*[0-9].*')\n","#find and retriev files even inside the subdirectories\n","glob.glob('root1/root2/**/*.txt',recursive = True)     #gets all txt files inside root2 and its subdirs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-bGeEJSZFdo","cellView":"form"},"source":["#@title parallel processing sub threading\n","\n","from multiprocessing import Pool\n","a = 0\n","def b(a):\n","  print('b0',a)\n","  a = 1\n","  print('b1',a)\n","  return a\n","def c(a):\n","  print('c0',a)\n","  a = 2\n","  print('c1',a)\n","  return a\n","\n","\n","pool = Pool()\n","result1 = pool.apply_async(b, [a])    # evaluate \"solve1(A)\" asynchronously\n","result2 = pool.apply_async(c, [a])    # evaluate \"solve2(B)\" asynchronously\n","answer1 = result1.get(timeout=10)\n","answer2 = result2.get(timeout=10)\n","print('a0',a)\n","print(answer1,'\\n',answer2)\n","print('a1',a)\n","#visit here for detailed version https://www.thepythoncode.com/article/using-threads-in-python"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FhvG3_XA9LXe","cellView":"form"},"source":["#@title datatypes int float long complex string character hex oct\n","#Python supports four different numerical types −\n","\n","int #signed integers\n","long #Long integers, they can also be represented in octal and hexadecimal\n","float #floating point real values\n","complex #complex numbers\n","\n","int\t      long\t                  float        complex\n","10        51924361L\t              0.0          3.14j\n","100       -0x19323L\t              15.20        45.j\n","-786      0122L\t                  -21.9        9.322e-36j\n","080       0xDEFABCECBDAECBFBAEl\t  32.3+e18     .876j\n","-0490     535633629843L           -90.         -.6545+0J\n","-0x260    -052318172735L          32.54e100    3e+26J\n","0x69      -4721885298529L\t        70.2-E12     4.53e-7j\n","\n","a = 0b1010 #Binary Literals; from binary\n","b = 100 #Decimal Literal ; from decimal or int\n","c = 0o310 #Octal Literal;   from octal\n","d = 0x12c #Hexadecimal Literal; from hex\n","\n","#Float Literal\n","float_1 = 10.5 \n","float_2 = 1.5e2\n","\n","#Complex Literal \n","x = 3.14j\n","\n","print(a, b, c, d)\n","print(float_1, float_2)\n","print(x, x.imag, x.real)\n","\n","strings = \"This is Python\"\n","char = \"C\"\n","multiline_str = \"\"\"This is a multiline string with more than one line code.\"\"\"\n","unicode = u\"\\u00dcnic\\u00f6de\"\n","raw_str = r\"raw \\n string\"\n","\n","print(strings)\n","print(char)\n","print(multiline_str)\n","print(unicode)\n","print(raw_str)\n","\n","\n","int8\tByte (-128 to 127)\n","int16\tInteger (-32768 to 32767)\n","int32\tInteger (-2147483648 to 2147483647)\n","int64\tInteger (-9223372036854775808 to 9223372036854775807)\n","uint8\tUnsigned integer (0 to 255)\n","uint16\tUnsigned integer (0 to 65535)\n","uint32\tUnsigned integer (0 to 4294967295)\n","uint64\tUnsigned integer (0 to 18446744073709551615)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBfVumiBNh-R","cellView":"form"},"source":["#@title calc precentage %\n","def pcent(current,total):\n","  p = current / (total/100)\n","  print(p,'%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-RPluUNPjo_n","cellView":"form"},"source":["#@title mega\n","#!pip install mega.py\n","from mega import Mega\n","mega = Mega()\n","m = mega.login('jaihosd51@gmail.com','MegAC00LP00L!00')\n","details = m.get_user()#user basic details\n","file = m.get_node_by_type(4)# 0,1,2,3,4 = dir,file, cloud drive,indbox, trashbin\n","quota = m.get_quota()\n","space = m.get_storage_space()\n","space = {'total':ut.getSize(space['total']),'used':ut.getSize(space['used'])}\n","files = m.get_files()\n","files = m.get_files_in_node('id')\n","file = m.find(fname,fileId)\n","root_id = m.root_id\n","#Upload a file to a destination folder\n","file = m.upload('/content/drive/My Drive/label.txt', m.find('apps',exclude_deleted=True)[0],dest_filename=None)\n","#link of uploaded file\n","url_link = m.get_upload_link(file)\n","#get link for file or folder in mega drive\n","url_link = m.export('label.txt')#(node_id='id of folder or file')\n","#Download a file from URL or file obj, optionally specify destination folder\n","  #file obj\n","m.download(m.find('label.txt') , '/content/')#,'optional_fname.txt')\n","  #url\n","m.download_url(url_link ,'/content/','optional_fname.txt')\n","#Create a folder\n","status = m.create_folder('new_folder/sub_folder/subsub_folder')\n","#rename\n","status = m.rename(m.find('label.txt'), 'my_file.doc')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9NvF7I0-dZcq","cellView":"form"},"source":["#@title exception error\n","x = -2\n","#custom error\n","class MyError(Exception):\n","  print('error bro')\n","# test block in which the error can occur\n","try:\n","  '''test code here'''\n","\n","# raise an error if no error occuring in test block\n","if x < 0:                           #condition when to error should raise\n","  raise --erro_name--('x is less than zero')        # 'Exception' is error type from the list of errors below\n","\n","# raise error if condition not setisfied\n","assert ( x == -2),'''error happened bro'''\n","\n","#for any exception\n","except:'''do this if error occured'''   #do anything when any exception occurs\n","except:     # get any error and its 'name' 'value' 'line number', none of them is string type, for that use str(name) str(string) str(line_num)\n","  name, string, line_num =  sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2].tb_lineno\n","#for any exception\n","except Exception as err: print(err) # err: gives comment of the error and is not a string, for that use str(err)\n","#for specific named exception\n","except --error_name-- as err:\n","  print(err) # err: gives comment of the error and is not a string, for that use str(err)\n","  print(type(err),err.args)# error type and error string\n","  print('--CustomErrorName--: ',err)# replace error type with 'my type error'os.sys\n","except --error_name--: raise # re-raise this specific exception because i want it to riase, i dont want it to handle\n","#only specified multiple exceptions\n","except (err1,err2,..):\n","\n","# if no exception takes place\n","else:'''do this if no error occured'''\n","# no matter what, just run this block, it has nothing to do with exception, it executes anyways\n","finally:''' do this, no matter what.'''\n","\n","Exception                                                                     \tCause of Error\n","\n","Exception                                                     raiseed only when user speficies, it is the baseclass\n","AssertionError                                             \tRaised when assert statement fails.\n","AttributeError\t                                             Raised when attribute assignment or reference fails.\n","EOFError\t                                             Raised when the input() functions hits end-of-file condition.\n","FloatingPointError\t                                             Raised when a floating point operation fails.\n","GeneratorExit                                             \tRaise when a generator's close() method is called.\n","ImportError                                             \tRaised when the imported module is not found.\n","IndexError                                             \tRaised when index of a sequence is out of range.\n","KeyError                                             \tRaised when a key is not found in a dictionary.\n","KeyboardInterrupt                                             \tRaised when the user hits interrupt key (Ctrl+c or delete).\n","MemoryError                                             \tRaised when an operation runs out of memory.\n","NameError                                             \tRaised when a variable is not found in local or global scope.\n","NotImplementedError                                             \tRaised by abstract methods.\n","OSError                                             \tRaised when system operation causes system related error.\n","OverflowError                                             \tRaised when result of an arithmetic operation is too large to be represented.\n","ReferenceError                                             \tRaised when a weak reference proxy is used to access a garbage collected referent.\n","RuntimeError                                             \tRaised when an error does not fall under any other category.\n","StopIteration                                             \tRaised by next() function to indicate that there is no further item to be returned by iterator.\n","SyntaxError\t                                             Raised by parser when syntax error is encountered.\n","IndentationError                                             \tRaised when there is incorrect indentation.\n","TabError                                             \tRaised when indentation consists of inconsistent tabs and spaces.\n","SystemError                                             \tRaised when interpreter detects internal error.\n","SystemExit                                             \tRaised by sys.exit() function.\n","TypeError                                             \tRaised when a function or operation is applied to an object of incorrect type.\n","UnboundLocalError                Raised when a reference is made to a local variable in a function or method, but no value has been bound to that variable.\n","UnicodeError                                             \tRaised when a Unicode-related encoding or decoding error occurs.\n","UnicodeEncodeError                                             \tRaised when a Unicode-related error occurs during encoding.\n","UnicodeDecodeError                                             \tRaised when a Unicode-related error occurs during decoding.\n","UnicodeTranslateError\t                                             Raised when a Unicode-related error occurs during translating.\n","ValueError                                             \tRaised when a function gets argument of correct type but improper value.\n","ZeroDivisionError\t                                             Raised when second operand of division or modulo operation is zero."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OqZ1uHW4yz9M","cellView":"form"},"source":["#@title locals() globals() scopes\n","# check if Desc exist in local scope or not\n","if 'Desc' in locals():\n","  print(True)\n","# check if Desc exist in global scope or not\n","if 'Desc' in globals():\n","  print(True)\n","\n","# get list of all variables\n","list(locals())\n","list(globals())\n","dir()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SsWLLKJ4TeB3","cellView":"form"},"source":["#  @title base64\n","# https://base64.guru\n","import base64\n","# Encoding -> simple string to base64 string\n","message        = \"Python is fun\"                         #string\n","message_bytes  = message.encode('ascii')                 #bytes b'' using any encoding but must use same encoding for both ends\n","base64_bytes   = base64.b64encode(message_bytes)         #base64 bytes b''\n","# or message_bytes  = base64.encodebytes(base64_bytes)        #bytes b''   both are same\n","base64_message = base64_bytes.decode('ascii')            #base64 string\n","# Decoding -> base64 string to simple string\n","base64_message = 'UHl0aG9uIGlzIGZ1bg=='                  #base64 string\n","base64_bytes   = base64_message.encode('ascii')          #base64 bytes b''\n","message_bytes  = base64.b64decode(base64_bytes)          #bytes b''\n","# or message_bytes  = base64.decodebytes(base64_bytes)        #bytes b''   both are same\n","message        = message_bytes.decode('ascii')           #string\n","\n","The Base64 character set contains:\n","  26 uppercase letters\n","  26 lowercase letters\n","  10 numbers\n","  02 + and / for new lines (some implementations may use different characters)\n","  26+26+10+2 = 64 character to encode any string\n","#8bit to 6bit\n","01010000 01111001 01110100 01101000 01101111 01101110   #6bytes x 8bits\n","010100 000111 100101 110100 011010 000110 111101 101110 #8bytes x 6bits\n","\n","# any data file to base64 data \n","import base64\n","with open(\"/content/sample.PNG\", \"rb\") as img_file:\n","    b64Data = base64.b64encode(img_file.read())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nbyWeA3SWVI7"},"source":["#Bussiness"]},{"cell_type":"code","metadata":{"id":"JNKAtLPYWp5G","cellView":"form"},"source":["#@title 1. Market Research and Analysis, find customers and Refine your idea.\n","main link (https://www.thebalancesmb.com/starting-a-small-business-4161641)\n","# refine your idea:\n","  # 1. Understand the Commitment and Challenges Involved in Starting a Business\n","    # Coming up with a great and unique product or service\n","    # Having a strong plan and vision for the business\n","    # Having sufficient capital and cash flow\n","    # Finding great employees\n","    # Firing bad employees quickly in a way that doesn’t result in legal liability\n","    # Working more than you expected\n","    # Not getting discouraged by rejections from customers\n","    # Managing your time efficiently\n","    # Maintaining a reasonable work/life balance\n","    # Knowing when to pivot your strategy\n","    # Maintaining the stamina to keep going even when it’s tough\n","  # 2. Protect Your Personal Assets by Forming the Business as a Corporation or LLC\n","\n","\n","# Research and Analysis\n","  # KYC (Know Your Consumer)\n","    # what is consumer's behavior?\n","    # Whate are Reactions to your logo?\n","    # What are the improvements you could make to a buying experience?\n","    # where customers might go instead of your business?\n","    # Tools: Surveys,Questionnaires, Focus groups, In-depth interviews\n","  # Statistics:\n","    # on industries, business conditions\n","    # Gain info on potential customers, consumer markets\n","    # Demographics(statistical data relating to the population and particular groups within it.)\n","    # Know unemployment rates, loans granted and more\n","    # what are interest rates?\n","    # What is the demand, costs and consumer spending?\n","    # What are the Statistics of specific industries?\n","  # competitive analysis:\n","    # Your Market share\n","    # Your Strengths and weaknesses\n","    # Your window of opportunity to enter the market\n","    # The importance of your target market to your competitors\n","    # Any barriers that may hinder you as you enter the market\n","    # Indirect or secondary competitors who may impact your success\n","    # who is your competition and how their products, services and marketing strategies affect you ?\n","    # Tools:\n","      # Porter's Five Forces:\n","        # 1. Competitive rivalry:\n","          # how intense the competition is in the marketplace?\n","            # What are the number of existing competitors and what each one can do?\n","        # 2. The bargaining power of suppliers:\n","          # how much power and control a business's supplier has over the potential to raise its prices, which, in turn, lowers a business's profitability.\n","          # What are the number of suppliers of raw materials and other resources that are available?\n","            # \"fewer the supplier, more power they have.\" Businesses are in a better position when there are multiple suppliers.\n","        # 3. The bargaining power of customers:\n","          # What is the power of the consumer, and their effect on pricing and quality?\n","            # Consumers have power when they are fewer in number and there are plentiful sellers and it's easy for consumers to switch to another seller.\n","            # The buying power is low when consumers purchase products in small amounts and the seller's product is very different from its competitors.\n","        # 4. The threat of new entrants:\n","          # how easy or difficult it is for competitors to join the marketplace?\n","            # The easier it is for a new competitor to gain entry, the greater the risk is of an established business's market share being depleted.\n","            # Barriers to entry include absolute cost advantages, access to inputs, economies of scale and strong brand identity.\n","        # 5. The threat of substitute products or services:\n","          # how easy it is for consumers to switch from a business's product or service to that of a competitor's product or service?\n","            # it examins the number of competitors and how their prices and quality compare to the business are being examined?\n","            # how much profit those competitors are earning, which would determine if they can lower their costs even more.\n","            # The threat of substitutes is informed by switching costs, both immediate and long-term, as well as consumers' inclination to change.\n","    # strategy implementaion:\n","      # Cost leadership:\n","        # Your goal is to increase profits by reducing costs while charging industry-standard prices, or to increase market share by reducing the sales price while retaining profits.\n","      # Differentiation:\n","        # your company's products need to be significantly better than the competition's, improving their competitiveness and value to the public. It requires thorough research and development, plus effective sales and marketing.\n","      # Focus:\n","        # Successful implementation entails the company selecting niche markets in which to sell their goods. It requires an intense understanding of the marketplace, its sellers, buyers and competitors.\n","  \n","  # what are the opportunities and limitations for gaining customers?\n","    # This could include population data on age, wealth, family, interests, or anything else that’s relevant for your business.\n","  # What are household incomes?\n","  # What is the specific market share that will impact your profits?\n","  \n","  # Trends:\n","    # What are latest small business trends?\n","    # what are economic trends?\n","    # What are the employment trends ?\n","    # What are income trends?\n","      # Pay your employees fair rates based on earnings data.\n","\n","    \n","\n","  # Demand:\n","    # Is there a desire for your product or service?\n","  # Market size:\n","    # How many people would be interested in your offering?\n","  # Economic indicators:\n","    # What is the income range and employment rate?\n","  # Location:\n","    # Where do your customers live and where can your business reach?\n","  # Market saturation:\n","    # How many similar options are already available to consumers?\n","  # Pricing:\n","    # What do potential customers pay for these alternatives?\n","\n","# specify your market.\n"," \t# Learn what current brand leaders are doing and figure out how you can do it better. If you think your business can deliver something other companies don't (or deliver the same thing, only faster and cheaper)\n","\n","# understand the reasoning behind your idea.\n","\t# (https://www.thebalancesmb.com/what-is-a-focus-group-2951756)\n","\t# conduct focus group:\"During the session, try not to focus on your business idea. Startup business owners often use a focus group to justify their own ideas, rather than actually listening to participants’ input. Instead of talking about your product or service’s features, ask what benefits participants want from similar products or services. Try to elicit participants' needs, wants, pain points, budget, and purchasing process.\n","\t# Does your idea have the potential to succeed?(https://www.thebalancesmb.com/how-to-qualify-a-business-idea-2951452)\n","\t# does your idea solve a problem, fulfill a need or offer something the market wants?\n","\t# Is there a need for your anticipated products/services?\n","\t# Who needs it?\n","\t# Are there other companies offering similar products/services now?\n","\t# What is the competition like?\n","\t# How will your business fit into the market?\n","\t# why you are launching your business?\n","\t# When your why is focused on meeting a need in the marketplace, the scope of your business will always be larger than a business that is designed to serve a personal need.\n","\t# idea of where you want to go?\n","\t# how you plan to get there?\n","\t\n","\n","# clarify your mission.\n","\t# who their customers will be, or why these people should want to buy from or hire them?\n","\t# why you want to work with these customers?\n","\t# how you will provide this value to your customers and how to communicate that value in a way that they are willing to pay?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16BWK89BWa4Y","cellView":"form"},"source":["#@title 2. Write a business plan.(a blueprint)\n","\"He who fails to plan is planning to fail.\"\n","# a business plan is a document that outlines the basics about your business, products, and services; the market you are targeting; the goals you have for your business; and how you will achieve those goals.\n","\n","# the business plan\n","  #traditional:\n","    # Executive summary:\n","      # What your company is and why it will be successful?\n","      # mission statement\n","      # what are your product or service?\n","      # company’s leadership team?\n","      # employees, and location\n","      # financial information and high-level growth plans\n","    # Company description:\n","      # what problems your business solves?\n","      # list out the consumers, organization, or businesses your company plans to serve\n","      # what are the competitive advantages that will make your business a success?\n","      # Are there experts on your team?\n","      # Have you found the perfect location for your store?\n","      # what are your strenghts\n","    # Market analysis:\n","      # what other businesses are doing and what their strengths are?\n","      # What are the trends and themes?\n","      # What do successful competitors do?\n","      # Why does it work?\n","      # Can you do it better?\n","    # Organization and management:\n","      # how your company will be structured?\n","      # who will run it?\n","      # what is the organizational chart to lay out who's in charge of what in your company?\n","      # how each person's unique experience will contribute to the success of your venture.(Consider including resumes and CVs of key members of your team.)\n","    # Service or product line:\n","      # what you sell or what service you offer?\n","      # how it benefits your customers and what the product lifecycle looks like?\n","      # Do you have copyright or patent filings?\n","    # Marketing and sales:\n","      # how your marketing stratagy  evolve and change to fit your unique needs?\n","      # how you'll attract and retain customers?\n","      # how a sale will actually happen?\n","    # Funding request:\n","      # how much funding you’ll need over the next five years?\n","      # how you'll use your funds?\n","      # why do you need funds? (to buy equipment or materials, pay salaries, or cover specific bills until revenue increases.)\n","      # how would you paying off debt or selling your business in future?\n","    # Financial projections:\n","      # How your business is stable and will be a financial success?\n","      # what are your financial outlook for the next five years about forecasted income statements, balance sheets, cash flow statements, and capital expenditure budgets?\n","      # where are the graphs and charts to tell the financial story of your business?\n","    # Appendix:\n","      # where are your supporting documents or other materials were specially requested like credit histories, resumes, product pictures, letters of reference, licenses, permits, or patents, legal documents, permits, and other contracts?\n","   \n","\n","\n","\n","\n","\n","\n","\n","\n","  # Vision:\n","    # What are you creating?\n","    # What will your business look like in one year, three years, and five years?\n","\t# Mission Statement:\n","    # What is the purpose of your business?\n","    # Why are you starting this business?\n","\t# Objectives:\n","    # What are your business goals?\n","    # How will you measure success in achieving your goals?\n","\t\t# Goals:\thelps from where you are right now to where you want to be.\n","\t\t\t# what you want to accomplish in the short term (one-12 months), as well as over the long term (12 months or more)?\n","\t\t\t# break each goal into small parts to achieve the goal.\n","\t\t\t# Use SMART for each goal\n","\t\t\t\t# Specific: You have clearly defined what you want to accomplish.\n","\t\t\t\t# Measurable: You have identified targets and milestones to track your progress.\n","\t\t\t\t# Attainable: Your goal is realistic and manageable.\n","\t\t\t\t# Relevant: You have identified a goal that fits with your business model.\n","\t\t\t\t# Time-Based: You have identified a specific period of time for the goal.\n","\t# Strategies:\n","    # How, what, and where?\n","    # How are you going to build your business?\n","    # What will you sell?\n","    # What is your unique selling proposition (i.e., what makes your business different from the competition)?\n","\t# Startup Capital:\n","    # How much do you need to launch?\n","\t# Expenses:\n","    # What will it cost to keep your business running for three months, for six months, and for one year?\n","\t# Projected Income:\n","    # What do you anticipate your business's ongoing monthly income will be immediately after launch, in three months, in six months, and in one year?\n","  # Action Plan:\n","    # What are the specific action items and tasks you need to complete now?\n","    # What are your future milestones?\n","    # What will need to be accomplished by those milestones in order to meet your objectives?\n","# the marketing plan\n","  # number of plans and types\n","    # 1. to promote your business.\n","      # Strategies:\n","        # What do you want your marketing plan to do for your business?\n","        # what you want to do and the details on how to do it?\n","\n","      # Mission Statement:\n","        # What is the purpose of your business? This is the same mission you included in your business plan.\n","        # How Will Your Marketing Plan Support Your Business Goals?\n","        # Your mission statement is the foundation of your marketing plan.\n","\n","      # Target Market:\n","        # Who is your ideal customer?\n","        # Who makes up your target audience\n","        # Where you can find them\n","        # What they value as important\n","        # What they are worried about\n","        # What they need right now\n","\n","      # Competitive Analysis:\n","        # Who are your competitors?\n","        # Who Are You Up Against, and Where Do You Rank?\n","        #use SWOT\n","          # Strengths:\n","            # What do you do well?\n","            # What are your unique skills?\n","            # What expert or specialized knowledge do you have?\n","            # What experience do you have?\n","            # What do you do better than your competitors?\n","            # Where are you most profitable in your business?\n","          # Weaknesses:\n","            # In what areas do you need to improve?\n","            # What resources do you lack?\n","            # What parts of your business are not profitable?\n","            # Where do you need further education and/or experience?\n","            # What costs you time and/or money?\n","          # Opportunities:\n","            # What are the business goals you are currently working towards?\n","            # How can you do more with your existing customers or clients?\n","            # How can you use technology to enhance your business?\n","            # Are there new target audiences you have the potential to reach?\n","            # Are there related products and services that provide an opportunity for your business?\n","          # Threats:\n","            # What obstacles do you face?\n","            # What are the strengths of your biggest competitors?\n","            # What are your competitors doing that you're not?\n","            # What's going on in the economy?\n","            # What's going on in the industry?\n","\n","      # Unique Selling Proposition:\n","        # What makes your business unique?\n","        # how your business, products, or services differ, in a better way, from your competition's.\n","        # what makes your business the better choice and why your target clients should choose you over the competition?\n","      \n","      # Pricing:\n","        # What will you charge, for your products or services and why?\n","      \n","      # Promotional Plan:\n","        # # How will you reach your target market?\n","        # how your product or service will be sold?\n","        # how you'll let your target market know about your offer, and customer service plans?\n","        #answer:\n","          # Advertising\n","          # Packaging\n","          # Public relations\n","          # Direct sales\n","          # Internet marketing\n","          # Sales promotions\n","          # Marketing materials\n","          # Other publicity efforts\n","      \n","      # Marketing Budget:\n","        # How much money will you spend, and on what?        \n","      \n","      # Action List:\n","        # What Tasks Do You Need to Complete to Reach Your Marketing Goals?\n","\n","      # Metrics:\n","        # How will you track the success of your marketing activities?\n","        # What Results Have You Achieved, and Where Can You Improve?\n","\n","\n","    # 2. to introduce new products or services to your market.\n","    # 3. to target new niche markets.\n","    # 4. to develops new ways to reach and attract customers. \n","# the financial plan\n","  \"not to spend more than you can afford\"\n","  # foramt link(https://www.sba.gov/sites/default/files/2020-08/Startup%20Costs%20Worksheet-508.pdf)\n","  # how much capital you'll require to start your business and where that capital will come from?\n","  #what are your  total expensives to calculate your capital ?\n","    # what are your one-time expenses?\n","      # (Buying major equipment, hiring a logo designer, and paying for permits, licenses, and fees are generally considered to be one-time expenses)\n","      # Equipment\n","      # Furniture\n","      # Software\n","      # Office space/store location\n","      # Remodeling work\n","      # Starting inventory\n","      # Public utility deposits\n","      # Legal and other professional fees\n","      # Licenses and permits\n","      # Insurance\n","      # Employee training\n","      # Website and other digital properties\n","      # Marketing collateral \n","      # Grand opening event\n","      # Advertising for grand opening\n","      # Office space\n","      # Equipment and supplies\n","      # Communications\n","      # Utilities\n","      # Licenses and permits\n","      # Insurance\n","      # Lawyer and accountant\n","      # Inventory\n","      # Employee salaries\n","      # Advertising and marketing\n","      # Market research\n","      # Printed marketing materials\n","      # Making a website\n","    # what are your montly expensives?\n","      # Your salary\n","      # Staff salaries\n","      # Rent\n","      # Utilities and bills\n","      # Advertising and promotion\n","      # Shipping and handling\n","      # Supplies\n","      # Telephone\n","      # High-speed Internet\n","      # Website maintenance\n","      # IT services\n","      # Bookkeeping or accounting services\n","      # Insurance\n","      # Taxes\n","  #fudning souraces:\n","    # self-funding (bootstrapping)\n","    # venture capital  (form investors)(angel investors)(venture capital firms)\n","      # Find an investor :\n","        # Almost all venture capitalists will, at a minimum, want a seat on the board of directors.\n","        # So be prepared to give up some portion of both control and ownership of your company in exchange for funding.\n","        # Is your investor reputable and has experience working with startup companies?\n","      # Share your business plan :\n","        # Most investment funds concentrate on an industry, geographic area, or stage of business development.\n","      # Go through due diligence review :\n","        # The investors will look at your company’s management team, market, products and services, corporate governance documents, and financial statements.\n","      # Work out the terms (contract like):\n","        # If they want to invest, the next step is to agree on a term sheet that describes the terms and conditions for the fund to make an investment.\n","      # Investment:\n","        # Venture funds normally come in “rounds.” As the company meets milestones, further rounds of financing are made available, with adjustments in price as the company executes its plan.\n","    # crowdfunding:\n","      # they don’t receive a share of ownership in the business and don’t expect a financial return on their money\n","      # Instead, crowdfunders expect to get a “gift” from your company as thanks for their contribution.\n","      # Often, that gift is the product you plan to sell or other special perks, like meeting the business owner or getting their name in the credits.\n","      # This makes crowdfunding a popular option for people who want to produce creative works (like a documentary), \n","      # or a physical product (like a high-tech cooler).\n","    # Loans ( banks and credit unions )\n","      # Do you have a business plan, expense sheet, and financial projections for the next five years to avail a loan?\n","\n","\n","# What is the purpose of your business?\n","# Who are you selling to?\n","# What are your end goals?\n","# How will you finance your startup costs?\n","# Who is going to buy your product or service?\n","# These questions can be answered in a well-written business plan. \n","# These helps in\n","\t# where your company is going?\n","\t# how it will overcome any potential difficulties?\n","\t# what you need to sustain it?\n","\n","# Conduct market research.\n","\t# Conducting in-depth market research on your field and the demographics of your potential clientele is an important part of crafting a business plan.\n","\t# This involves running surveys, holding focus groups, and researching SEO and public data.\n","\t# Market research helps you understand your target customer – their needs, preferences and behavior – as well as your industry and competitors.\n","\t# The U.S. Small Business Administration (SBA) recommends \"gathering demographic information\"(https://www.sba.gov/business-guide/plan-your-business/market-research-competitive-analysis) to better understand opportunities and limitations within your market. \n","  \n","\t# The best small businesses have products or services that are differentiated from the competition. This has a significant impact on your competitive landscape and allows you to convey unique value to potential customers. A guide to conducting market research can be found on our sister site (https://www.business.com/articles/guide-to-market-research/?_ga=2.83254101.2118168243.1581361076-1680643775.1572916112)\n","# Consider an exit strategy.\n","\t# What is your exit strategy to leave your bussiness to look in future?\n","\t# At least three or four pre-determined exit routes.\n","  # this includes retirement or leaving/selling bussiness etc..\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZpU9RDKLXD43","cellView":"form"},"source":["#@title 3. Assess your finances.\n","# \tfind out how much your startup costs will be.(https://www.businessnewsdaily.com/5-small-business-start-up-costs-options.html)\n","\n","# \tMany startups fail because they run out of money before turning a profit. It's never a bad idea to overestimate the amount of startup capital you need.\n","\n","# Perform a break-even analysis.\n","# \t(https://www.business.com/articles/in-pursuit-of-profit-applications-and-uses-of-breakeven-analysis/)\n","# \tBreak-Even Point = Fixed Costs / (Average Price – Variable Costs)\n","# \tThis is the minimum performance your business must achieve to avoid losing money.\n","# \thelps, where your profits come from, so you can set production goals accordingly.\n","# Determine profitability.\n","# \tHow much revenue do I need to generate to cover all my expenses?\n","# \tWhich products or services turn a profit and which ones are sold at a loss?\n","# Price a product or service.\n","# \thow much their product costs to create and how competitors are pricing their products?\n","# \tWhat are the fixed rates?\n","# \twhat are the variable costs?\n","# \twhat is the total cost?\n","# \tWhat is the cost of any physical goods?\n","# \twhat is the cost of labor?\n","# Analyze the data.\n","# \tWhat volumes of goods or services do you have to sell to be profitable?\n","# \tHow can I reduce my overall fixed costs?\n","# \tHow can I reduce the variable costs per unit?\n","# \tHow can I improve sales? \n","# Watch your expenses.\n","# \tDon't overspend when starting a business.\n","# \tUnderstand the types of purchases that make sense for your business and avoid overspending on fancy new equipment that won't help you reach your business goals.\n","# \tSpend as little as possible when you start and only on the things that are essential for the business to grow and be a success. Luxuries can come when you're established.\n","\t\n","# capital sources.\n","# \tbussiness laons(https://www.businessnewsdaily.com/8448-best-business-loans.html)(http://sba.gov/)\n","# \tInvestors(https://www.businessnewsdaily.com/9078-angel-investor-funding-reasons.html)\n","# \tor crowdfunding (https://www.businessnewsdaily.com/10620-crowdfunding-tips-business.html)\n","# \tothers (https://www.businessnewsdaily.com/1733-small-business-financing-options-.html)\n","# Choose the right business bank.\n","# \tsmall banks want to build a personal relationship with you and ultimately help you if you run into problems and miss a payment. Another good thing about smaller banks is that decisions are made at the branch level, which can be much quicker than big banks where decisions are made at a higher level.\n","\t\n","# \tWhat is important\n","# \t\tDo I want to build a close relationship with a bank that's willing to help me in any ay possible?\n","# \t\tDo I want to be just another bank account like big banks will view me as?\n","\n","# \task questions about how they work with small businesses to find the best bank for your business.\n","# \tdocuments(https://www.businessnewsdaily.com/6477-business-bank-account-documentation.html)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qlNPiPizXKma","cellView":"form"},"source":["#@title 4. Determine your legal business structure.\n","# \tBefore you can register your company, you need to decide what kind of entity it is. Your business structure legally affects everything from how you file your taxes to your personal liability if something goes wrong.\n","# \t(https://www.businessnewsdaily.com/8163-choose-legal-business-structure.html)\n","\t\n","# \twhich type of enity is the best.(https://www.businessnewsdaily.com/15339-llc-vs-scorporation.html)\n","# \t\tsole proprietorship\n","# \t\t\tIf you own the business entirely by yourself and plan to be responsible for all debts and obligations, you can register for a sole proprietorship. Be warned that this route can directly affect your personal credit. \n","# \t\tA partnership\n","# \t\t\tAs its name implies, means that two or more people are held personally liable as business owners. You don't have to go it alone if you can find a business partner with complementary skills to your own. It's usually a good idea to add someone into the mix to help your business flourish.\n","# \t\tlimited liability corporation.(LLC)\n","# \t\t\tIf you want to separate your personal liability from your company's liability, you may want to consider forming one of several types of corporations. This makes a business a separate entity apart from its owners, and, therefore, corporations can own property, assume liability, pay taxes, enter contracts, sue and be sued like any other individual.\n","# \t\t\tThis hybrid structure has the legal protections of a corporation while allowing for the tax benefits of a partnership.\n","# Corporation\n","  #entity owned by a list of shareholders. The shareholders have the mandate to elect a board of directors whose work is to oversee the day to day running of the corporation. When it comes to decision making, it is the responsibility of the directors to make sure that any decision made benefits the corporation and is in support of the corporation's objectives. Also, the directors have the power to hire and fire employees. \n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cpjj2aIrXOiz","cellView":"form"},"source":["#@title 5. Register with the government and IRS.\n","# \t\"articles of incorporation\" document(https://www.businessnewsdaily.com/4038-articles-of-incorporation.html)\n","# \tit includes business name, business purpose, corporate structure, stock details and other information about your company. \n","\n","# \t \"doing business as\" (DBA) name(your real name or company name)(https://www.businessnewsdaily.com/48-doing-business-as-how-to-register-a-dba-name.html)\n","# \t if you are the sole proprietor.\n","# \t trademark your business name for extra legal protection.\n","\t\n","# \tDBA certificate.\n","# \t It's best to contact or visit your local county clerk's office and ask about specific requirements and fees. Generally, there is a registration fee involved. \n","\n","# \tEmployer Identification Number (EIN) from the IRS. (https://www.businessnewsdaily.com/17-federal-employer-identification-number-criteria.html)(http://www.irs.gov/Businesses/Small-Businesses-&-Self-Employed/Do-You-Need-an-EIN?)\n","# \tgenerally not required for sole proprietorships with no employees, but you may want to apply for one anyway to keep your personal and business taxes separate, or simply to save yourself the trouble later if you decide to hire someone. \n","\n","# \tfederal and state income tax obligations.(https://www.sba.gov/business-guide/launch-your-business/get-federal-and-state-tax-id-numbers)\n","# \tThe forms you need are determined by your business structure. A complete list of the forms each type of entity will need can be found on SBA's website. You will need to check your state's website for information on state-specific and local tax obligations. \n","\t\n","# \tif you start with a proper foundation, your business will have fewer hiccups to worry about in the long run\n","\n","# Obtain all required licenses and permits.\n","# \trequire federal, state or local licenses and permits to operate.\n","# \tThe best place to obtain a business license is at your local city hall.\n","# \tYou can then use the SBA's database to search for licensing requirements by state and business type. (http://www.sba.gov/content/business-licenses-and-permits)\n","\t\n","# \tprofessional licenses.(https://www.businessnewsdaily.com/2492-occupations-requiring-licenses.html)\n","# \tex:commercial driver's license (CDL).\n","# \tIndividuals with a CDL are allowed to operate certain types of vehicles, such as buses, tank trucks and tractor-trailers. A CDL is divided into three classes: Class A, Class B and Class C. \n","\n","# \tseller's permit( other names: resale permit, resell permit, permit license, reseller permit, resale ID, state tax ID number, reseller number, reseller license permit or certificate of authority)\n","# \tto collect sales tax from your customers.\n","# \tthese requirements and names vary from state to state. You can register for a seller's permit through the state government website of the state(s) you're doing business in.\n","\t\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9f9at0RXS3z","cellView":"form"},"source":["#@title 6. Purchase an insurance policy.\n","(https://www.businessnewsdaily.com/138-determining-small-business-insurance-needs.html)\n","# \tDealing with incidents such as property damage, theft or even a customer lawsuit can be costly, and you need to be sure that you're properly protected. \n","\t\n","# \tIf your business will have employees.\n","# \tworkers' compensation(https://www.businessnewsdaily.com/10670-what-is-workers-compensation.html)\n","# \tunemployment insurance(https://www.businessnewsdaily.com/10701-unemployment-insurance-small-business.html)\n","\n","# \tgeneral liability (GL) insurance, or a business owner's policy.(https://www.businessnewsdaily.com/335-small-business-insuranc-policy-primer.html)\n","# \tGL covers property damage, bodily injury and personal injury to yourself or a third party.\n","\n","# \tIf your business provides a service, you may also want to consider\n","# \tprofessional liability insurance.\n","# \tIt covers you if you do something wrong or neglect to do something you should have done while operating your business. \n","\n","# \tothers(https://www.businessnewsdaily.com/5896-small-business-insurance-tips.html)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fpQ3sFbbXXPO","cellView":"form"},"source":["#@title 7. Build your team.\n","# \t(https://www.businessnewsdaily.com/9692-recruiting-strategies.html)\n","# \t(https://www.businessnewsdaily.com/15186-first-startup-hires.html)\n","\t\n","# \tIdentifying your founding team, understanding what gaps exist?\n","# \tdetermining how and when you will address them ? should be top priority\n","# \tFiguring out how the team will work together?(https://www.businessnewsdaily.com/6139-workplace-collaboration-tips.html)\n","# \tDefining roles and responsibility, division of labor.\n","# \thow to give feedback ?\n","# \thow to work together when not everyone is in the same room?\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pe8tIvKaXbWg","cellView":"form"},"source":["#@title 8. Choose your vendors.\n","(the third-party)(me , my team and employes , vendors)\n","# \tCompanies in every industry from HR to business phone systems exist to partner with you and help you run your business better.\n","\t\n","# \tB2B partners(bussiness 2 bussiness)(choose carefully.)(https://www.businessnewsdaily.com/8451-find-b2b-partners.html)(https://www.businessnewsdaily.com/find-a-solution)\n","# \tThese companies will have access to vital and potentially sensitive business data, so it's critical to find someone you can trust.\n","\t\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CeucZ6o8XcXH","cellView":"form"},"source":["#@title 9. Brand yourself and advertise.\n","   # (https://www.businessnewsdaily.com/8276-best-email-marketing-software.html)\n","# \tBefore you start selling your product or service, you need to build up your brand and get a following of people who are ready to jump when you open your doors for business.\n","\t\n","# \tcreate a logo (https://www.businessnewsdaily.com/10269-corporate-logo-brand-business.html)\n","# \tthat can help people easily identify your brand, and be consistent in using it across all of your platforms, including your all-important company website.(https://www.businessnewsdaily.com/4661-starting-a-business-website.html)\n","# \tUse social media to spread the word about your new business.(https://www.businessnewsdaily.com/7832-social-media-for-business.html)\n","\t\n","# \tCreating a marketing plan(https://www.businessnewsdaily.com/4-creating-effective-business-marketing-plan.html)\n","\t\n","# \tAsk customers to opt-in to your marketing communications.\n","# \task your customers and potential customers for permission to communicate with them.\n","# \tby using opt-in forms. These are \"forms of consent\" given by web users, authorizing you to contact them with further information about your business\n","# \tOpt-in forms are a great starting point for building trust and respect with potential customers.\n","\t\n","# \tit's important to know that these forms are required by law.\n","# \tThe CAN-SPAM Act of 2003 (https://www.ftc.gov/sites/default/files/documents/cases/2007/11/canspam.pdf)\n","# \tsets requirements for commercial email by the Federal Trade Commission. \n","# \tThis law doesn't just apply to bulk email,\n","# \tit covers all commercial messages in which the law defines as \"any electronic mail message the primary purpose of which is the commercial advertisement or promotion of a commercial product or service.\"\n","# \tEach email in violation of this law is subject to fines of more than $40,000.(https://www.ftc.gov/tips-advice/business-center/guidance/can-spam-act-compliance-guide-business)\n","\t\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cleK370qXb7-","cellView":"form"},"source":["#@title 10. Grow your business.\n","# \tTo make a profit and stay afloat, you always need to be growing your business.\n","\t\n","# \tCollaborating with more established brands in your industry is a great way to achieve growth.\n","# \tReach out to other companies or even https://www.businessnewsdaily.com and ask for some promotion in exchange for a free product sample or service.\n","# \tPartner with a charity organization, and volunteer some of your time or products to get your name out there. To grow your business quickly.(https://www.businessnewsdaily.com/7690-rapid-business-growth-tips.html)\n","\n","# there is not perfect plan.(https://www.sba.gov/sites/default/files/Business-Survival.pdf)\n","# things can get tangled.\n","# Be prepared to adjust.\n","# you must adapt to the changing situations.\n","# As an entrepreneur, your value lies in solving problems whether that is your product or service solving problems for other people or you solving problems within your organization.\n"],"execution_count":null,"outputs":[]}]}